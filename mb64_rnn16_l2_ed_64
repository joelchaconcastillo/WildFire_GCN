2022-12-29 19:39: log dir: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122919393567017369118
2022-12-29 19:39: Experiment log path in: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122919393567017369118
2022-12-29 19:39: Argument: Namespace(batch_size=256, clc='vec', cuda=True, dataset='2020', dataset_root='/home/joel.chacon/tmp/datasets_grl/', debug=False, default_graph=True, device='cpu', dynamic_features=['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh'], early_stop=True, early_stop_patience=8, embed_dim=64, epochs=30, grad_norm=False, horizon=1, input_dim=25, lag=10, link_len=2, log_dir='/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122919393567017369118', log_step=1, loss_func='nllloss', lr_decay=True, lr_decay_rate=0.1, lr_decay_step='15, 20', lr_init=0.0005, max_grad_norm=5, minbatch_size=64, mode='train', model='fire_GCN', nan_fill=0.5, num_layers=2, num_nodes=625, num_workers=20, output_dim=2, patch_height=25, patch_width=25, persistent_workers=True, pin_memory=True, plot=False, positive_weight=0.5, prefetch_factor=2, real_value=True, rnn_units=16, seed=10000, static_features=['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density'], teacher_forcing=False, weight_decay=0.0, window_len=10)
2022-12-29 19:39: Argument batch_size: 256
2022-12-29 19:39: Argument clc: 'vec'
2022-12-29 19:39: Argument cuda: True
2022-12-29 19:39: Argument dataset: '2020'
2022-12-29 19:39: Argument dataset_root: '/home/joel.chacon/tmp/datasets_grl/'
2022-12-29 19:39: Argument debug: False
2022-12-29 19:39: Argument default_graph: True
2022-12-29 19:39: Argument device: 'cpu'
2022-12-29 19:39: Argument dynamic_features: ['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh']
2022-12-29 19:39: Argument early_stop: True
2022-12-29 19:39: Argument early_stop_patience: 8
2022-12-29 19:39: Argument embed_dim: 64
2022-12-29 19:39: Argument epochs: 30
2022-12-29 19:39: Argument grad_norm: False
2022-12-29 19:39: Argument horizon: 1
2022-12-29 19:39: Argument input_dim: 25
2022-12-29 19:39: Argument lag: 10
2022-12-29 19:39: Argument link_len: 2
2022-12-29 19:39: Argument log_dir: '/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122919393567017369118'
2022-12-29 19:39: Argument log_step: 1
2022-12-29 19:39: Argument loss_func: 'nllloss'
2022-12-29 19:39: Argument lr_decay: True
2022-12-29 19:39: Argument lr_decay_rate: 0.1
2022-12-29 19:39: Argument lr_decay_step: '15, 20'
2022-12-29 19:39: Argument lr_init: 0.0005
2022-12-29 19:39: Argument max_grad_norm: 5
2022-12-29 19:39: Argument minbatch_size: 64
2022-12-29 19:39: Argument mode: 'train'
2022-12-29 19:39: Argument model: 'fire_GCN'
2022-12-29 19:39: Argument nan_fill: 0.5
2022-12-29 19:39: Argument num_layers: 2
2022-12-29 19:39: Argument num_nodes: 625
2022-12-29 19:39: Argument num_workers: 20
2022-12-29 19:39: Argument output_dim: 2
2022-12-29 19:39: Argument patch_height: 25
2022-12-29 19:39: Argument patch_width: 25
2022-12-29 19:39: Argument persistent_workers: True
2022-12-29 19:39: Argument pin_memory: True
2022-12-29 19:39: Argument plot: False
2022-12-29 19:39: Argument positive_weight: 0.5
2022-12-29 19:39: Argument prefetch_factor: 2
2022-12-29 19:39: Argument real_value: True
2022-12-29 19:39: Argument rnn_units: 16
2022-12-29 19:39: Argument seed: 10000
2022-12-29 19:39: Argument static_features: ['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density']
2022-12-29 19:39: Argument teacher_forcing: False
2022-12-29 19:39: Argument weight_decay: 0.0
2022-12-29 19:39: Argument window_len: 10
++++++++++++++
2020_fire_GCN.conf
++++++++++++++
*****************Model Parameter*****************
node_embeddings torch.Size([625, 64]) True
ln1.weight torch.Size([25]) True
ln1.bias torch.Size([25]) True
encoder.cell_list.0.gate.weights_pool torch.Size([64, 2, 41, 16]) True
encoder.cell_list.0.gate.weights_window torch.Size([64, 1, 16]) True
encoder.cell_list.0.gate.bias_pool torch.Size([64, 32]) True
encoder.cell_list.0.gate.T torch.Size([10]) True
encoder.cell_list.0.update.weights_pool torch.Size([64, 2, 41, 8]) True
encoder.cell_list.0.update.weights_window torch.Size([64, 1, 8]) True
encoder.cell_list.0.update.bias_pool torch.Size([64, 16]) True
encoder.cell_list.0.update.T torch.Size([10]) True
encoder.cell_list.1.gate.weights_pool torch.Size([64, 2, 32, 16]) True
encoder.cell_list.1.gate.weights_window torch.Size([64, 16, 16]) True
encoder.cell_list.1.gate.bias_pool torch.Size([64, 32]) True
encoder.cell_list.1.gate.T torch.Size([10]) True
encoder.cell_list.1.update.weights_pool torch.Size([64, 2, 32, 8]) True
encoder.cell_list.1.update.weights_window torch.Size([64, 16, 8]) True
encoder.cell_list.1.update.bias_pool torch.Size([64, 16]) True
encoder.cell_list.1.update.T torch.Size([10]) True
fc1.weight torch.Size([2, 10000]) True
fc1.bias torch.Size([2]) True
Total params num: 316604
*****************Finish Parameter****************
Positives: 5201 / Negatives: 10402
Dataset length 15603
Positives: 1300 / Negatives: 2600
Dataset length 3900
Positives: 1228 / Negatives: 2456
Dataset length 3684
Applying learning rate decay.
Creat Log File in:  /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122919393567017369118/run.log
2022-12-29 19:39: Train Epoch 1: 3/244 Loss: 0.493138
2022-12-29 19:40: Train Epoch 1: 7/244 Loss: 4.777565
2022-12-29 19:40: Train Epoch 1: 11/244 Loss: 2.286289
2022-12-29 19:40: Train Epoch 1: 15/244 Loss: 1.198206
2022-12-29 19:40: Train Epoch 1: 19/244 Loss: 1.839341
2022-12-29 19:40: Train Epoch 1: 23/244 Loss: 1.502316
2022-12-29 19:41: Train Epoch 1: 27/244 Loss: 1.050096
2022-12-29 19:41: Train Epoch 1: 31/244 Loss: 0.546141
2022-12-29 19:41: Train Epoch 1: 35/244 Loss: 0.959641
2022-12-29 19:41: Train Epoch 1: 39/244 Loss: 0.680061
2022-12-29 19:41: Train Epoch 1: 43/244 Loss: 0.248211
2022-12-29 19:41: Train Epoch 1: 47/244 Loss: 0.480876
2022-12-29 19:42: Train Epoch 1: 51/244 Loss: 0.689945
2022-12-29 19:42: Train Epoch 1: 55/244 Loss: 0.734636
2022-12-29 19:42: Train Epoch 1: 59/244 Loss: 0.454875
2022-12-29 19:42: Train Epoch 1: 63/244 Loss: 0.249803
2022-12-29 19:42: Train Epoch 1: 67/244 Loss: 0.309668
2022-12-29 19:43: Train Epoch 1: 71/244 Loss: 0.488764
2022-12-29 19:43: Train Epoch 1: 75/244 Loss: 0.393256
2022-12-29 19:43: Train Epoch 1: 79/244 Loss: 0.322220
2022-12-29 19:43: Train Epoch 1: 83/244 Loss: 0.457882
2022-12-29 19:43: Train Epoch 1: 87/244 Loss: 0.395498
2022-12-29 19:43: Train Epoch 1: 91/244 Loss: 0.317172
2022-12-29 19:44: Train Epoch 1: 95/244 Loss: 0.213117
2022-12-29 19:44: Train Epoch 1: 99/244 Loss: 0.345230
2022-12-29 19:44: Train Epoch 1: 103/244 Loss: 0.392608
2022-12-29 19:44: Train Epoch 1: 107/244 Loss: 0.230094
2022-12-29 19:44: Train Epoch 1: 111/244 Loss: 0.234029
2022-12-29 19:45: Train Epoch 1: 115/244 Loss: 0.295470
2022-12-29 19:45: Train Epoch 1: 119/244 Loss: 0.282292
2022-12-29 19:45: Train Epoch 1: 123/244 Loss: 0.254245
2022-12-29 19:45: Train Epoch 1: 127/244 Loss: 0.235204
2022-12-29 19:45: Train Epoch 1: 131/244 Loss: 0.255943
2022-12-29 19:45: Train Epoch 1: 135/244 Loss: 0.202955
2022-12-29 19:46: Train Epoch 1: 139/244 Loss: 0.305560
2022-12-29 19:46: Train Epoch 1: 143/244 Loss: 0.229233
2022-12-29 19:46: Train Epoch 1: 147/244 Loss: 0.202719
2022-12-29 19:46: Train Epoch 1: 151/244 Loss: 0.265306
2022-12-29 19:46: Train Epoch 1: 155/244 Loss: 0.273932
2022-12-29 19:46: Train Epoch 1: 159/244 Loss: 0.187368
2022-12-29 19:47: Train Epoch 1: 163/244 Loss: 0.175051
2022-12-29 19:47: Train Epoch 1: 167/244 Loss: 0.248980
2022-12-29 19:47: Train Epoch 1: 171/244 Loss: 0.244337
2022-12-29 19:47: Train Epoch 1: 175/244 Loss: 0.225084
2022-12-29 19:47: Train Epoch 1: 179/244 Loss: 0.170768
2022-12-29 19:47: Train Epoch 1: 183/244 Loss: 0.225760
2022-12-29 19:48: Train Epoch 1: 187/244 Loss: 0.186079
2022-12-29 19:48: Train Epoch 1: 191/244 Loss: 0.327451
2022-12-29 19:48: Train Epoch 1: 195/244 Loss: 0.181970
2022-12-29 19:48: Train Epoch 1: 199/244 Loss: 0.234697
2022-12-29 19:49: Train Epoch 1: 203/244 Loss: 0.274380
2022-12-29 19:49: Train Epoch 1: 207/244 Loss: 0.145762
2022-12-29 19:49: Train Epoch 1: 211/244 Loss: 0.189471
2022-12-29 19:49: Train Epoch 1: 215/244 Loss: 0.279328
2022-12-29 19:49: Train Epoch 1: 219/244 Loss: 0.213093
2022-12-29 19:49: Train Epoch 1: 223/244 Loss: 0.224003
2022-12-29 19:50: Train Epoch 1: 227/244 Loss: 0.214043
2022-12-29 19:50: Train Epoch 1: 231/244 Loss: 0.194362
2022-12-29 19:50: Train Epoch 1: 235/244 Loss: 0.214062
2022-12-29 19:50: Train Epoch 1: 239/244 Loss: 0.191724
2022-12-29 19:50: Train Epoch 1: 243/244 Loss: 0.176837
2022-12-29 19:50: **********Train Epoch 1: averaged Loss: 0.488822 
2022-12-29 19:50: 
Epoch time elapsed: 672.8297917842865

2022-12-29 19:51: 
 metrics validation: {'precision': 0.8142589118198874, 'recall': 0.33384615384615385, 'f1-score': 0.4735406437534097, 'support': 1300, 'AUC': 0.8099272189349113, 'AUCPR': 0.7195631036821492, 'TP': 434, 'FP': 99, 'TN': 2501, 'FN': 866} 

2022-12-29 19:51: **********Val Epoch 1: average Loss: 0.365943
2022-12-29 19:51: *********************************Current best model saved!
2022-12-29 19:52: 
 Testing metrics {'precision': 0.874, 'recall': 0.3558631921824104, 'f1-score': 0.505787037037037, 'support': 1228, 'AUC': 0.8502016599645619, 'AUCPR': 0.7776808553221668, 'TP': 437, 'FP': 63, 'TN': 2393, 'FN': 791} 

2022-12-29 19:52: Train Epoch 2: 3/244 Loss: 0.224279
2022-12-29 19:52: Train Epoch 2: 7/244 Loss: 0.212325
2022-12-29 19:53: Train Epoch 2: 11/244 Loss: 0.196988
2022-12-29 19:53: Train Epoch 2: 15/244 Loss: 0.251968
2022-12-29 19:53: Train Epoch 2: 19/244 Loss: 0.162465
2022-12-29 19:53: Train Epoch 2: 23/244 Loss: 0.182551
2022-12-29 19:53: Train Epoch 2: 27/244 Loss: 0.186545
2022-12-29 19:53: Train Epoch 2: 31/244 Loss: 0.188139
2022-12-29 19:54: Train Epoch 2: 35/244 Loss: 0.193564
2022-12-29 19:54: Train Epoch 2: 39/244 Loss: 0.272014
2022-12-29 19:54: Train Epoch 2: 43/244 Loss: 0.215143
2022-12-29 19:54: Train Epoch 2: 47/244 Loss: 0.182542
2022-12-29 19:54: Train Epoch 2: 51/244 Loss: 0.236249
2022-12-29 19:55: Train Epoch 2: 55/244 Loss: 0.193507
2022-12-29 19:55: Train Epoch 2: 59/244 Loss: 0.208168
2022-12-29 19:55: Train Epoch 2: 63/244 Loss: 0.170060
2022-12-29 19:55: Train Epoch 2: 67/244 Loss: 0.234687
2022-12-29 19:55: Train Epoch 2: 71/244 Loss: 0.200257
2022-12-29 19:56: Train Epoch 2: 75/244 Loss: 0.185452
2022-12-29 19:56: Train Epoch 2: 79/244 Loss: 0.165357
2022-12-29 19:56: Train Epoch 2: 83/244 Loss: 0.298463
2022-12-29 19:56: Train Epoch 2: 87/244 Loss: 0.193548
2022-12-29 19:56: Train Epoch 2: 91/244 Loss: 0.189943
2022-12-29 19:56: Train Epoch 2: 95/244 Loss: 0.214685
2022-12-29 19:57: Train Epoch 2: 99/244 Loss: 0.194090
2022-12-29 19:57: Train Epoch 2: 103/244 Loss: 0.157339
2022-12-29 19:57: Train Epoch 2: 107/244 Loss: 0.225907
2022-12-29 19:57: Train Epoch 2: 111/244 Loss: 0.213055
2022-12-29 19:57: Train Epoch 2: 115/244 Loss: 0.160975
2022-12-29 19:58: Train Epoch 2: 119/244 Loss: 0.189918
2022-12-29 19:58: Train Epoch 2: 123/244 Loss: 0.207573
2022-12-29 19:58: Train Epoch 2: 127/244 Loss: 0.181465
2022-12-29 19:58: Train Epoch 2: 131/244 Loss: 0.166171
2022-12-29 19:58: Train Epoch 2: 135/244 Loss: 0.208085
2022-12-29 19:59: Train Epoch 2: 139/244 Loss: 0.222554
2022-12-29 19:59: Train Epoch 2: 143/244 Loss: 0.146238
2022-12-29 19:59: Train Epoch 2: 147/244 Loss: 0.222386
2022-12-29 19:59: Train Epoch 2: 151/244 Loss: 0.152261
2022-12-29 19:59: Train Epoch 2: 155/244 Loss: 0.162440
2022-12-29 19:59: Train Epoch 2: 159/244 Loss: 0.192364
2022-12-29 20:00: Train Epoch 2: 163/244 Loss: 0.182220
2022-12-29 20:00: Train Epoch 2: 167/244 Loss: 0.160693
2022-12-29 20:00: Train Epoch 2: 171/244 Loss: 0.166905
2022-12-29 20:00: Train Epoch 2: 175/244 Loss: 0.168730
2022-12-29 20:01: Train Epoch 2: 179/244 Loss: 0.183599
2022-12-29 20:01: Train Epoch 2: 183/244 Loss: 0.154444
2022-12-29 20:01: Train Epoch 2: 187/244 Loss: 0.163016
2022-12-29 20:01: Train Epoch 2: 191/244 Loss: 0.187896
2022-12-29 20:01: Train Epoch 2: 195/244 Loss: 0.181900
2022-12-29 20:01: Train Epoch 2: 199/244 Loss: 0.198790
2022-12-29 20:02: Train Epoch 2: 203/244 Loss: 0.165967
2022-12-29 20:02: Train Epoch 2: 207/244 Loss: 0.181251
2022-12-29 20:02: Train Epoch 2: 211/244 Loss: 0.215606
2022-12-29 20:02: Train Epoch 2: 215/244 Loss: 0.174837
2022-12-29 20:02: Train Epoch 2: 219/244 Loss: 0.203919
2022-12-29 20:02: Train Epoch 2: 223/244 Loss: 0.192895
2022-12-29 20:03: Train Epoch 2: 227/244 Loss: 0.186059
2022-12-29 20:03: Train Epoch 2: 231/244 Loss: 0.178332
2022-12-29 20:03: Train Epoch 2: 235/244 Loss: 0.187111
2022-12-29 20:03: Train Epoch 2: 239/244 Loss: 0.190982
2022-12-29 20:03: Train Epoch 2: 243/244 Loss: 0.173501
2022-12-29 20:03: **********Train Epoch 2: averaged Loss: 0.192793 
2022-12-29 20:03: 
Epoch time elapsed: 665.821008682251

2022-12-29 20:04: 
 metrics validation: {'precision': 0.8110328638497653, 'recall': 0.5315384615384615, 'f1-score': 0.6421933085501859, 'support': 1300, 'AUC': 0.8351733727810652, 'AUCPR': 0.765133397321839, 'TP': 691, 'FP': 161, 'TN': 2439, 'FN': 609} 

2022-12-29 20:04: **********Val Epoch 2: average Loss: 0.278908
2022-12-29 20:04: *********************************Current best model saved!
2022-12-29 20:05: 
 Testing metrics {'precision': 0.8418740849194729, 'recall': 0.46824104234527686, 'f1-score': 0.6017791732077447, 'support': 1228, 'AUC': 0.8579301902407452, 'AUCPR': 0.8011649874678857, 'TP': 575, 'FP': 108, 'TN': 2348, 'FN': 653} 

2022-12-29 20:05: Train Epoch 3: 3/244 Loss: 0.183714
2022-12-29 20:05: Train Epoch 3: 7/244 Loss: 0.159808
2022-12-29 20:05: Train Epoch 3: 11/244 Loss: 0.189871
2022-12-29 20:06: Train Epoch 3: 15/244 Loss: 0.166390
2022-12-29 20:06: Train Epoch 3: 19/244 Loss: 0.185378
2022-12-29 20:06: Train Epoch 3: 23/244 Loss: 0.177402
2022-12-29 20:06: Train Epoch 3: 27/244 Loss: 0.177031
2022-12-29 20:06: Train Epoch 3: 31/244 Loss: 0.196779
2022-12-29 20:06: Train Epoch 3: 35/244 Loss: 0.187628
2022-12-29 20:07: Train Epoch 3: 39/244 Loss: 0.162528
2022-12-29 20:07: Train Epoch 3: 43/244 Loss: 0.170950
2022-12-29 20:07: Train Epoch 3: 47/244 Loss: 0.169953
2022-12-29 20:07: Train Epoch 3: 51/244 Loss: 0.159796
2022-12-29 20:07: Train Epoch 3: 55/244 Loss: 0.147869
2022-12-29 20:07: Train Epoch 3: 59/244 Loss: 0.187634
2022-12-29 20:08: Train Epoch 3: 63/244 Loss: 0.199105
2022-12-29 20:08: Train Epoch 3: 67/244 Loss: 0.166051
2022-12-29 20:08: Train Epoch 3: 71/244 Loss: 0.153585
2022-12-29 20:08: Train Epoch 3: 75/244 Loss: 0.175090
2022-12-29 20:08: Train Epoch 3: 79/244 Loss: 0.203011
2022-12-29 20:09: Train Epoch 3: 83/244 Loss: 0.171193
2022-12-29 20:09: Train Epoch 3: 87/244 Loss: 0.184327
2022-12-29 20:09: Train Epoch 3: 91/244 Loss: 0.194030
2022-12-29 20:09: Train Epoch 3: 95/244 Loss: 0.141480
2022-12-29 20:09: Train Epoch 3: 99/244 Loss: 0.179618
2022-12-29 20:09: Train Epoch 3: 103/244 Loss: 0.180172
2022-12-29 20:10: Train Epoch 3: 107/244 Loss: 0.171533
2022-12-29 20:10: Train Epoch 3: 111/244 Loss: 0.192907
2022-12-29 20:10: Train Epoch 3: 115/244 Loss: 0.171533
2022-12-29 20:10: Train Epoch 3: 119/244 Loss: 0.191495
2022-12-29 20:10: Train Epoch 3: 123/244 Loss: 0.184678
2022-12-29 20:11: Train Epoch 3: 127/244 Loss: 0.175290
2022-12-29 20:11: Train Epoch 3: 131/244 Loss: 0.211117
2022-12-29 20:11: Train Epoch 3: 135/244 Loss: 0.174868
2022-12-29 20:11: Train Epoch 3: 139/244 Loss: 0.169027
2022-12-29 20:11: Train Epoch 3: 143/244 Loss: 0.204320
2022-12-29 20:11: Train Epoch 3: 147/244 Loss: 0.230721
2022-12-29 20:12: Train Epoch 3: 151/244 Loss: 0.247035
2022-12-29 20:12: Train Epoch 3: 155/244 Loss: 0.200012
2022-12-29 20:12: Train Epoch 3: 159/244 Loss: 0.158170
2022-12-29 20:12: Train Epoch 3: 163/244 Loss: 0.262467
2022-12-29 20:12: Train Epoch 3: 167/244 Loss: 0.154999
2022-12-29 20:12: Train Epoch 3: 171/244 Loss: 0.159539
2022-12-29 20:13: Train Epoch 3: 175/244 Loss: 0.165298
2022-12-29 20:13: Train Epoch 3: 179/244 Loss: 0.179261
2022-12-29 20:13: Train Epoch 3: 183/244 Loss: 0.151491
2022-12-29 20:13: Train Epoch 3: 187/244 Loss: 0.180940
2022-12-29 20:13: Train Epoch 3: 191/244 Loss: 0.173637
2022-12-29 20:13: Train Epoch 3: 195/244 Loss: 0.164529
2022-12-29 20:14: Train Epoch 3: 199/244 Loss: 0.213230
2022-12-29 20:14: Train Epoch 3: 203/244 Loss: 0.168926
2022-12-29 20:14: Train Epoch 3: 207/244 Loss: 0.136202
2022-12-29 20:14: Train Epoch 3: 211/244 Loss: 0.183436
2022-12-29 20:14: Train Epoch 3: 215/244 Loss: 0.142311
2022-12-29 20:14: Train Epoch 3: 219/244 Loss: 0.182156
2022-12-29 20:15: Train Epoch 3: 223/244 Loss: 0.180124
2022-12-29 20:15: Train Epoch 3: 227/244 Loss: 0.153097
2022-12-29 20:15: Train Epoch 3: 231/244 Loss: 0.193057
2022-12-29 20:15: Train Epoch 3: 235/244 Loss: 0.154267
2022-12-29 20:15: Train Epoch 3: 239/244 Loss: 0.200741
2022-12-29 20:15: Train Epoch 3: 243/244 Loss: 0.158382
2022-12-29 20:15: **********Train Epoch 3: averaged Loss: 0.178872 
2022-12-29 20:15: 
Epoch time elapsed: 627.4681103229523

2022-12-29 20:16: 
 metrics validation: {'precision': 0.7261538461538461, 'recall': 0.7261538461538461, 'f1-score': 0.7261538461538461, 'support': 1300, 'AUC': 0.8561248520710061, 'AUCPR': 0.7867809947413356, 'TP': 944, 'FP': 356, 'TN': 2244, 'FN': 356} 

2022-12-29 20:16: **********Val Epoch 3: average Loss: 0.241414
2022-12-29 20:16: *********************************Current best model saved!
2022-12-29 20:17: 
 Testing metrics {'precision': 0.7584124245038827, 'recall': 0.7157980456026058, 'f1-score': 0.7364893171344784, 'support': 1228, 'AUC': 0.8709140813165127, 'AUCPR': 0.8184693471770149, 'TP': 879, 'FP': 280, 'TN': 2176, 'FN': 349} 

2022-12-29 20:17: Train Epoch 4: 3/244 Loss: 0.175130
2022-12-29 20:17: Train Epoch 4: 7/244 Loss: 0.155907
2022-12-29 20:18: Train Epoch 4: 11/244 Loss: 0.162576
2022-12-29 20:18: Train Epoch 4: 15/244 Loss: 0.169892
2022-12-29 20:18: Train Epoch 4: 19/244 Loss: 0.179080
2022-12-29 20:18: Train Epoch 4: 23/244 Loss: 0.186170
2022-12-29 20:18: Train Epoch 4: 27/244 Loss: 0.168745
2022-12-29 20:18: Train Epoch 4: 31/244 Loss: 0.173619
2022-12-29 20:19: Train Epoch 4: 35/244 Loss: 0.152567
2022-12-29 20:19: Train Epoch 4: 39/244 Loss: 0.147961
2022-12-29 20:19: Train Epoch 4: 43/244 Loss: 0.173786
2022-12-29 20:19: Train Epoch 4: 47/244 Loss: 0.171513
2022-12-29 20:19: Train Epoch 4: 51/244 Loss: 0.157590
2022-12-29 20:20: Train Epoch 4: 55/244 Loss: 0.175081
2022-12-29 20:20: Train Epoch 4: 59/244 Loss: 0.134106
2022-12-29 20:20: Train Epoch 4: 63/244 Loss: 0.154522
2022-12-29 20:20: Train Epoch 4: 67/244 Loss: 0.166828
2022-12-29 20:20: Train Epoch 4: 71/244 Loss: 0.157285
2022-12-29 20:20: Train Epoch 4: 75/244 Loss: 0.166906
2022-12-29 20:21: Train Epoch 4: 79/244 Loss: 0.140233
2022-12-29 20:21: Train Epoch 4: 83/244 Loss: 0.163813
2022-12-29 20:21: Train Epoch 4: 87/244 Loss: 0.172501
2022-12-29 20:21: Train Epoch 4: 91/244 Loss: 0.166711
2022-12-29 20:21: Train Epoch 4: 95/244 Loss: 0.160707
2022-12-29 20:22: Train Epoch 4: 99/244 Loss: 0.195206
2022-12-29 20:22: Train Epoch 4: 103/244 Loss: 0.176954
2022-12-29 20:22: Train Epoch 4: 107/244 Loss: 0.175114
2022-12-29 20:22: Train Epoch 4: 111/244 Loss: 0.183541
2022-12-29 20:22: Train Epoch 4: 115/244 Loss: 0.137952
2022-12-29 20:22: Train Epoch 4: 119/244 Loss: 0.177252
2022-12-29 20:23: Train Epoch 4: 123/244 Loss: 0.142474
2022-12-29 20:23: Train Epoch 4: 127/244 Loss: 0.147591
2022-12-29 20:23: Train Epoch 4: 131/244 Loss: 0.176212
2022-12-29 20:23: Train Epoch 4: 135/244 Loss: 0.132114
2022-12-29 20:23: Train Epoch 4: 139/244 Loss: 0.180606
2022-12-29 20:24: Train Epoch 4: 143/244 Loss: 0.162498
2022-12-29 20:24: Train Epoch 4: 147/244 Loss: 0.164774
2022-12-29 20:24: Train Epoch 4: 151/244 Loss: 0.176652
2022-12-29 20:24: Train Epoch 4: 155/244 Loss: 0.139976
2022-12-29 20:24: Train Epoch 4: 159/244 Loss: 0.142479
2022-12-29 20:24: Train Epoch 4: 163/244 Loss: 0.206377
2022-12-29 20:25: Train Epoch 4: 167/244 Loss: 0.178334
2022-12-29 20:25: Train Epoch 4: 171/244 Loss: 0.148747
2022-12-29 20:25: Train Epoch 4: 175/244 Loss: 0.163350
2022-12-29 20:25: Train Epoch 4: 179/244 Loss: 0.121009
2022-12-29 20:25: Train Epoch 4: 183/244 Loss: 0.155409
2022-12-29 20:25: Train Epoch 4: 187/244 Loss: 0.152796
2022-12-29 20:26: Train Epoch 4: 191/244 Loss: 0.167654
2022-12-29 20:26: Train Epoch 4: 195/244 Loss: 0.138296
2022-12-29 20:26: Train Epoch 4: 199/244 Loss: 0.180676
2022-12-29 20:26: Train Epoch 4: 203/244 Loss: 0.158731
2022-12-29 20:26: Train Epoch 4: 207/244 Loss: 0.150888
2022-12-29 20:27: Train Epoch 4: 211/244 Loss: 0.145170
2022-12-29 20:27: Train Epoch 4: 215/244 Loss: 0.212755
2022-12-29 20:27: Train Epoch 4: 219/244 Loss: 0.170124
2022-12-29 20:27: Train Epoch 4: 223/244 Loss: 0.206627
2022-12-29 20:27: Train Epoch 4: 227/244 Loss: 0.174114
2022-12-29 20:28: Train Epoch 4: 231/244 Loss: 0.204560
2022-12-29 20:28: Train Epoch 4: 235/244 Loss: 0.181059
2022-12-29 20:28: Train Epoch 4: 239/244 Loss: 0.209418
2022-12-29 20:28: Train Epoch 4: 243/244 Loss: 0.156648
2022-12-29 20:28: **********Train Epoch 4: averaged Loss: 0.166022 
2022-12-29 20:28: 
Epoch time elapsed: 662.6914682388306

2022-12-29 20:29: 
 metrics validation: {'precision': 0.9104991394148021, 'recall': 0.40692307692307694, 'f1-score': 0.5624667729930889, 'support': 1300, 'AUC': 0.8894183431952664, 'AUCPR': 0.8250185046426631, 'TP': 529, 'FP': 52, 'TN': 2548, 'FN': 771} 

2022-12-29 20:29: **********Val Epoch 4: average Loss: 0.260263
2022-12-29 20:30: 
 Testing metrics {'precision': 0.7584124245038827, 'recall': 0.7157980456026058, 'f1-score': 0.7364893171344784, 'support': 1228, 'AUC': 0.8709140813165127, 'AUCPR': 0.8184693471770149, 'TP': 879, 'FP': 280, 'TN': 2176, 'FN': 349} 

2022-12-29 20:30: Train Epoch 5: 3/244 Loss: 0.190201
2022-12-29 20:30: Train Epoch 5: 7/244 Loss: 0.153830
2022-12-29 20:30: Train Epoch 5: 11/244 Loss: 0.282204
2022-12-29 20:31: Train Epoch 5: 15/244 Loss: 0.173568
2022-12-29 20:31: Train Epoch 5: 19/244 Loss: 0.199309
2022-12-29 20:31: Train Epoch 5: 23/244 Loss: 0.229463
2022-12-29 20:31: Train Epoch 5: 27/244 Loss: 0.177078
2022-12-29 20:31: Train Epoch 5: 31/244 Loss: 0.204013
2022-12-29 20:31: Train Epoch 5: 35/244 Loss: 0.197970
2022-12-29 20:32: Train Epoch 5: 39/244 Loss: 0.135982
2022-12-29 20:32: Train Epoch 5: 43/244 Loss: 0.219185
2022-12-29 20:32: Train Epoch 5: 47/244 Loss: 0.188868
2022-12-29 20:32: Train Epoch 5: 51/244 Loss: 0.161693
2022-12-29 20:32: Train Epoch 5: 55/244 Loss: 0.157088
2022-12-29 20:33: Train Epoch 5: 59/244 Loss: 0.165340
2022-12-29 20:33: Train Epoch 5: 63/244 Loss: 0.180377
2022-12-29 20:33: Train Epoch 5: 67/244 Loss: 0.135220
2022-12-29 20:33: Train Epoch 5: 71/244 Loss: 0.168770
2022-12-29 20:33: Train Epoch 5: 75/244 Loss: 0.159244
2022-12-29 20:33: Train Epoch 5: 79/244 Loss: 0.132695
2022-12-29 20:34: Train Epoch 5: 83/244 Loss: 0.161495
2022-12-29 20:34: Train Epoch 5: 87/244 Loss: 0.177954
2022-12-29 20:34: Train Epoch 5: 91/244 Loss: 0.160831
2022-12-29 20:34: Train Epoch 5: 95/244 Loss: 0.177610
2022-12-29 20:34: Train Epoch 5: 99/244 Loss: 0.154252
2022-12-29 20:35: Train Epoch 5: 103/244 Loss: 0.156722
2022-12-29 20:35: Train Epoch 5: 107/244 Loss: 0.137509
2022-12-29 20:35: Train Epoch 5: 111/244 Loss: 0.165326
2022-12-29 20:35: Train Epoch 5: 115/244 Loss: 0.114222
2022-12-29 20:35: Train Epoch 5: 119/244 Loss: 0.138907
2022-12-29 20:35: Train Epoch 5: 123/244 Loss: 0.179419
2022-12-29 20:36: Train Epoch 5: 127/244 Loss: 0.147041
2022-12-29 20:36: Train Epoch 5: 131/244 Loss: 0.139852
2022-12-29 20:36: Train Epoch 5: 135/244 Loss: 0.157897
2022-12-29 20:36: Train Epoch 5: 139/244 Loss: 0.168969
2022-12-29 20:36: Train Epoch 5: 143/244 Loss: 0.182495
2022-12-29 20:37: Train Epoch 5: 147/244 Loss: 0.170042
2022-12-29 20:37: Train Epoch 5: 151/244 Loss: 0.161154
2022-12-29 20:37: Train Epoch 5: 155/244 Loss: 0.153690
2022-12-29 20:37: Train Epoch 5: 159/244 Loss: 0.149407
2022-12-29 20:37: Train Epoch 5: 163/244 Loss: 0.140704
2022-12-29 20:37: Train Epoch 5: 167/244 Loss: 0.160874
2022-12-29 20:38: Train Epoch 5: 171/244 Loss: 0.161526
2022-12-29 20:38: Train Epoch 5: 175/244 Loss: 0.175158
2022-12-29 20:38: Train Epoch 5: 179/244 Loss: 0.171274
2022-12-29 20:38: Train Epoch 5: 183/244 Loss: 0.183108
2022-12-29 20:38: Train Epoch 5: 187/244 Loss: 0.143381
2022-12-29 20:38: Train Epoch 5: 191/244 Loss: 0.165719
2022-12-29 20:39: Train Epoch 5: 195/244 Loss: 0.184044
2022-12-29 20:39: Train Epoch 5: 199/244 Loss: 0.119086
2022-12-29 20:39: Train Epoch 5: 203/244 Loss: 0.149306
2022-12-29 20:39: Train Epoch 5: 207/244 Loss: 0.156081
2022-12-29 20:39: Train Epoch 5: 211/244 Loss: 0.155760
2022-12-29 20:40: Train Epoch 5: 215/244 Loss: 0.164612
2022-12-29 20:40: Train Epoch 5: 219/244 Loss: 0.164645
2022-12-29 20:40: Train Epoch 5: 223/244 Loss: 0.168988
2022-12-29 20:40: Train Epoch 5: 227/244 Loss: 0.168744
2022-12-29 20:40: Train Epoch 5: 231/244 Loss: 0.178808
2022-12-29 20:41: Train Epoch 5: 235/244 Loss: 0.135859
2022-12-29 20:41: Train Epoch 5: 239/244 Loss: 0.161379
2022-12-29 20:41: Train Epoch 5: 243/244 Loss: 0.155404
2022-12-29 20:41: **********Train Epoch 5: averaged Loss: 0.166088 
2022-12-29 20:41: 
Epoch time elapsed: 666.1640980243683

2022-12-29 20:42: 
 metrics validation: {'precision': 0.8831385642737897, 'recall': 0.40692307692307694, 'f1-score': 0.5571353343865193, 'support': 1300, 'AUC': 0.8728420118343196, 'AUCPR': 0.803902466894723, 'TP': 529, 'FP': 70, 'TN': 2530, 'FN': 771} 

2022-12-29 20:42: **********Val Epoch 5: average Loss: 0.300123
2022-12-29 20:42: 
 Testing metrics {'precision': 0.7584124245038827, 'recall': 0.7157980456026058, 'f1-score': 0.7364893171344784, 'support': 1228, 'AUC': 0.8709140813165127, 'AUCPR': 0.8184693471770149, 'TP': 879, 'FP': 280, 'TN': 2176, 'FN': 349} 

Traceback (most recent call last):
  File "/home/joel.chacon/tmp/WildFire_GCN/Run_Model.py", line 197, in <module>
    trainer.train()
  File "/home/joel.chacon/tmp/WildFire_GCN/Trainer.py", line 132, in train
    epoch_time = time.time()
  File "/home/joel.chacon/tmp/WildFire_GCN/Trainer.py", line 95, in train_epoch
    
  File "/home/joel.chacon/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/joel.chacon/tmp/WildFire_GCN/fire_modules_GCN.py", line 205, in forward
    x, _ = self.encoder(x, self.node_embeddings) #B, T, N, hidden_dim
  File "/home/joel.chacon/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/joel.chacon/tmp/WildFire_GCN/fire_modules_GCN.py", line 138, in forward
    state = self.cell_list[layer_idx](cur_layer_input[:, t, :, :], state, cur_layer_input, node_embeddings)
  File "/home/joel.chacon/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/joel.chacon/tmp/WildFire_GCN/fire_modules_GCN.py", line 102, in forward
    z_r = torch.sigmoid(self.gate(input_and_state, x_full, node_embeddings))
  File "/home/joel.chacon/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/joel.chacon/tmp/WildFire_GCN/fire_modules_GCN.py", line 64, in forward
    x_g = torch.einsum("knm,bmc->bknc", supports, x) #B, link_len, N, dim_in : on N
  File "/home/joel.chacon/.local/lib/python3.6/site-packages/torch/functional.py", line 327, in einsum
    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]
KeyboardInterrupt
