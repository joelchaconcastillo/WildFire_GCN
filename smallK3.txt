2023-01-06 18:50: log dir: /home/joel.chacon/tmp/WildFire_GCNv2/experiments/2020/2023010618505660176169386
2023-01-06 18:50: Experiment log path in: /home/joel.chacon/tmp/WildFire_GCNv2/experiments/2020/2023010618505660176169386
2023-01-06 18:50: Argument: Namespace(batch_size=256, clc='vec', cuda=True, dataset='2020', dataset_root='/home/joel.chacon/tmp/datasets_grl/', debug=False, default_graph=True, device='cpu', dynamic_features=['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh'], early_stop=True, early_stop_patience=8, embed_dim=64, epochs=30, grad_norm=False, horizon=1, input_dim=25, lag=10, link_len=3, log_dir='/home/joel.chacon/tmp/WildFire_GCNv2/experiments/2020/2023010618505660176169386', log_step=1, loss_func='nllloss', lr_decay=True, lr_decay_rate=0.1, lr_decay_step='20', lr_init=0.0001, max_grad_norm=5, minbatch_size=64, mode='train', model='fire_GCN', nan_fill=-1.0, num_layers=1, num_nodes=625, num_workers=12, output_dim=2, patch_height=25, patch_width=25, persistent_workers=True, pin_memory=True, plot=False, positive_weight=0.5, prefetch_factor=2, real_value=True, rnn_units=48, seed=10000, static_features=['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density'], teacher_forcing=False, weight_decay=0.0, window_len=10)
2023-01-06 18:50: Argument batch_size: 256
2023-01-06 18:50: Argument clc: 'vec'
2023-01-06 18:50: Argument cuda: True
2023-01-06 18:50: Argument dataset: '2020'
2023-01-06 18:50: Argument dataset_root: '/home/joel.chacon/tmp/datasets_grl/'
2023-01-06 18:50: Argument debug: False
2023-01-06 18:50: Argument default_graph: True
2023-01-06 18:50: Argument device: 'cpu'
2023-01-06 18:50: Argument dynamic_features: ['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh']
2023-01-06 18:50: Argument early_stop: True
2023-01-06 18:50: Argument early_stop_patience: 8
2023-01-06 18:50: Argument embed_dim: 64
2023-01-06 18:50: Argument epochs: 30
2023-01-06 18:50: Argument grad_norm: False
2023-01-06 18:50: Argument horizon: 1
2023-01-06 18:50: Argument input_dim: 25
2023-01-06 18:50: Argument lag: 10
2023-01-06 18:50: Argument link_len: 3
2023-01-06 18:50: Argument log_dir: '/home/joel.chacon/tmp/WildFire_GCNv2/experiments/2020/2023010618505660176169386'
2023-01-06 18:50: Argument log_step: 1
2023-01-06 18:50: Argument loss_func: 'nllloss'
2023-01-06 18:50: Argument lr_decay: True
2023-01-06 18:50: Argument lr_decay_rate: 0.1
2023-01-06 18:50: Argument lr_decay_step: '20'
2023-01-06 18:50: Argument lr_init: 0.0001
2023-01-06 18:50: Argument max_grad_norm: 5
2023-01-06 18:50: Argument minbatch_size: 64
2023-01-06 18:50: Argument mode: 'train'
2023-01-06 18:50: Argument model: 'fire_GCN'
2023-01-06 18:50: Argument nan_fill: -1.0
2023-01-06 18:50: Argument num_layers: 1
2023-01-06 18:50: Argument num_nodes: 625
2023-01-06 18:50: Argument num_workers: 12
2023-01-06 18:50: Argument output_dim: 2
2023-01-06 18:50: Argument patch_height: 25
2023-01-06 18:50: Argument patch_width: 25
2023-01-06 18:50: Argument persistent_workers: True
2023-01-06 18:50: Argument pin_memory: True
2023-01-06 18:50: Argument plot: False
2023-01-06 18:50: Argument positive_weight: 0.5
2023-01-06 18:50: Argument prefetch_factor: 2
2023-01-06 18:50: Argument real_value: True
2023-01-06 18:50: Argument rnn_units: 48
2023-01-06 18:50: Argument seed: 10000
2023-01-06 18:50: Argument static_features: ['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density']
2023-01-06 18:50: Argument teacher_forcing: False
2023-01-06 18:50: Argument weight_decay: 0.0
2023-01-06 18:50: Argument window_len: 10
++++++++++++++
2020_fire_GCN.conf
++++++++++++++
*****************Model Parameter*****************
node_embeddings torch.Size([625, 64]) True
ln1.weight torch.Size([25]) True
ln1.bias torch.Size([25]) True
encoder.cell_list.0.gate.weights_pool torch.Size([64, 3, 73, 32]) True
encoder.cell_list.0.gate.weights_pool_adj torch.Size([64, 3, 73, 32]) True
encoder.cell_list.0.gate.weights_window torch.Size([64, 1, 32]) True
encoder.cell_list.0.gate.bias_pool torch.Size([64, 96]) True
encoder.cell_list.0.gate.bias_pool_adj torch.Size([64, 96]) True
encoder.cell_list.0.gate.T torch.Size([10]) True
encoder.cell_list.0.update.weights_pool torch.Size([64, 3, 73, 16]) True
encoder.cell_list.0.update.weights_pool_adj torch.Size([64, 3, 73, 16]) True
encoder.cell_list.0.update.weights_window torch.Size([64, 1, 16]) True
encoder.cell_list.0.update.bias_pool torch.Size([64, 48]) True
encoder.cell_list.0.update.bias_pool_adj torch.Size([64, 48]) True
encoder.cell_list.0.update.T torch.Size([10]) True
fc1.weight torch.Size([2, 30000]) True
fc1.bias torch.Size([2]) True
Total params num: 1467112
*****************Finish Parameter****************
Positives: 500 / Negatives: 1000
Dataset length 1500
Positives: 500 / Negatives: 1000
Dataset length 1500
Positives: 500 / Negatives: 1000
Dataset length 1500
Positives: 500 / Negatives: 1000
Dataset length 1500
Applying learning rate decay.
Creat Log File in:  /home/joel.chacon/tmp/WildFire_GCNv2/experiments/2020/2023010618505660176169386/run.log
2023-01-06 18:51: Train Epoch 1: 3/24 Loss: 0.383874
2023-01-06 18:51: Train Epoch 1: 7/24 Loss: 0.524325
2023-01-06 18:51: Train Epoch 1: 11/24 Loss: 0.365627
2023-01-06 18:52: Train Epoch 1: 15/24 Loss: 0.269080
2023-01-06 18:52: Train Epoch 1: 19/24 Loss: 0.315101
2023-01-06 18:52: Train Epoch 1: 23/24 Loss: 0.261895
2023-01-06 18:52: **********Train Epoch 1: averaged Loss: 0.353317 
2023-01-06 18:52: 
Epoch time elapsed: 93.27832078933716

2023-01-06 18:53: 
 metrics validation: {'precision': 0.5265306122448979, 'recall': 0.774, 'f1-score': 0.6267206477732793, 'support': 500, 'AUC': 0.7752779999999999, 'AUCPR': 0.6463559586176653, 'TP': 387, 'FP': 348, 'TN': 652, 'FN': 113} 

2023-01-06 18:53: **********Val Epoch 1: average Loss: 0.286922
2023-01-06 18:53: *********************************Current best model saved!
2023-01-06 18:53: 
 Testing metrics {'precision': 0.5606469002695418, 'recall': 0.832, 'f1-score': 0.6698872785829308, 'support': 500, 'AUC': 0.85936, 'AUCPR': 0.7750574763720892, 'TP': 416, 'FP': 326, 'TN': 674, 'FN': 84} 

2023-01-06 18:54: 
 Testing metrics {'precision': 0.6895043731778425, 'recall': 0.946, 'f1-score': 0.7976391231028667, 'support': 500, 'AUC': 0.9549129999999999, 'AUCPR': 0.9315879919431734, 'TP': 473, 'FP': 213, 'TN': 787, 'FN': 27} 

2023-01-06 18:54: Train Epoch 2: 3/24 Loss: 0.285759
2023-01-06 18:54: Train Epoch 2: 7/24 Loss: 0.278676
2023-01-06 18:55: Train Epoch 2: 11/24 Loss: 0.226347
2023-01-06 18:55: Train Epoch 2: 15/24 Loss: 0.268832
2023-01-06 18:55: Train Epoch 2: 19/24 Loss: 0.245704
2023-01-06 18:55: Train Epoch 2: 23/24 Loss: 0.200371
2023-01-06 18:55: **********Train Epoch 2: averaged Loss: 0.250948 
2023-01-06 18:55: 
Epoch time elapsed: 97.79182457923889

2023-01-06 18:56: 
 metrics validation: {'precision': 0.6391509433962265, 'recall': 0.542, 'f1-score': 0.5865800865800865, 'support': 500, 'AUC': 0.7861699999999999, 'AUCPR': 0.657972271322109, 'TP': 271, 'FP': 153, 'TN': 847, 'FN': 229} 

2023-01-06 18:56: **********Val Epoch 2: average Loss: 0.262434
2023-01-06 18:56: *********************************Current best model saved!
2023-01-06 18:57: 
 Testing metrics {'precision': 0.7642276422764228, 'recall': 0.752, 'f1-score': 0.7580645161290323, 'support': 500, 'AUC': 0.864144, 'AUCPR': 0.7834266202080614, 'TP': 376, 'FP': 116, 'TN': 884, 'FN': 124} 

2023-01-06 18:57: 
 Testing metrics {'precision': 0.8514285714285714, 'recall': 0.894, 'f1-score': 0.8721951219512195, 'support': 500, 'AUC': 0.9597039999999999, 'AUCPR': 0.9394992603615389, 'TP': 447, 'FP': 78, 'TN': 922, 'FN': 53} 

2023-01-06 18:57: Train Epoch 3: 3/24 Loss: 0.212767
2023-01-06 18:58: Train Epoch 3: 7/24 Loss: 0.250481
2023-01-06 18:58: Train Epoch 3: 11/24 Loss: 0.239740
2023-01-06 18:58: Train Epoch 3: 15/24 Loss: 0.265162
2023-01-06 18:58: Train Epoch 3: 19/24 Loss: 0.229170
2023-01-06 18:59: Train Epoch 3: 23/24 Loss: 0.190089
2023-01-06 18:59: **********Train Epoch 3: averaged Loss: 0.231235 
2023-01-06 18:59: 
Epoch time elapsed: 96.58540201187134

2023-01-06 18:59: 
 metrics validation: {'precision': 0.7104477611940299, 'recall': 0.476, 'f1-score': 0.5700598802395209, 'support': 500, 'AUC': 0.7928359999999999, 'AUCPR': 0.670606961568881, 'TP': 238, 'FP': 97, 'TN': 903, 'FN': 262} 

2023-01-06 18:59: **********Val Epoch 3: average Loss: 0.276429
2023-01-06 19:00: Train Epoch 4: 3/24 Loss: 0.219139
2023-01-06 19:00: Train Epoch 4: 7/24 Loss: 0.219923
2023-01-06 19:00: Train Epoch 4: 11/24 Loss: 0.217313
2023-01-06 19:00: Train Epoch 4: 15/24 Loss: 0.218604
2023-01-06 19:01: Train Epoch 4: 19/24 Loss: 0.237673
2023-01-06 19:01: Train Epoch 4: 23/24 Loss: 0.191889
2023-01-06 19:01: **********Train Epoch 4: averaged Loss: 0.217423 
2023-01-06 19:01: 
Epoch time elapsed: 95.68257355690002

2023-01-06 19:01: 
 metrics validation: {'precision': 0.6723300970873787, 'recall': 0.554, 'f1-score': 0.6074561403508772, 'support': 500, 'AUC': 0.795668, 'AUCPR': 0.676825517059748, 'TP': 277, 'FP': 135, 'TN': 865, 'FN': 223} 

2023-01-06 19:01: **********Val Epoch 4: average Loss: 0.261310
2023-01-06 19:01: *********************************Current best model saved!
2023-01-06 19:02: 
 Testing metrics {'precision': 0.7918454935622318, 'recall': 0.738, 'f1-score': 0.763975155279503, 'support': 500, 'AUC': 0.867776, 'AUCPR': 0.7914787011813802, 'TP': 369, 'FP': 97, 'TN': 903, 'FN': 131} 

2023-01-06 19:02: 
 Testing metrics {'precision': 0.8624031007751938, 'recall': 0.89, 'f1-score': 0.8759842519685039, 'support': 500, 'AUC': 0.96278, 'AUCPR': 0.9434091811478409, 'TP': 445, 'FP': 71, 'TN': 929, 'FN': 55} 

2023-01-06 19:03: Train Epoch 5: 3/24 Loss: 0.215417
2023-01-06 19:03: Train Epoch 5: 7/24 Loss: 0.217923
2023-01-06 19:03: Train Epoch 5: 11/24 Loss: 0.214684
2023-01-06 19:04: Train Epoch 5: 15/24 Loss: 0.206254
2023-01-06 19:04: Train Epoch 5: 19/24 Loss: 0.221879
2023-01-06 19:04: Train Epoch 5: 23/24 Loss: 0.200313
2023-01-06 19:04: **********Train Epoch 5: averaged Loss: 0.212745 
2023-01-06 19:04: 
Epoch time elapsed: 94.04941058158875

2023-01-06 19:05: 
 metrics validation: {'precision': 0.6805251641137856, 'recall': 0.622, 'f1-score': 0.6499477533960293, 'support': 500, 'AUC': 0.802674, 'AUCPR': 0.6915451598633744, 'TP': 311, 'FP': 146, 'TN': 854, 'FN': 189} 

2023-01-06 19:05: **********Val Epoch 5: average Loss: 0.255748
2023-01-06 19:05: *********************************Current best model saved!
2023-01-06 19:05: 
 Testing metrics {'precision': 0.7867803837953091, 'recall': 0.738, 'f1-score': 0.7616099071207431, 'support': 500, 'AUC': 0.8686079999999999, 'AUCPR': 0.7943165934468387, 'TP': 369, 'FP': 100, 'TN': 900, 'FN': 131} 

2023-01-06 19:06: 
 Testing metrics {'precision': 0.8566037735849057, 'recall': 0.908, 'f1-score': 0.8815533980582525, 'support': 500, 'AUC': 0.9643919999999999, 'AUCPR': 0.9449109520119243, 'TP': 454, 'FP': 76, 'TN': 924, 'FN': 46} 

2023-01-06 19:06: Train Epoch 6: 3/24 Loss: 0.217596
2023-01-06 19:06: Train Epoch 6: 7/24 Loss: 0.213254
2023-01-06 19:06: Train Epoch 6: 11/24 Loss: 0.203870
2023-01-06 19:07: Train Epoch 6: 15/24 Loss: 0.203705
2023-01-06 19:07: Train Epoch 6: 19/24 Loss: 0.213597
2023-01-06 19:07: Train Epoch 6: 23/24 Loss: 0.174013
2023-01-06 19:07: **********Train Epoch 6: averaged Loss: 0.204339 
2023-01-06 19:07: 
Epoch time elapsed: 94.42943286895752

2023-01-06 19:08: 
 metrics validation: {'precision': 0.707983193277311, 'recall': 0.674, 'f1-score': 0.6905737704918034, 'support': 500, 'AUC': 0.808886, 'AUCPR': 0.7054282457462868, 'TP': 337, 'FP': 139, 'TN': 861, 'FN': 163} 

2023-01-06 19:08: **********Val Epoch 6: average Loss: 0.252787
2023-01-06 19:08: *********************************Current best model saved!
2023-01-06 19:08: 
 Testing metrics {'precision': 0.7908496732026143, 'recall': 0.726, 'f1-score': 0.7570385818561002, 'support': 500, 'AUC': 0.868064, 'AUCPR': 0.794630847400918, 'TP': 363, 'FP': 96, 'TN': 904, 'FN': 137} 

2023-01-06 19:09: 
 Testing metrics {'precision': 0.8587786259541985, 'recall': 0.9, 'f1-score': 0.87890625, 'support': 500, 'AUC': 0.9651879999999999, 'AUCPR': 0.9439887882140241, 'TP': 450, 'FP': 74, 'TN': 926, 'FN': 50} 

2023-01-06 19:09: Train Epoch 7: 3/24 Loss: 0.199550
2023-01-06 19:09: Train Epoch 7: 7/24 Loss: 0.201534
2023-01-06 19:10: Train Epoch 7: 11/24 Loss: 0.183997
2023-01-06 19:10: Train Epoch 7: 15/24 Loss: 0.207170
2023-01-06 19:10: Train Epoch 7: 19/24 Loss: 0.213105
2023-01-06 19:10: Train Epoch 7: 23/24 Loss: 0.171944
2023-01-06 19:10: **********Train Epoch 7: averaged Loss: 0.196217 
2023-01-06 19:10: 
Epoch time elapsed: 97.46657061576843

2023-01-06 19:11: 
 metrics validation: {'precision': 0.7527173913043478, 'recall': 0.554, 'f1-score': 0.6382488479262672, 'support': 500, 'AUC': 0.811202, 'AUCPR': 0.7142677131552851, 'TP': 277, 'FP': 91, 'TN': 909, 'FN': 223} 

2023-01-06 19:11: **********Val Epoch 7: average Loss: 0.258455
2023-01-06 19:11: Train Epoch 8: 3/24 Loss: 0.244070
2023-01-06 19:11: Train Epoch 8: 7/24 Loss: 0.196497
2023-01-06 19:12: Train Epoch 8: 11/24 Loss: 0.220577
2023-01-06 19:12: Train Epoch 8: 15/24 Loss: 0.213707
2023-01-06 19:12: Train Epoch 8: 19/24 Loss: 0.192188
2023-01-06 19:13: Train Epoch 8: 23/24 Loss: 0.178642
2023-01-06 19:13: **********Train Epoch 8: averaged Loss: 0.207614 
2023-01-06 19:13: 
Epoch time elapsed: 94.39258122444153

2023-01-06 19:13: 
 metrics validation: {'precision': 0.6927710843373494, 'recall': 0.69, 'f1-score': 0.691382765531062, 'support': 500, 'AUC': 0.81035, 'AUCPR': 0.7130396342279626, 'TP': 345, 'FP': 153, 'TN': 847, 'FN': 155} 

2023-01-06 19:13: **********Val Epoch 8: average Loss: 0.251734
2023-01-06 19:13: *********************************Current best model saved!
2023-01-06 19:14: 
 Testing metrics {'precision': 0.7745901639344263, 'recall': 0.756, 'f1-score': 0.7651821862348178, 'support': 500, 'AUC': 0.8685540000000002, 'AUCPR': 0.796520114035173, 'TP': 378, 'FP': 110, 'TN': 890, 'FN': 122} 

2023-01-06 19:14: 
 Testing metrics {'precision': 0.84375, 'recall': 0.918, 'f1-score': 0.8793103448275862, 'support': 500, 'AUC': 0.9665980000000001, 'AUCPR': 0.9454113726581592, 'TP': 459, 'FP': 85, 'TN': 915, 'FN': 41} 

2023-01-06 19:14: Train Epoch 9: 3/24 Loss: 0.208524
2023-01-06 19:15: Train Epoch 9: 7/24 Loss: 0.192487
2023-01-06 19:15: Train Epoch 9: 11/24 Loss: 0.199276
2023-01-06 19:15: Train Epoch 9: 15/24 Loss: 0.221338
2023-01-06 19:16: Train Epoch 9: 19/24 Loss: 0.208571
2023-01-06 19:16: Train Epoch 9: 23/24 Loss: 0.176466
2023-01-06 19:16: **********Train Epoch 9: averaged Loss: 0.201110 
2023-01-06 19:16: 
Epoch time elapsed: 99.955894947052

2023-01-06 19:16: 
 metrics validation: {'precision': 0.7361477572559367, 'recall': 0.558, 'f1-score': 0.6348122866894198, 'support': 500, 'AUC': 0.8081180000000001, 'AUCPR': 0.7177646610905175, 'TP': 279, 'FP': 100, 'TN': 900, 'FN': 221} 

2023-01-06 19:16: **********Val Epoch 9: average Loss: 0.258302
2023-01-06 19:17: Train Epoch 10: 3/24 Loss: 0.197548
2023-01-06 19:17: Train Epoch 10: 7/24 Loss: 0.193116
2023-01-06 19:17: Train Epoch 10: 11/24 Loss: 0.189216
2023-01-06 19:17: Train Epoch 10: 15/24 Loss: 0.198306
2023-01-06 19:18: Train Epoch 10: 19/24 Loss: 0.242473
2023-01-06 19:18: Train Epoch 10: 23/24 Loss: 0.197863
2023-01-06 19:18: **********Train Epoch 10: averaged Loss: 0.203087 
2023-01-06 19:18: 
Epoch time elapsed: 93.97814893722534

2023-01-06 19:18: 
 metrics validation: {'precision': 0.7058823529411765, 'recall': 0.624, 'f1-score': 0.6624203821656052, 'support': 500, 'AUC': 0.807676, 'AUCPR': 0.7152871147202025, 'TP': 312, 'FP': 130, 'TN': 870, 'FN': 188} 

2023-01-06 19:18: **********Val Epoch 10: average Loss: 0.256317
2023-01-06 19:19: Train Epoch 11: 3/24 Loss: 0.211783
2023-01-06 19:19: Train Epoch 11: 7/24 Loss: 0.244797
2023-01-06 19:19: Train Epoch 11: 11/24 Loss: 0.205759
2023-01-06 19:19: Train Epoch 11: 15/24 Loss: 0.180709
2023-01-06 19:20: Train Epoch 11: 19/24 Loss: 0.217453
2023-01-06 19:20: Train Epoch 11: 23/24 Loss: 0.151700
2023-01-06 19:20: **********Train Epoch 11: averaged Loss: 0.202034 
2023-01-06 19:20: 
Epoch time elapsed: 93.64134573936462

2023-01-06 19:21: 
 metrics validation: {'precision': 0.6991869918699187, 'recall': 0.688, 'f1-score': 0.6935483870967742, 'support': 500, 'AUC': 0.8123979999999998, 'AUCPR': 0.7199236225675029, 'TP': 344, 'FP': 148, 'TN': 852, 'FN': 156} 

2023-01-06 19:21: **********Val Epoch 11: average Loss: 0.249505
2023-01-06 19:21: *********************************Current best model saved!
2023-01-06 19:21: 
 Testing metrics {'precision': 0.7791666666666667, 'recall': 0.748, 'f1-score': 0.763265306122449, 'support': 500, 'AUC': 0.870474, 'AUCPR': 0.7999799606407119, 'TP': 374, 'FP': 106, 'TN': 894, 'FN': 126} 

2023-01-06 19:22: 
 Testing metrics {'precision': 0.84375, 'recall': 0.918, 'f1-score': 0.8793103448275862, 'support': 500, 'AUC': 0.968034, 'AUCPR': 0.9481590987124323, 'TP': 459, 'FP': 85, 'TN': 915, 'FN': 41} 

2023-01-06 19:22: Train Epoch 12: 3/24 Loss: 0.199950
2023-01-06 19:22: Train Epoch 12: 7/24 Loss: 0.216909
2023-01-06 19:22: Train Epoch 12: 11/24 Loss: 0.209717
2023-01-06 19:23: Train Epoch 12: 15/24 Loss: 0.212346
2023-01-06 19:23: Train Epoch 12: 19/24 Loss: 0.205609
2023-01-06 19:23: Train Epoch 12: 23/24 Loss: 0.176260
2023-01-06 19:23: **********Train Epoch 12: averaged Loss: 0.203465 
2023-01-06 19:23: 
Epoch time elapsed: 97.83245396614075

2023-01-06 19:24: 
 metrics validation: {'precision': 0.7602339181286549, 'recall': 0.52, 'f1-score': 0.6175771971496438, 'support': 500, 'AUC': 0.811326, 'AUCPR': 0.7216837160079825, 'TP': 260, 'FP': 82, 'TN': 918, 'FN': 240} 

2023-01-06 19:24: **********Val Epoch 12: average Loss: 0.260078
2023-01-06 19:24: Train Epoch 13: 3/24 Loss: 0.190793
2023-01-06 19:24: Train Epoch 13: 7/24 Loss: 0.186672
2023-01-06 19:25: Train Epoch 13: 11/24 Loss: 0.192629
2023-01-06 19:25: Train Epoch 13: 15/24 Loss: 0.232612
2023-01-06 19:25: Train Epoch 13: 19/24 Loss: 0.208409
2023-01-06 19:25: Train Epoch 13: 23/24 Loss: 0.186987
2023-01-06 19:25: **********Train Epoch 13: averaged Loss: 0.199684 
2023-01-06 19:25: 
Epoch time elapsed: 90.7006721496582

2023-01-06 19:26: 
 metrics validation: {'precision': 0.7320099255583127, 'recall': 0.59, 'f1-score': 0.6533776301218162, 'support': 500, 'AUC': 0.80953, 'AUCPR': 0.7196899003496111, 'TP': 295, 'FP': 108, 'TN': 892, 'FN': 205} 

2023-01-06 19:26: **********Val Epoch 13: average Loss: 0.257138
2023-01-06 19:26: Train Epoch 14: 3/24 Loss: 0.207346
2023-01-06 19:26: Train Epoch 14: 7/24 Loss: 0.214470
2023-01-06 19:27: Train Epoch 14: 11/24 Loss: 0.196317
2023-01-06 19:27: Train Epoch 14: 15/24 Loss: 0.202251
2023-01-06 19:27: Train Epoch 14: 19/24 Loss: 0.183316
2023-01-06 19:27: Train Epoch 14: 23/24 Loss: 0.190070
2023-01-06 19:27: **********Train Epoch 14: averaged Loss: 0.198962 
2023-01-06 19:27: 
Epoch time elapsed: 93.46825122833252

2023-01-06 19:28: 
 metrics validation: {'precision': 0.7685185185185185, 'recall': 0.498, 'f1-score': 0.604368932038835, 'support': 500, 'AUC': 0.808228, 'AUCPR': 0.7194223548904554, 'TP': 249, 'FP': 75, 'TN': 925, 'FN': 251} 

2023-01-06 19:28: **********Val Epoch 14: average Loss: 0.263436
2023-01-06 19:28: Train Epoch 15: 3/24 Loss: 0.205211
2023-01-06 19:28: Train Epoch 15: 7/24 Loss: 0.199985
2023-01-06 19:29: Train Epoch 15: 11/24 Loss: 0.187146
2023-01-06 19:29: Train Epoch 15: 15/24 Loss: 0.219777
2023-01-06 19:29: Train Epoch 15: 19/24 Loss: 0.200071
2023-01-06 19:29: Train Epoch 15: 23/24 Loss: 0.181045
2023-01-06 19:29: **********Train Epoch 15: averaged Loss: 0.198872 
2023-01-06 19:29: 
Epoch time elapsed: 87.87322545051575

2023-01-06 19:30: 
 metrics validation: {'precision': 0.6915322580645161, 'recall': 0.686, 'f1-score': 0.6887550200803213, 'support': 500, 'AUC': 0.80853, 'AUCPR': 0.7182565800207568, 'TP': 343, 'FP': 153, 'TN': 847, 'FN': 157} 

2023-01-06 19:30: **********Val Epoch 15: average Loss: 0.255000
2023-01-06 19:30: Train Epoch 16: 3/24 Loss: 0.208530
2023-01-06 19:30: Train Epoch 16: 7/24 Loss: 0.205120
2023-01-06 19:31: Train Epoch 16: 11/24 Loss: 0.196694
2023-01-06 19:31: Train Epoch 16: 15/24 Loss: 0.180258
2023-01-06 19:31: Train Epoch 16: 19/24 Loss: 0.198540
2023-01-06 19:31: Train Epoch 16: 23/24 Loss: 0.156261
2023-01-06 19:31: **********Train Epoch 16: averaged Loss: 0.190901 
2023-01-06 19:31: 
Epoch time elapsed: 89.14436769485474

2023-01-06 19:32: 
 metrics validation: {'precision': 0.6884920634920635, 'recall': 0.694, 'f1-score': 0.6912350597609562, 'support': 500, 'AUC': 0.807602, 'AUCPR': 0.7184472255626482, 'TP': 347, 'FP': 157, 'TN': 843, 'FN': 153} 

2023-01-06 19:32: **********Val Epoch 16: average Loss: 0.255232
2023-01-06 19:32: Train Epoch 17: 3/24 Loss: 0.218644
2023-01-06 19:32: Train Epoch 17: 7/24 Loss: 0.210915
2023-01-06 19:33: Train Epoch 17: 11/24 Loss: 0.180958
2023-01-06 19:33: Train Epoch 17: 15/24 Loss: 0.217017
2023-01-06 19:33: Train Epoch 17: 19/24 Loss: 0.188111
2023-01-06 19:33: Train Epoch 17: 23/24 Loss: 0.163938
2023-01-06 19:33: **********Train Epoch 17: averaged Loss: 0.196597 
2023-01-06 19:33: 
Epoch time elapsed: 91.7109112739563

2023-01-06 19:34: 
 metrics validation: {'precision': 0.7923875432525952, 'recall': 0.458, 'f1-score': 0.5804816223067173, 'support': 500, 'AUC': 0.804988, 'AUCPR': 0.7170318483696936, 'TP': 229, 'FP': 60, 'TN': 940, 'FN': 271} 

2023-01-06 19:34: **********Val Epoch 17: average Loss: 0.275649
2023-01-06 19:34: Train Epoch 18: 3/24 Loss: 0.207123
2023-01-06 19:34: Train Epoch 18: 7/24 Loss: 0.235474
2023-01-06 19:35: Train Epoch 18: 11/24 Loss: 0.207993
2023-01-06 19:35: Train Epoch 18: 15/24 Loss: 0.183288
2023-01-06 19:35: Train Epoch 18: 19/24 Loss: 0.195181
2023-01-06 19:36: Train Epoch 18: 23/24 Loss: 0.186461
2023-01-06 19:36: **********Train Epoch 18: averaged Loss: 0.202587 
2023-01-06 19:36: 
Epoch time elapsed: 93.61288475990295

2023-01-06 19:36: 
 metrics validation: {'precision': 0.7182448036951501, 'recall': 0.622, 'f1-score': 0.6666666666666666, 'support': 500, 'AUC': 0.8095180000000001, 'AUCPR': 0.720209628342159, 'TP': 311, 'FP': 122, 'TN': 878, 'FN': 189} 

2023-01-06 19:36: **********Val Epoch 18: average Loss: 0.255552
2023-01-06 19:36: Train Epoch 19: 3/24 Loss: 0.218506
2023-01-06 19:37: Train Epoch 19: 7/24 Loss: 0.220628
2023-01-06 19:37: Train Epoch 19: 11/24 Loss: 0.186750
2023-01-06 19:37: Train Epoch 19: 15/24 Loss: 0.231450
2023-01-06 19:37: Train Epoch 19: 19/24 Loss: 0.225010
2023-01-06 19:38: Train Epoch 19: 23/24 Loss: 0.173732
2023-01-06 19:38: **********Train Epoch 19: averaged Loss: 0.209346 
2023-01-06 19:38: 
Epoch time elapsed: 89.64795303344727

2023-01-06 19:38: 
 metrics validation: {'precision': 0.694, 'recall': 0.694, 'f1-score': 0.694, 'support': 500, 'AUC': 0.8105899999999999, 'AUCPR': 0.721460221480421, 'TP': 347, 'FP': 153, 'TN': 847, 'FN': 153} 

2023-01-06 19:38: **********Val Epoch 19: average Loss: 0.251890
2023-01-06 19:38: Validation performance didn't improve for 8 epochs. Training stops.
2023-01-06 19:38: Total training time: 47.5970min, best loss: 0.249505
2023-01-06 19:38: Saving current best model to /home/joel.chacon/tmp/WildFire_GCNv2/experiments/2020/2023010618505660176169386/best_model.pth
2023-01-06 19:39: 
 Testing metrics {'precision': 0.7791666666666667, 'recall': 0.748, 'f1-score': 0.763265306122449, 'support': 500, 'AUC': 0.870474, 'AUCPR': 0.7999799606407119, 'TP': 374, 'FP': 106, 'TN': 894, 'FN': 126} 

2023-01-06 19:39: 
 Testing metrics {'precision': 0.84375, 'recall': 0.918, 'f1-score': 0.8793103448275862, 'support': 500, 'AUC': 0.968034, 'AUCPR': 0.9481590987124323, 'TP': 459, 'FP': 85, 'TN': 915, 'FN': 41} 

