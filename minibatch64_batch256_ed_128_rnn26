2022-12-29 17:48: log dir: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122917483451445369118
2022-12-29 17:48: Experiment log path in: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122917483451445369118
2022-12-29 17:48: Argument: Namespace(batch_size=256, clc='vec', cuda=True, dataset='2020', dataset_root='/home/joel.chacon/tmp/datasets_grl/', debug=False, default_graph=True, device='cpu', dynamic_features=['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh'], early_stop=True, early_stop_patience=8, embed_dim=128, epochs=30, grad_norm=False, horizon=1, input_dim=25, lag=10, link_len=2, log_dir='/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122917483451445369118', log_step=1, loss_func='nllloss', lr_decay=True, lr_decay_rate=0.1, lr_decay_step='15, 20', lr_init=0.0005, max_grad_norm=5, minbatch_size=64, mode='train', model='fire_GCN', nan_fill=0.5, num_layers=2, num_nodes=625, num_workers=20, output_dim=2, patch_height=25, patch_width=25, persistent_workers=True, pin_memory=True, plot=False, positive_weight=0.5, prefetch_factor=2, real_value=True, rnn_units=16, seed=10000, static_features=['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density'], teacher_forcing=False, weight_decay=0.0, window_len=10)
2022-12-29 17:48: Argument batch_size: 256
2022-12-29 17:48: Argument clc: 'vec'
2022-12-29 17:48: Argument cuda: True
2022-12-29 17:48: Argument dataset: '2020'
2022-12-29 17:48: Argument dataset_root: '/home/joel.chacon/tmp/datasets_grl/'
2022-12-29 17:48: Argument debug: False
2022-12-29 17:48: Argument default_graph: True
2022-12-29 17:48: Argument device: 'cpu'
2022-12-29 17:48: Argument dynamic_features: ['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh']
2022-12-29 17:48: Argument early_stop: True
2022-12-29 17:48: Argument early_stop_patience: 8
2022-12-29 17:48: Argument embed_dim: 128
2022-12-29 17:48: Argument epochs: 30
2022-12-29 17:48: Argument grad_norm: False
2022-12-29 17:48: Argument horizon: 1
2022-12-29 17:48: Argument input_dim: 25
2022-12-29 17:48: Argument lag: 10
2022-12-29 17:48: Argument link_len: 2
2022-12-29 17:48: Argument log_dir: '/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122917483451445369118'
2022-12-29 17:48: Argument log_step: 1
2022-12-29 17:48: Argument loss_func: 'nllloss'
2022-12-29 17:48: Argument lr_decay: True
2022-12-29 17:48: Argument lr_decay_rate: 0.1
2022-12-29 17:48: Argument lr_decay_step: '15, 20'
2022-12-29 17:48: Argument lr_init: 0.0005
2022-12-29 17:48: Argument max_grad_norm: 5
2022-12-29 17:48: Argument minbatch_size: 64
2022-12-29 17:48: Argument mode: 'train'
2022-12-29 17:48: Argument model: 'fire_GCN'
2022-12-29 17:48: Argument nan_fill: 0.5
2022-12-29 17:48: Argument num_layers: 2
2022-12-29 17:48: Argument num_nodes: 625
2022-12-29 17:48: Argument num_workers: 20
2022-12-29 17:48: Argument output_dim: 2
2022-12-29 17:48: Argument patch_height: 25
2022-12-29 17:48: Argument patch_width: 25
2022-12-29 17:48: Argument persistent_workers: True
2022-12-29 17:48: Argument pin_memory: True
2022-12-29 17:48: Argument plot: False
2022-12-29 17:48: Argument positive_weight: 0.5
2022-12-29 17:48: Argument prefetch_factor: 2
2022-12-29 17:48: Argument real_value: True
2022-12-29 17:48: Argument rnn_units: 16
2022-12-29 17:48: Argument seed: 10000
2022-12-29 17:48: Argument static_features: ['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density']
2022-12-29 17:48: Argument teacher_forcing: False
2022-12-29 17:48: Argument weight_decay: 0.0
2022-12-29 17:48: Argument window_len: 10
++++++++++++++
2020_fire_GCN.conf
++++++++++++++
*****************Model Parameter*****************
node_embeddings torch.Size([625, 128]) True
ln1.weight torch.Size([25]) True
ln1.bias torch.Size([25]) True
encoder.cell_list.0.gate.weights_pool torch.Size([128, 2, 41, 16]) True
encoder.cell_list.0.gate.weights_window torch.Size([128, 1, 16]) True
encoder.cell_list.0.gate.bias_pool torch.Size([128, 32]) True
encoder.cell_list.0.gate.T torch.Size([10]) True
encoder.cell_list.0.update.weights_pool torch.Size([128, 2, 41, 8]) True
encoder.cell_list.0.update.weights_window torch.Size([128, 1, 8]) True
encoder.cell_list.0.update.bias_pool torch.Size([128, 16]) True
encoder.cell_list.0.update.T torch.Size([10]) True
encoder.cell_list.1.gate.weights_pool torch.Size([128, 2, 32, 16]) True
encoder.cell_list.1.gate.weights_window torch.Size([128, 16, 16]) True
encoder.cell_list.1.gate.bias_pool torch.Size([128, 32]) True
encoder.cell_list.1.gate.T torch.Size([10]) True
encoder.cell_list.1.update.weights_pool torch.Size([128, 2, 32, 8]) True
encoder.cell_list.1.update.weights_window torch.Size([128, 16, 8]) True
encoder.cell_list.1.update.bias_pool torch.Size([128, 16]) True
encoder.cell_list.1.update.T torch.Size([10]) True
fc1.weight torch.Size([2, 10000]) True
fc1.bias torch.Size([2]) True
Total params num: 613116
*****************Finish Parameter****************
Positives: 5201 / Negatives: 10402
Dataset length 15603
Positives: 1300 / Negatives: 2600
Dataset length 3900
Positives: 1228 / Negatives: 2456
Dataset length 3684
Applying learning rate decay.
Creat Log File in:  /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122917483451445369118/run.log
2022-12-29 17:48: Train Epoch 1: 3/244 Loss: 0.776155
2022-12-29 17:48: Train Epoch 1: 7/244 Loss: 4.690007
2022-12-29 17:48: Train Epoch 1: 11/244 Loss: 3.489748
2022-12-29 17:49: Train Epoch 1: 15/244 Loss: 2.430667
2022-12-29 17:49: Train Epoch 1: 19/244 Loss: 0.569166
2022-12-29 17:49: Train Epoch 1: 23/244 Loss: 2.258800
2022-12-29 17:49: Train Epoch 1: 27/244 Loss: 1.639845
2022-12-29 17:49: Train Epoch 1: 31/244 Loss: 0.684832
2022-12-29 17:49: Train Epoch 1: 35/244 Loss: 1.387963
2022-12-29 17:49: Train Epoch 1: 39/244 Loss: 1.541647
2022-12-29 17:50: Train Epoch 1: 43/244 Loss: 1.005605
2022-12-29 17:50: Train Epoch 1: 47/244 Loss: 0.716616
2022-12-29 17:50: Train Epoch 1: 51/244 Loss: 0.600554
2022-12-29 17:50: Train Epoch 1: 55/244 Loss: 0.853436
2022-12-29 17:50: Train Epoch 1: 59/244 Loss: 0.543672
2022-12-29 17:50: Train Epoch 1: 63/244 Loss: 0.485846
2022-12-29 17:50: Train Epoch 1: 67/244 Loss: 0.519855
2022-12-29 17:50: Train Epoch 1: 71/244 Loss: 0.737006
2022-12-29 17:51: Train Epoch 1: 75/244 Loss: 0.610800
2022-12-29 17:51: Train Epoch 1: 79/244 Loss: 0.370075
2022-12-29 17:51: Train Epoch 1: 83/244 Loss: 0.312151
2022-12-29 17:51: Train Epoch 1: 87/244 Loss: 0.557279
2022-12-29 17:51: Train Epoch 1: 91/244 Loss: 0.353727
2022-12-29 17:51: Train Epoch 1: 95/244 Loss: 0.303750
2022-12-29 17:51: Train Epoch 1: 99/244 Loss: 0.383590
2022-12-29 17:52: Train Epoch 1: 103/244 Loss: 0.318918
2022-12-29 17:52: Train Epoch 1: 107/244 Loss: 0.339411
2022-12-29 17:52: Train Epoch 1: 111/244 Loss: 0.283696
2022-12-29 17:52: Train Epoch 1: 115/244 Loss: 0.310620
2022-12-29 17:52: Train Epoch 1: 119/244 Loss: 0.269043
2022-12-29 17:52: Train Epoch 1: 123/244 Loss: 0.270926
2022-12-29 17:52: Train Epoch 1: 127/244 Loss: 0.318112
2022-12-29 17:53: Train Epoch 1: 131/244 Loss: 0.371919
2022-12-29 17:53: Train Epoch 1: 135/244 Loss: 0.243223
2022-12-29 17:53: Train Epoch 1: 139/244 Loss: 0.344694
2022-12-29 17:53: Train Epoch 1: 143/244 Loss: 0.306993
2022-12-29 17:53: Train Epoch 1: 147/244 Loss: 0.246085
2022-12-29 17:53: Train Epoch 1: 151/244 Loss: 0.264770
2022-12-29 17:53: Train Epoch 1: 155/244 Loss: 0.265006
2022-12-29 17:54: Train Epoch 1: 159/244 Loss: 0.251095
2022-12-29 17:54: Train Epoch 1: 163/244 Loss: 0.242505
2022-12-29 17:54: Train Epoch 1: 167/244 Loss: 0.234602
2022-12-29 17:54: Train Epoch 1: 171/244 Loss: 0.238635
2022-12-29 17:54: Train Epoch 1: 175/244 Loss: 0.237193
2022-12-29 17:54: Train Epoch 1: 179/244 Loss: 0.230030
2022-12-29 17:54: Train Epoch 1: 183/244 Loss: 0.242857
2022-12-29 17:55: Train Epoch 1: 187/244 Loss: 0.223930
2022-12-29 17:55: Train Epoch 1: 191/244 Loss: 0.251994
2022-12-29 17:55: Train Epoch 1: 195/244 Loss: 0.223240
2022-12-29 17:55: Train Epoch 1: 199/244 Loss: 0.211538
2022-12-29 17:55: Train Epoch 1: 203/244 Loss: 0.245457
2022-12-29 17:55: Train Epoch 1: 207/244 Loss: 0.226819
2022-12-29 17:55: Train Epoch 1: 211/244 Loss: 0.213026
2022-12-29 17:56: Train Epoch 1: 215/244 Loss: 0.242920
2022-12-29 17:56: Train Epoch 1: 219/244 Loss: 0.233370
2022-12-29 17:56: Train Epoch 1: 223/244 Loss: 0.246857
2022-12-29 17:56: Train Epoch 1: 227/244 Loss: 0.183515
2022-12-29 17:56: Train Epoch 1: 231/244 Loss: 0.227325
2022-12-29 17:56: Train Epoch 1: 235/244 Loss: 0.215680
2022-12-29 17:56: Train Epoch 1: 239/244 Loss: 0.197367
2022-12-29 17:56: Train Epoch 1: 243/244 Loss: 0.223269
2022-12-29 17:56: **********Train Epoch 1: averaged Loss: 0.606876 
2022-12-29 17:56: 
Epoch time elapsed: 501.96613359451294

2022-12-29 17:57: 
 metrics validation: {'precision': 0.687557603686636, 'recall': 0.5738461538461539, 'f1-score': 0.6255765199161426, 'support': 1300, 'AUC': 0.7648273668639053, 'AUCPR': 0.6627394123487694, 'TP': 746, 'FP': 339, 'TN': 2261, 'FN': 554} 

2022-12-29 17:57: **********Val Epoch 1: average Loss: 0.285038
2022-12-29 17:57: *********************************Current best model saved!
2022-12-29 17:58: 
 Testing metrics {'precision': 0.7631133671742809, 'recall': 0.7345276872964169, 'f1-score': 0.7485477178423235, 'support': 1228, 'AUC': 0.8419051528398179, 'AUCPR': 0.7494536136123621, 'TP': 902, 'FP': 280, 'TN': 2176, 'FN': 326} 

2022-12-29 17:58: Train Epoch 2: 3/244 Loss: 0.216360
2022-12-29 17:58: Train Epoch 2: 7/244 Loss: 0.201880
2022-12-29 17:58: Train Epoch 2: 11/244 Loss: 0.216032
2022-12-29 17:58: Train Epoch 2: 15/244 Loss: 0.194440
2022-12-29 17:59: Train Epoch 2: 19/244 Loss: 0.207077
2022-12-29 17:59: Train Epoch 2: 23/244 Loss: 0.222437
2022-12-29 17:59: Train Epoch 2: 27/244 Loss: 0.218473
2022-12-29 17:59: Train Epoch 2: 31/244 Loss: 0.213214
2022-12-29 17:59: Train Epoch 2: 35/244 Loss: 0.231218
2022-12-29 17:59: Train Epoch 2: 39/244 Loss: 0.228920
2022-12-29 17:59: Train Epoch 2: 43/244 Loss: 0.188662
2022-12-29 18:00: Train Epoch 2: 47/244 Loss: 0.240042
2022-12-29 18:00: Train Epoch 2: 51/244 Loss: 0.183410
2022-12-29 18:00: Train Epoch 2: 55/244 Loss: 0.222875
2022-12-29 18:00: Train Epoch 2: 59/244 Loss: 0.181698
2022-12-29 18:00: Train Epoch 2: 63/244 Loss: 0.214880
2022-12-29 18:00: Train Epoch 2: 67/244 Loss: 0.201574
2022-12-29 18:01: Train Epoch 2: 71/244 Loss: 0.220477
2022-12-29 18:01: Train Epoch 2: 75/244 Loss: 0.240394
2022-12-29 18:01: Train Epoch 2: 79/244 Loss: 0.244966
2022-12-29 18:01: Train Epoch 2: 83/244 Loss: 0.225469
2022-12-29 18:01: Train Epoch 2: 87/244 Loss: 0.275440
2022-12-29 18:01: Train Epoch 2: 91/244 Loss: 0.219574
2022-12-29 18:01: Train Epoch 2: 95/244 Loss: 0.206363
2022-12-29 18:01: Train Epoch 2: 99/244 Loss: 0.248283
2022-12-29 18:02: Train Epoch 2: 103/244 Loss: 0.211068
2022-12-29 18:02: Train Epoch 2: 107/244 Loss: 0.153938
2022-12-29 18:02: Train Epoch 2: 111/244 Loss: 0.193030
2022-12-29 18:02: Train Epoch 2: 115/244 Loss: 0.187167
2022-12-29 18:02: Train Epoch 2: 119/244 Loss: 0.191850
2022-12-29 18:02: Train Epoch 2: 123/244 Loss: 0.206137
2022-12-29 18:02: Train Epoch 2: 127/244 Loss: 0.195470
2022-12-29 18:03: Train Epoch 2: 131/244 Loss: 0.225982
2022-12-29 18:03: Train Epoch 2: 135/244 Loss: 0.220452
2022-12-29 18:03: Train Epoch 2: 139/244 Loss: 0.193150
2022-12-29 18:03: Train Epoch 2: 143/244 Loss: 0.194111
2022-12-29 18:03: Train Epoch 2: 147/244 Loss: 0.162616
2022-12-29 18:03: Train Epoch 2: 151/244 Loss: 0.178434
2022-12-29 18:03: Train Epoch 2: 155/244 Loss: 0.205628
2022-12-29 18:04: Train Epoch 2: 159/244 Loss: 0.196369
2022-12-29 18:04: Train Epoch 2: 163/244 Loss: 0.206844
2022-12-29 18:04: Train Epoch 2: 167/244 Loss: 0.190059
2022-12-29 18:04: Train Epoch 2: 171/244 Loss: 0.215306
2022-12-29 18:04: Train Epoch 2: 175/244 Loss: 0.207962
2022-12-29 18:04: Train Epoch 2: 179/244 Loss: 0.238193
2022-12-29 18:05: Train Epoch 2: 183/244 Loss: 0.182841
2022-12-29 18:05: Train Epoch 2: 187/244 Loss: 0.220884
2022-12-29 18:05: Train Epoch 2: 191/244 Loss: 0.203580
2022-12-29 18:05: Train Epoch 2: 195/244 Loss: 0.209041
2022-12-29 18:05: Train Epoch 2: 199/244 Loss: 0.183356
2022-12-29 18:05: Train Epoch 2: 203/244 Loss: 0.219164
2022-12-29 18:05: Train Epoch 2: 207/244 Loss: 0.211347
2022-12-29 18:06: Train Epoch 2: 211/244 Loss: 0.222619
2022-12-29 18:06: Train Epoch 2: 215/244 Loss: 0.227195
2022-12-29 18:06: Train Epoch 2: 219/244 Loss: 0.170831
2022-12-29 18:06: Train Epoch 2: 223/244 Loss: 0.193859
2022-12-29 18:06: Train Epoch 2: 227/244 Loss: 0.184488
2022-12-29 18:06: Train Epoch 2: 231/244 Loss: 0.196978
2022-12-29 18:06: Train Epoch 2: 235/244 Loss: 0.221693
2022-12-29 18:07: Train Epoch 2: 239/244 Loss: 0.185035
2022-12-29 18:07: Train Epoch 2: 243/244 Loss: 0.192170
2022-12-29 18:07: **********Train Epoch 2: averaged Loss: 0.207590 
2022-12-29 18:07: 
Epoch time elapsed: 529.9417498111725

2022-12-29 18:07: 
 metrics validation: {'precision': 0.6855287569573284, 'recall': 0.5684615384615385, 'f1-score': 0.6215306980656014, 'support': 1300, 'AUC': 0.7837937869822487, 'AUCPR': 0.7128826563862709, 'TP': 739, 'FP': 339, 'TN': 2261, 'FN': 561} 

2022-12-29 18:07: **********Val Epoch 2: average Loss: 0.300016
2022-12-29 18:08: 
 Testing metrics {'precision': 0.7631133671742809, 'recall': 0.7345276872964169, 'f1-score': 0.7485477178423235, 'support': 1228, 'AUC': 0.8419051528398179, 'AUCPR': 0.7494536136123621, 'TP': 902, 'FP': 280, 'TN': 2176, 'FN': 326} 

2022-12-29 18:08: Train Epoch 3: 3/244 Loss: 0.191053
2022-12-29 18:08: Train Epoch 3: 7/244 Loss: 0.230937
2022-12-29 18:08: Train Epoch 3: 11/244 Loss: 0.211983
2022-12-29 18:08: Train Epoch 3: 15/244 Loss: 0.223492
2022-12-29 18:09: Train Epoch 3: 19/244 Loss: 0.246702
2022-12-29 18:09: Train Epoch 3: 23/244 Loss: 0.250947
2022-12-29 18:09: Train Epoch 3: 27/244 Loss: 0.265813
2022-12-29 18:09: Train Epoch 3: 31/244 Loss: 0.226768
2022-12-29 18:09: Train Epoch 3: 35/244 Loss: 0.217326
2022-12-29 18:09: Train Epoch 3: 39/244 Loss: 0.199934
2022-12-29 18:09: Train Epoch 3: 43/244 Loss: 0.231381
2022-12-29 18:10: Train Epoch 3: 47/244 Loss: 0.211440
2022-12-29 18:10: Train Epoch 3: 51/244 Loss: 0.196171
2022-12-29 18:10: Train Epoch 3: 55/244 Loss: 0.220431
2022-12-29 18:10: Train Epoch 3: 59/244 Loss: 0.207524
2022-12-29 18:10: Train Epoch 3: 63/244 Loss: 0.219322
2022-12-29 18:10: Train Epoch 3: 67/244 Loss: 0.224907
2022-12-29 18:10: Train Epoch 3: 71/244 Loss: 0.221996
2022-12-29 18:11: Train Epoch 3: 75/244 Loss: 0.239877
2022-12-29 18:11: Train Epoch 3: 79/244 Loss: 0.220594
2022-12-29 18:11: Train Epoch 3: 83/244 Loss: 0.230392
2022-12-29 18:11: Train Epoch 3: 87/244 Loss: 0.211682
2022-12-29 18:11: Train Epoch 3: 91/244 Loss: 0.244568
2022-12-29 18:11: Train Epoch 3: 95/244 Loss: 0.196966
2022-12-29 18:11: Train Epoch 3: 99/244 Loss: 0.243171
2022-12-29 18:12: Train Epoch 3: 103/244 Loss: 0.231232
2022-12-29 18:12: Train Epoch 3: 107/244 Loss: 0.252979
2022-12-29 18:12: Train Epoch 3: 111/244 Loss: 0.266766
2022-12-29 18:12: Train Epoch 3: 115/244 Loss: 0.201836
2022-12-29 18:12: Train Epoch 3: 119/244 Loss: 0.238227
2022-12-29 18:12: Train Epoch 3: 123/244 Loss: 0.228751
2022-12-29 18:12: Train Epoch 3: 127/244 Loss: 0.241809
2022-12-29 18:13: Train Epoch 3: 131/244 Loss: 0.265778
2022-12-29 18:13: Train Epoch 3: 135/244 Loss: 0.194123
2022-12-29 18:13: Train Epoch 3: 139/244 Loss: 0.244039
2022-12-29 18:13: Train Epoch 3: 143/244 Loss: 0.212018
2022-12-29 18:13: Train Epoch 3: 147/244 Loss: 0.230619
2022-12-29 18:13: Train Epoch 3: 151/244 Loss: 0.278700
2022-12-29 18:14: Train Epoch 3: 155/244 Loss: 0.222186
2022-12-29 18:14: Train Epoch 3: 159/244 Loss: 0.243005
2022-12-29 18:14: Train Epoch 3: 163/244 Loss: 0.180203
2022-12-29 18:14: Train Epoch 3: 167/244 Loss: 0.279053
2022-12-29 18:14: Train Epoch 3: 171/244 Loss: 0.189725
2022-12-29 18:14: Train Epoch 3: 175/244 Loss: 0.229842
2022-12-29 18:14: Train Epoch 3: 179/244 Loss: 0.263558
2022-12-29 18:15: Train Epoch 3: 183/244 Loss: 0.218631
2022-12-29 18:15: Train Epoch 3: 187/244 Loss: 0.277897
2022-12-29 18:15: Train Epoch 3: 191/244 Loss: 0.212277
2022-12-29 18:15: Train Epoch 3: 195/244 Loss: 0.294351
2022-12-29 18:15: Train Epoch 3: 199/244 Loss: 0.184085
2022-12-29 18:15: Train Epoch 3: 203/244 Loss: 0.224611
2022-12-29 18:15: Train Epoch 3: 207/244 Loss: 0.202160
2022-12-29 18:15: Train Epoch 3: 211/244 Loss: 0.175103
2022-12-29 18:16: Train Epoch 3: 215/244 Loss: 0.190457
2022-12-29 18:16: Train Epoch 3: 219/244 Loss: 0.214656
2022-12-29 18:16: Train Epoch 3: 223/244 Loss: 0.186479
2022-12-29 18:16: Train Epoch 3: 227/244 Loss: 0.212518
2022-12-29 18:16: Train Epoch 3: 231/244 Loss: 0.205267
2022-12-29 18:16: Train Epoch 3: 235/244 Loss: 0.191177
2022-12-29 18:17: Train Epoch 3: 239/244 Loss: 0.186964
2022-12-29 18:17: Train Epoch 3: 243/244 Loss: 0.191168
2022-12-29 18:17: **********Train Epoch 3: averaged Loss: 0.223732 
2022-12-29 18:17: 
Epoch time elapsed: 534.3673923015594

2022-12-29 18:17: 
 metrics validation: {'precision': 0.6530291109362707, 'recall': 0.6384615384615384, 'f1-score': 0.6456631660832362, 'support': 1300, 'AUC': 0.7831124260355029, 'AUCPR': 0.7107097641814925, 'TP': 830, 'FP': 441, 'TN': 2159, 'FN': 470} 

2022-12-29 18:17: **********Val Epoch 3: average Loss: 0.304077
2022-12-29 18:18: 
 Testing metrics {'precision': 0.7631133671742809, 'recall': 0.7345276872964169, 'f1-score': 0.7485477178423235, 'support': 1228, 'AUC': 0.8419051528398179, 'AUCPR': 0.7494536136123621, 'TP': 902, 'FP': 280, 'TN': 2176, 'FN': 326} 

2022-12-29 18:18: Train Epoch 4: 3/244 Loss: 0.211278
2022-12-29 18:18: Train Epoch 4: 7/244 Loss: 0.226257
2022-12-29 18:18: Train Epoch 4: 11/244 Loss: 0.186652
2022-12-29 18:18: Train Epoch 4: 15/244 Loss: 0.212398
2022-12-29 18:19: Train Epoch 4: 19/244 Loss: 0.186014
2022-12-29 18:19: Train Epoch 4: 23/244 Loss: 0.214903
2022-12-29 18:19: Train Epoch 4: 27/244 Loss: 0.228192
2022-12-29 18:19: Train Epoch 4: 31/244 Loss: 0.223002
2022-12-29 18:19: Train Epoch 4: 35/244 Loss: 0.191373
2022-12-29 18:19: Train Epoch 4: 39/244 Loss: 0.225441
2022-12-29 18:20: Train Epoch 4: 43/244 Loss: 0.205034
2022-12-29 18:20: Train Epoch 4: 47/244 Loss: 0.210705
2022-12-29 18:20: Train Epoch 4: 51/244 Loss: 0.215998
2022-12-29 18:20: Train Epoch 4: 55/244 Loss: 0.227363
2022-12-29 18:20: Train Epoch 4: 59/244 Loss: 0.227779
2022-12-29 18:21: Train Epoch 4: 63/244 Loss: 0.220182
2022-12-29 18:21: Train Epoch 4: 67/244 Loss: 0.219301
2022-12-29 18:21: Train Epoch 4: 71/244 Loss: 0.221767
2022-12-29 18:21: Train Epoch 4: 75/244 Loss: 0.218959
2022-12-29 18:21: Train Epoch 4: 79/244 Loss: 0.209467
2022-12-29 18:21: Train Epoch 4: 83/244 Loss: 0.208935
2022-12-29 18:22: Train Epoch 4: 87/244 Loss: 0.222206
2022-12-29 18:22: Train Epoch 4: 91/244 Loss: 0.267090
2022-12-29 18:22: Train Epoch 4: 95/244 Loss: 0.257839
2022-12-29 18:22: Train Epoch 4: 99/244 Loss: 0.209006
2022-12-29 18:22: Train Epoch 4: 103/244 Loss: 0.247539
2022-12-29 18:23: Train Epoch 4: 107/244 Loss: 0.205474
2022-12-29 18:23: Train Epoch 4: 111/244 Loss: 0.244809
2022-12-29 18:23: Train Epoch 4: 115/244 Loss: 0.233880
2022-12-29 18:23: Train Epoch 4: 119/244 Loss: 0.202705
2022-12-29 18:23: Train Epoch 4: 123/244 Loss: 0.231548
2022-12-29 18:24: Train Epoch 4: 127/244 Loss: 0.169742
2022-12-29 18:24: Train Epoch 4: 131/244 Loss: 0.232702
2022-12-29 18:24: Train Epoch 4: 135/244 Loss: 0.205401
2022-12-29 18:24: Train Epoch 4: 139/244 Loss: 0.257158
2022-12-29 18:24: Train Epoch 4: 143/244 Loss: 0.213069
2022-12-29 18:24: Train Epoch 4: 147/244 Loss: 0.248696
2022-12-29 18:25: Train Epoch 4: 151/244 Loss: 0.232748
2022-12-29 18:25: Train Epoch 4: 155/244 Loss: 0.256302
2022-12-29 18:25: Train Epoch 4: 159/244 Loss: 0.217357
2022-12-29 18:25: Train Epoch 4: 163/244 Loss: 0.196306
2022-12-29 18:25: Train Epoch 4: 167/244 Loss: 0.188098
2022-12-29 18:26: Train Epoch 4: 171/244 Loss: 0.226674
2022-12-29 18:26: Train Epoch 4: 175/244 Loss: 0.213185
2022-12-29 18:26: Train Epoch 4: 179/244 Loss: 0.207257
2022-12-29 18:26: Train Epoch 4: 183/244 Loss: 0.216393
2022-12-29 18:26: Train Epoch 4: 187/244 Loss: 0.237243
2022-12-29 18:26: Train Epoch 4: 191/244 Loss: 0.229790
2022-12-29 18:27: Train Epoch 4: 195/244 Loss: 0.193087
2022-12-29 18:27: Train Epoch 4: 199/244 Loss: 0.167094
2022-12-29 18:27: Train Epoch 4: 203/244 Loss: 0.180903
2022-12-29 18:27: Train Epoch 4: 207/244 Loss: 0.215287
2022-12-29 18:27: Train Epoch 4: 211/244 Loss: 0.167634
2022-12-29 18:28: Train Epoch 4: 215/244 Loss: 0.184339
2022-12-29 18:28: Train Epoch 4: 219/244 Loss: 0.184752
2022-12-29 18:28: Train Epoch 4: 223/244 Loss: 0.211625
2022-12-29 18:28: Train Epoch 4: 227/244 Loss: 0.249414
2022-12-29 18:28: Train Epoch 4: 231/244 Loss: 0.207107
2022-12-29 18:28: Train Epoch 4: 235/244 Loss: 0.180672
2022-12-29 18:28: Train Epoch 4: 239/244 Loss: 0.209236
2022-12-29 18:29: Train Epoch 4: 243/244 Loss: 0.204426
2022-12-29 18:29: **********Train Epoch 4: averaged Loss: 0.215029 
2022-12-29 18:29: 
Epoch time elapsed: 648.3000867366791

2022-12-29 18:29: 
 metrics validation: {'precision': 0.8237885462555066, 'recall': 0.43153846153846154, 'f1-score': 0.5663806158505805, 'support': 1300, 'AUC': 0.7842825443786982, 'AUCPR': 0.7160353464900459, 'TP': 561, 'FP': 120, 'TN': 2480, 'FN': 739} 

2022-12-29 18:29: **********Val Epoch 4: average Loss: 0.345333
2022-12-29 18:30: 
 Testing metrics {'precision': 0.7631133671742809, 'recall': 0.7345276872964169, 'f1-score': 0.7485477178423235, 'support': 1228, 'AUC': 0.8419051528398179, 'AUCPR': 0.7494536136123621, 'TP': 902, 'FP': 280, 'TN': 2176, 'FN': 326} 

2022-12-29 18:30: Train Epoch 5: 3/244 Loss: 0.188456
2022-12-29 18:31: Train Epoch 5: 7/244 Loss: 0.214463
2022-12-29 18:31: Train Epoch 5: 11/244 Loss: 0.228277
2022-12-29 18:31: Train Epoch 5: 15/244 Loss: 0.232397
2022-12-29 18:31: Train Epoch 5: 19/244 Loss: 0.208678
2022-12-29 18:31: Train Epoch 5: 23/244 Loss: 0.229767
2022-12-29 18:31: Train Epoch 5: 27/244 Loss: 0.222513
2022-12-29 18:32: Train Epoch 5: 31/244 Loss: 0.202113
2022-12-29 18:32: Train Epoch 5: 35/244 Loss: 0.212679
2022-12-29 18:32: Train Epoch 5: 39/244 Loss: 0.187166
2022-12-29 18:32: Train Epoch 5: 43/244 Loss: 0.226109
2022-12-29 18:32: Train Epoch 5: 47/244 Loss: 0.227623
2022-12-29 18:33: Train Epoch 5: 51/244 Loss: 0.209907
2022-12-29 18:33: Train Epoch 5: 55/244 Loss: 0.213881
2022-12-29 18:33: Train Epoch 5: 59/244 Loss: 0.209483
2022-12-29 18:33: Train Epoch 5: 63/244 Loss: 0.227770
2022-12-29 18:33: Train Epoch 5: 67/244 Loss: 0.210068
2022-12-29 18:33: Train Epoch 5: 71/244 Loss: 0.221484
2022-12-29 18:34: Train Epoch 5: 75/244 Loss: 0.217398
2022-12-29 18:34: Train Epoch 5: 79/244 Loss: 0.222340
2022-12-29 18:34: Train Epoch 5: 83/244 Loss: 0.227380
2022-12-29 18:34: Train Epoch 5: 87/244 Loss: 0.199705
2022-12-29 18:34: Train Epoch 5: 91/244 Loss: 0.240010
2022-12-29 18:34: Train Epoch 5: 95/244 Loss: 0.186555
2022-12-29 18:35: Train Epoch 5: 99/244 Loss: 0.204771
2022-12-29 18:35: Train Epoch 5: 103/244 Loss: 0.234121
2022-12-29 18:35: Train Epoch 5: 107/244 Loss: 0.220598
2022-12-29 18:35: Train Epoch 5: 111/244 Loss: 0.196337
2022-12-29 18:35: Train Epoch 5: 115/244 Loss: 0.209524
2022-12-29 18:36: Train Epoch 5: 119/244 Loss: 0.187949
2022-12-29 18:36: Train Epoch 5: 123/244 Loss: 0.214927
2022-12-29 18:36: Train Epoch 5: 127/244 Loss: 0.215714
2022-12-29 18:36: Train Epoch 5: 131/244 Loss: 0.175437
2022-12-29 18:36: Train Epoch 5: 135/244 Loss: 0.173848
2022-12-29 18:36: Train Epoch 5: 139/244 Loss: 0.187720
2022-12-29 18:37: Train Epoch 5: 143/244 Loss: 0.210008
2022-12-29 18:37: Train Epoch 5: 147/244 Loss: 0.193788
2022-12-29 18:37: Train Epoch 5: 151/244 Loss: 0.191044
2022-12-29 18:37: Train Epoch 5: 155/244 Loss: 0.176595
2022-12-29 18:37: Train Epoch 5: 159/244 Loss: 0.194687
2022-12-29 18:38: Train Epoch 5: 163/244 Loss: 0.229005
2022-12-29 18:38: Train Epoch 5: 167/244 Loss: 0.195592
2022-12-29 18:38: Train Epoch 5: 171/244 Loss: 0.218215
2022-12-29 18:38: Train Epoch 5: 175/244 Loss: 0.200286
2022-12-29 18:38: Train Epoch 5: 179/244 Loss: 0.184722
2022-12-29 18:38: Train Epoch 5: 183/244 Loss: 0.228990
2022-12-29 18:39: Train Epoch 5: 187/244 Loss: 0.192563
2022-12-29 18:39: Train Epoch 5: 191/244 Loss: 0.250558
2022-12-29 18:39: Train Epoch 5: 195/244 Loss: 0.205781
2022-12-29 18:39: Train Epoch 5: 199/244 Loss: 0.188668
2022-12-29 18:39: Train Epoch 5: 203/244 Loss: 0.214240
2022-12-29 18:39: Train Epoch 5: 207/244 Loss: 0.182331
2022-12-29 18:40: Train Epoch 5: 211/244 Loss: 0.191469
2022-12-29 18:40: Train Epoch 5: 215/244 Loss: 0.245420
2022-12-29 18:40: Train Epoch 5: 219/244 Loss: 0.232033
2022-12-29 18:40: Train Epoch 5: 223/244 Loss: 0.190937
2022-12-29 18:40: Train Epoch 5: 227/244 Loss: 0.256577
2022-12-29 18:41: Train Epoch 5: 231/244 Loss: 0.178677
2022-12-29 18:41: Train Epoch 5: 235/244 Loss: 0.201375
2022-12-29 18:41: Train Epoch 5: 239/244 Loss: 0.205732
2022-12-29 18:41: Train Epoch 5: 243/244 Loss: 0.226482
2022-12-29 18:41: **********Train Epoch 5: averaged Loss: 0.209393 
2022-12-29 18:41: 
Epoch time elapsed: 642.637442111969

2022-12-29 18:42: 
 metrics validation: {'precision': 0.8561872909698997, 'recall': 0.39384615384615385, 'f1-score': 0.5395152792413066, 'support': 1300, 'AUC': 0.7876331360946747, 'AUCPR': 0.7186710913901021, 'TP': 512, 'FP': 86, 'TN': 2514, 'FN': 788} 

2022-12-29 18:42: **********Val Epoch 5: average Loss: 0.347569
2022-12-29 18:43: 
 Testing metrics {'precision': 0.7631133671742809, 'recall': 0.7345276872964169, 'f1-score': 0.7485477178423235, 'support': 1228, 'AUC': 0.8419051528398179, 'AUCPR': 0.7494536136123621, 'TP': 902, 'FP': 280, 'TN': 2176, 'FN': 326} 

2022-12-29 18:43: Train Epoch 6: 3/244 Loss: 0.222454
2022-12-29 18:43: Train Epoch 6: 7/244 Loss: 0.292846
2022-12-29 18:43: Train Epoch 6: 11/244 Loss: 0.259069
2022-12-29 18:43: Train Epoch 6: 15/244 Loss: 0.248910
2022-12-29 18:43: Train Epoch 6: 19/244 Loss: 0.236628
2022-12-29 18:44: Train Epoch 6: 23/244 Loss: 0.286186
2022-12-29 18:44: Train Epoch 6: 27/244 Loss: 0.234416
2022-12-29 18:44: Train Epoch 6: 31/244 Loss: 0.271431
2022-12-29 18:44: Train Epoch 6: 35/244 Loss: 0.265163
2022-12-29 18:44: Train Epoch 6: 39/244 Loss: 0.228575
2022-12-29 18:44: Train Epoch 6: 43/244 Loss: 0.239070
2022-12-29 18:45: Train Epoch 6: 47/244 Loss: 0.254029
2022-12-29 18:45: Train Epoch 6: 51/244 Loss: 0.189809
2022-12-29 18:45: Train Epoch 6: 55/244 Loss: 0.237314
2022-12-29 18:45: Train Epoch 6: 59/244 Loss: 0.219270
2022-12-29 18:45: Train Epoch 6: 63/244 Loss: 0.259974
2022-12-29 18:46: Train Epoch 6: 67/244 Loss: 0.209884
2022-12-29 18:46: Train Epoch 6: 71/244 Loss: 0.242219
2022-12-29 18:46: Train Epoch 6: 75/244 Loss: 0.188914
2022-12-29 18:46: Train Epoch 6: 79/244 Loss: 0.203410
2022-12-29 18:46: Train Epoch 6: 83/244 Loss: 0.215303
2022-12-29 18:46: Train Epoch 6: 87/244 Loss: 0.201699
2022-12-29 18:47: Train Epoch 6: 91/244 Loss: 0.243717
2022-12-29 18:47: Train Epoch 6: 95/244 Loss: 0.184735
2022-12-29 18:47: Train Epoch 6: 99/244 Loss: 0.191433
2022-12-29 18:47: Train Epoch 6: 103/244 Loss: 0.201191
2022-12-29 18:47: Train Epoch 6: 107/244 Loss: 0.215223
2022-12-29 18:47: Train Epoch 6: 111/244 Loss: 0.194544
2022-12-29 18:48: Train Epoch 6: 115/244 Loss: 0.166005
2022-12-29 18:48: Train Epoch 6: 119/244 Loss: 0.195346
2022-12-29 18:48: Train Epoch 6: 123/244 Loss: 0.223783
2022-12-29 18:48: Train Epoch 6: 127/244 Loss: 0.195255
2022-12-29 18:48: Train Epoch 6: 131/244 Loss: 0.248656
2022-12-29 18:49: Train Epoch 6: 135/244 Loss: 0.234142
2022-12-29 18:49: Train Epoch 6: 139/244 Loss: 0.207240
2022-12-29 18:49: Train Epoch 6: 143/244 Loss: 0.214986
2022-12-29 18:49: Train Epoch 6: 147/244 Loss: 0.249231
2022-12-29 18:49: Train Epoch 6: 151/244 Loss: 0.179910
2022-12-29 18:49: Train Epoch 6: 155/244 Loss: 0.258630
2022-12-29 18:50: Train Epoch 6: 159/244 Loss: 0.183831
2022-12-29 18:50: Train Epoch 6: 163/244 Loss: 0.192948
2022-12-29 18:50: Train Epoch 6: 167/244 Loss: 0.241357
2022-12-29 18:50: Train Epoch 6: 171/244 Loss: 0.223037
2022-12-29 18:50: Train Epoch 6: 175/244 Loss: 0.231261
2022-12-29 18:51: Train Epoch 6: 179/244 Loss: 0.231091
2022-12-29 18:51: Train Epoch 6: 183/244 Loss: 0.253635
2022-12-29 18:51: Train Epoch 6: 187/244 Loss: 0.222753
2022-12-29 18:51: Train Epoch 6: 191/244 Loss: 0.291676
2022-12-29 18:51: Train Epoch 6: 195/244 Loss: 0.200942
2022-12-29 18:51: Train Epoch 6: 199/244 Loss: 0.195819
2022-12-29 18:52: Train Epoch 6: 203/244 Loss: 0.200430
2022-12-29 18:52: Train Epoch 6: 207/244 Loss: 0.202014
2022-12-29 18:52: Train Epoch 6: 211/244 Loss: 0.187339
2022-12-29 18:52: Train Epoch 6: 215/244 Loss: 0.211436
2022-12-29 18:52: Train Epoch 6: 219/244 Loss: 0.201676
2022-12-29 18:53: Train Epoch 6: 223/244 Loss: 0.238871
2022-12-29 18:53: Train Epoch 6: 227/244 Loss: 0.201520
2022-12-29 18:53: Train Epoch 6: 231/244 Loss: 0.322476
2022-12-29 18:53: Train Epoch 6: 235/244 Loss: 0.206488
2022-12-29 18:53: Train Epoch 6: 239/244 Loss: 0.230986
2022-12-29 18:53: Train Epoch 6: 243/244 Loss: 0.173812
2022-12-29 18:53: **********Train Epoch 6: averaged Loss: 0.223869 
2022-12-29 18:53: 
Epoch time elapsed: 647.6697959899902

2022-12-29 18:54: 
 metrics validation: {'precision': 0.8909090909090909, 'recall': 0.3392307692307692, 'f1-score': 0.4913649025069638, 'support': 1300, 'AUC': 0.7863668639053254, 'AUCPR': 0.7125440685803177, 'TP': 441, 'FP': 54, 'TN': 2546, 'FN': 859} 

2022-12-29 18:54: **********Val Epoch 6: average Loss: 0.388399
2022-12-29 18:55: 
 Testing metrics {'precision': 0.7631133671742809, 'recall': 0.7345276872964169, 'f1-score': 0.7485477178423235, 'support': 1228, 'AUC': 0.8419051528398179, 'AUCPR': 0.7494536136123621, 'TP': 902, 'FP': 280, 'TN': 2176, 'FN': 326} 

2022-12-29 18:55: Train Epoch 7: 3/244 Loss: 0.200476
2022-12-29 18:55: Train Epoch 7: 7/244 Loss: 0.244518
2022-12-29 18:56: Train Epoch 7: 11/244 Loss: 0.237104
2022-12-29 18:56: Train Epoch 7: 15/244 Loss: 0.209795
2022-12-29 18:56: Train Epoch 7: 19/244 Loss: 0.211996
2022-12-29 18:56: Train Epoch 7: 23/244 Loss: 0.222496
2022-12-29 18:56: Train Epoch 7: 27/244 Loss: 0.242451
2022-12-29 18:56: Train Epoch 7: 31/244 Loss: 0.206131
2022-12-29 18:57: Train Epoch 7: 35/244 Loss: 0.203415
2022-12-29 18:57: Train Epoch 7: 39/244 Loss: 0.213652
2022-12-29 18:57: Train Epoch 7: 43/244 Loss: 0.237543
2022-12-29 18:57: Train Epoch 7: 47/244 Loss: 0.236926
2022-12-29 18:57: Train Epoch 7: 51/244 Loss: 0.224283
2022-12-29 18:58: Train Epoch 7: 55/244 Loss: 0.200209
2022-12-29 18:58: Train Epoch 7: 59/244 Loss: 0.225725
2022-12-29 18:58: Train Epoch 7: 63/244 Loss: 0.237243
2022-12-29 18:58: Train Epoch 7: 67/244 Loss: 0.221855
2022-12-29 18:58: Train Epoch 7: 71/244 Loss: 0.254928
2022-12-29 18:58: Train Epoch 7: 75/244 Loss: 0.209314
2022-12-29 18:59: Train Epoch 7: 79/244 Loss: 0.225844
2022-12-29 18:59: Train Epoch 7: 83/244 Loss: 0.194131
2022-12-29 18:59: Train Epoch 7: 87/244 Loss: 0.219963
2022-12-29 18:59: Train Epoch 7: 91/244 Loss: 0.184759
2022-12-29 18:59: Train Epoch 7: 95/244 Loss: 0.191809
2022-12-29 18:59: Train Epoch 7: 99/244 Loss: 0.207418
2022-12-29 19:00: Train Epoch 7: 103/244 Loss: 0.248015
2022-12-29 19:00: Train Epoch 7: 107/244 Loss: 0.210497
2022-12-29 19:00: Train Epoch 7: 111/244 Loss: 0.236027
2022-12-29 19:00: Train Epoch 7: 115/244 Loss: 0.207889
2022-12-29 19:00: Train Epoch 7: 119/244 Loss: 0.189722
2022-12-29 19:01: Train Epoch 7: 123/244 Loss: 0.244006
2022-12-29 19:01: Train Epoch 7: 127/244 Loss: 0.190601
2022-12-29 19:01: Train Epoch 7: 131/244 Loss: 0.163119
2022-12-29 19:01: Train Epoch 7: 135/244 Loss: 0.214250
2022-12-29 19:01: Train Epoch 7: 139/244 Loss: 0.214022
2022-12-29 19:02: Train Epoch 7: 143/244 Loss: 0.178261
2022-12-29 19:02: Train Epoch 7: 147/244 Loss: 0.187900
2022-12-29 19:02: Train Epoch 7: 151/244 Loss: 0.190442
2022-12-29 19:02: Train Epoch 7: 155/244 Loss: 0.207269
2022-12-29 19:02: Train Epoch 7: 159/244 Loss: 0.190531
2022-12-29 19:02: Train Epoch 7: 163/244 Loss: 0.197635
2022-12-29 19:03: Train Epoch 7: 167/244 Loss: 0.219851
2022-12-29 19:03: Train Epoch 7: 171/244 Loss: 0.222647
2022-12-29 19:03: Train Epoch 7: 175/244 Loss: 0.260846
2022-12-29 19:03: Train Epoch 7: 179/244 Loss: 0.221128
2022-12-29 19:03: Train Epoch 7: 183/244 Loss: 0.285470
2022-12-29 19:04: Train Epoch 7: 187/244 Loss: 0.202698
2022-12-29 19:04: Train Epoch 7: 191/244 Loss: 0.250483
2022-12-29 19:04: Train Epoch 7: 195/244 Loss: 0.210256
2022-12-29 19:04: Train Epoch 7: 199/244 Loss: 0.295336
2022-12-29 19:04: Train Epoch 7: 203/244 Loss: 0.217979
2022-12-29 19:05: Train Epoch 7: 207/244 Loss: 0.223141
2022-12-29 19:05: Train Epoch 7: 211/244 Loss: 0.204173
2022-12-29 19:05: Train Epoch 7: 215/244 Loss: 0.186601
2022-12-29 19:05: Train Epoch 7: 219/244 Loss: 0.203701
2022-12-29 19:05: Train Epoch 7: 223/244 Loss: 0.179642
2022-12-29 19:05: Train Epoch 7: 227/244 Loss: 0.193305
2022-12-29 19:05: Train Epoch 7: 231/244 Loss: 0.170935
2022-12-29 19:06: Train Epoch 7: 235/244 Loss: 0.180241
2022-12-29 19:06: Train Epoch 7: 239/244 Loss: 0.212225
2022-12-29 19:06: Train Epoch 7: 243/244 Loss: 0.170292
2022-12-29 19:06: **********Train Epoch 7: averaged Loss: 0.213854 
2022-12-29 19:06: 
Epoch time elapsed: 655.3987216949463

2022-12-29 19:07: 
 metrics validation: {'precision': 0.720348204570185, 'recall': 0.5092307692307693, 'f1-score': 0.5966651644885085, 'support': 1300, 'AUC': 0.7841281065088757, 'AUCPR': 0.7091155313664892, 'TP': 662, 'FP': 257, 'TN': 2343, 'FN': 638} 

2022-12-29 19:07: **********Val Epoch 7: average Loss: 0.318139
2022-12-29 19:08: 
 Testing metrics {'precision': 0.7631133671742809, 'recall': 0.7345276872964169, 'f1-score': 0.7485477178423235, 'support': 1228, 'AUC': 0.8419051528398179, 'AUCPR': 0.7494536136123621, 'TP': 902, 'FP': 280, 'TN': 2176, 'FN': 326} 

2022-12-29 19:08: Train Epoch 8: 3/244 Loss: 0.203214
2022-12-29 19:08: Train Epoch 8: 7/244 Loss: 0.216291
2022-12-29 19:08: Train Epoch 8: 11/244 Loss: 0.207809
2022-12-29 19:08: Train Epoch 8: 15/244 Loss: 0.200770
2022-12-29 19:08: Train Epoch 8: 19/244 Loss: 0.193801
2022-12-29 19:09: Train Epoch 8: 23/244 Loss: 0.203978
2022-12-29 19:09: Train Epoch 8: 27/244 Loss: 0.201993
2022-12-29 19:09: Train Epoch 8: 31/244 Loss: 0.250999
2022-12-29 19:09: Train Epoch 8: 35/244 Loss: 0.213674
2022-12-29 19:09: Train Epoch 8: 39/244 Loss: 0.310926
2022-12-29 19:09: Train Epoch 8: 43/244 Loss: 0.223613
2022-12-29 19:10: Train Epoch 8: 47/244 Loss: 0.225023
2022-12-29 19:10: Train Epoch 8: 51/244 Loss: 0.210688
2022-12-29 19:10: Train Epoch 8: 55/244 Loss: 0.208101
2022-12-29 19:10: Train Epoch 8: 59/244 Loss: 0.185320
2022-12-29 19:11: Train Epoch 8: 63/244 Loss: 0.225169
2022-12-29 19:11: Train Epoch 8: 67/244 Loss: 0.192456
2022-12-29 19:11: Train Epoch 8: 71/244 Loss: 0.230928
2022-12-29 19:11: Train Epoch 8: 75/244 Loss: 0.188117
2022-12-29 19:11: Train Epoch 8: 79/244 Loss: 0.259790
2022-12-29 19:11: Train Epoch 8: 83/244 Loss: 0.210795
2022-12-29 19:12: Train Epoch 8: 87/244 Loss: 0.203637
2022-12-29 19:12: Train Epoch 8: 91/244 Loss: 0.214772
2022-12-29 19:12: Train Epoch 8: 95/244 Loss: 0.227223
2022-12-29 19:12: Train Epoch 8: 99/244 Loss: 0.225829
2022-12-29 19:12: Train Epoch 8: 103/244 Loss: 0.276238
2022-12-29 19:12: Train Epoch 8: 107/244 Loss: 0.237937
2022-12-29 19:13: Train Epoch 8: 111/244 Loss: 0.186647
2022-12-29 19:13: Train Epoch 8: 115/244 Loss: 0.238982
2022-12-29 19:13: Train Epoch 8: 119/244 Loss: 0.180269
2022-12-29 19:13: Train Epoch 8: 123/244 Loss: 0.226940
2022-12-29 19:13: Train Epoch 8: 127/244 Loss: 0.187975
2022-12-29 19:14: Train Epoch 8: 131/244 Loss: 0.253866
2022-12-29 19:14: Train Epoch 8: 135/244 Loss: 0.311695
2022-12-29 19:14: Train Epoch 8: 139/244 Loss: 0.201016
2022-12-29 19:14: Train Epoch 8: 143/244 Loss: 0.346411
2022-12-29 19:14: Train Epoch 8: 147/244 Loss: 0.196929
2022-12-29 19:14: Train Epoch 8: 151/244 Loss: 0.251845
2022-12-29 19:15: Train Epoch 8: 155/244 Loss: 0.206408
2022-12-29 19:15: Train Epoch 8: 159/244 Loss: 0.269687
2022-12-29 19:15: Train Epoch 8: 163/244 Loss: 0.221034
2022-12-29 19:15: Train Epoch 8: 167/244 Loss: 0.357684
2022-12-29 19:15: Train Epoch 8: 171/244 Loss: 0.337477
2022-12-29 19:15: Train Epoch 8: 175/244 Loss: 0.281201
2022-12-29 19:16: Train Epoch 8: 179/244 Loss: 0.250603
2022-12-29 19:16: Train Epoch 8: 183/244 Loss: 0.249511
2022-12-29 19:16: Train Epoch 8: 187/244 Loss: 0.243317
2022-12-29 19:16: Train Epoch 8: 191/244 Loss: 0.197196
2022-12-29 19:16: Train Epoch 8: 195/244 Loss: 0.277289
2022-12-29 19:17: Train Epoch 8: 199/244 Loss: 0.238172
2022-12-29 19:17: Train Epoch 8: 203/244 Loss: 0.229029
2022-12-29 19:17: Train Epoch 8: 207/244 Loss: 0.277641
2022-12-29 19:17: Train Epoch 8: 211/244 Loss: 0.261590
2022-12-29 19:17: Train Epoch 8: 215/244 Loss: 0.164485
2022-12-29 19:17: Train Epoch 8: 219/244 Loss: 0.241865
2022-12-29 19:17: Train Epoch 8: 223/244 Loss: 0.257687
2022-12-29 19:18: Train Epoch 8: 227/244 Loss: 0.212979
2022-12-29 19:18: Train Epoch 8: 231/244 Loss: 0.225081
2022-12-29 19:18: Train Epoch 8: 235/244 Loss: 0.227288
2022-12-29 19:18: Train Epoch 8: 239/244 Loss: 0.240311
2022-12-29 19:18: Train Epoch 8: 243/244 Loss: 0.203460
2022-12-29 19:18: **********Train Epoch 8: averaged Loss: 0.232831 
2022-12-29 19:18: 
Epoch time elapsed: 644.1922605037689

2022-12-29 19:19: 
 metrics validation: {'precision': 0.7573170731707317, 'recall': 0.4776923076923077, 'f1-score': 0.5858490566037736, 'support': 1300, 'AUC': 0.7878488165680473, 'AUCPR': 0.7163482910803256, 'TP': 621, 'FP': 199, 'TN': 2401, 'FN': 679} 

2022-12-29 19:19: **********Val Epoch 8: average Loss: 0.338140
2022-12-29 19:20: 
 Testing metrics {'precision': 0.7631133671742809, 'recall': 0.7345276872964169, 'f1-score': 0.7485477178423235, 'support': 1228, 'AUC': 0.8419051528398179, 'AUCPR': 0.7494536136123621, 'TP': 902, 'FP': 280, 'TN': 2176, 'FN': 326} 

2022-12-29 19:20: Train Epoch 9: 3/244 Loss: 0.202902
2022-12-29 19:20: Train Epoch 9: 7/244 Loss: 0.197203
2022-12-29 19:21: Train Epoch 9: 11/244 Loss: 0.207142
2022-12-29 19:21: Train Epoch 9: 15/244 Loss: 0.223906
2022-12-29 19:21: Train Epoch 9: 19/244 Loss: 0.264942
2022-12-29 19:21: Train Epoch 9: 23/244 Loss: 0.211306
2022-12-29 19:21: Train Epoch 9: 27/244 Loss: 0.278688
2022-12-29 19:21: Train Epoch 9: 31/244 Loss: 0.208944
2022-12-29 19:22: Train Epoch 9: 35/244 Loss: 0.223538
2022-12-29 19:22: Train Epoch 9: 39/244 Loss: 0.258123
2022-12-29 19:22: Train Epoch 9: 43/244 Loss: 0.237522
2022-12-29 19:22: Train Epoch 9: 47/244 Loss: 0.200213
2022-12-29 19:22: Train Epoch 9: 51/244 Loss: 0.276821
2022-12-29 19:23: Train Epoch 9: 55/244 Loss: 0.206888
2022-12-29 19:23: Train Epoch 9: 59/244 Loss: 0.227056
2022-12-29 19:23: Train Epoch 9: 63/244 Loss: 0.223574
2022-12-29 19:23: Train Epoch 9: 67/244 Loss: 0.237096
2022-12-29 19:23: Train Epoch 9: 71/244 Loss: 0.189742
2022-12-29 19:23: Train Epoch 9: 75/244 Loss: 0.218267
2022-12-29 19:24: Train Epoch 9: 79/244 Loss: 0.184882
2022-12-29 19:24: Train Epoch 9: 83/244 Loss: 0.201451
2022-12-29 19:24: Train Epoch 9: 87/244 Loss: 0.192839
2022-12-29 19:24: Train Epoch 9: 91/244 Loss: 0.214082
2022-12-29 19:24: Train Epoch 9: 95/244 Loss: 0.174623
2022-12-29 19:24: Train Epoch 9: 99/244 Loss: 0.210602
2022-12-29 19:25: Train Epoch 9: 103/244 Loss: 0.224698
2022-12-29 19:25: Train Epoch 9: 107/244 Loss: 0.178565
2022-12-29 19:25: Train Epoch 9: 111/244 Loss: 0.202333
2022-12-29 19:25: Train Epoch 9: 115/244 Loss: 0.210187
2022-12-29 19:25: Train Epoch 9: 119/244 Loss: 0.184514
2022-12-29 19:26: Train Epoch 9: 123/244 Loss: 0.282730
2022-12-29 19:26: Train Epoch 9: 127/244 Loss: 0.192908
2022-12-29 19:26: Train Epoch 9: 131/244 Loss: 0.249195
2022-12-29 19:26: Train Epoch 9: 135/244 Loss: 0.196300
2022-12-29 19:26: Train Epoch 9: 139/244 Loss: 0.213908
2022-12-29 19:26: Train Epoch 9: 143/244 Loss: 0.215712
2022-12-29 19:27: Train Epoch 9: 147/244 Loss: 0.220412
2022-12-29 19:27: Train Epoch 9: 151/244 Loss: 0.187206
2022-12-29 19:27: Train Epoch 9: 155/244 Loss: 0.232510
2022-12-29 19:27: Train Epoch 9: 159/244 Loss: 0.270985
2022-12-29 19:27: Train Epoch 9: 163/244 Loss: 0.210147
2022-12-29 19:27: Train Epoch 9: 167/244 Loss: 0.248364
2022-12-29 19:28: Train Epoch 9: 171/244 Loss: 0.231416
2022-12-29 19:28: Train Epoch 9: 175/244 Loss: 0.254249
2022-12-29 19:28: Train Epoch 9: 179/244 Loss: 0.192408
2022-12-29 19:28: Train Epoch 9: 183/244 Loss: 0.293803
2022-12-29 19:28: Train Epoch 9: 187/244 Loss: 0.177682
2022-12-29 19:29: Train Epoch 9: 191/244 Loss: 0.248475
2022-12-29 19:29: Train Epoch 9: 195/244 Loss: 0.275006
2022-12-29 19:29: Train Epoch 9: 199/244 Loss: 0.280634
2022-12-29 19:29: Train Epoch 9: 203/244 Loss: 0.225249
2022-12-29 19:29: Train Epoch 9: 207/244 Loss: 0.213354
2022-12-29 19:29: Train Epoch 9: 211/244 Loss: 0.262108
2022-12-29 19:30: Train Epoch 9: 215/244 Loss: 0.197866
2022-12-29 19:30: Train Epoch 9: 219/244 Loss: 0.304526
2022-12-29 19:30: Train Epoch 9: 223/244 Loss: 0.210361
2022-12-29 19:30: Train Epoch 9: 227/244 Loss: 0.241548
2022-12-29 19:30: Train Epoch 9: 231/244 Loss: 0.205875
2022-12-29 19:31: Train Epoch 9: 235/244 Loss: 0.235061
2022-12-29 19:31: Train Epoch 9: 239/244 Loss: 0.231264
2022-12-29 19:31: Train Epoch 9: 243/244 Loss: 0.196020
2022-12-29 19:31: **********Train Epoch 9: averaged Loss: 0.224097 
2022-12-29 19:31: 
Epoch time elapsed: 648.1994652748108

2022-12-29 19:32: 
 metrics validation: {'precision': 0.5304528891202499, 'recall': 0.7838461538461539, 'f1-score': 0.6327227569077926, 'support': 1300, 'AUC': 0.7838766272189349, 'AUCPR': 0.7062618205694327, 'TP': 1019, 'FP': 902, 'TN': 1698, 'FN': 281} 

2022-12-29 19:32: **********Val Epoch 9: average Loss: 0.350626
2022-12-29 19:32: Validation performance didn't improve for 8 epochs. Training stops.
2022-12-29 19:32: Total training time: 103.6238min, best loss: 0.285038
2022-12-29 19:32: Saving current best model to /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122917483451445369118/best_model.pth
2022-12-29 19:32: 
 Testing metrics {'precision': 0.7631133671742809, 'recall': 0.7345276872964169, 'f1-score': 0.7485477178423235, 'support': 1228, 'AUC': 0.8419051528398179, 'AUCPR': 0.7494536136123621, 'TP': 902, 'FP': 280, 'TN': 2176, 'FN': 326} 

