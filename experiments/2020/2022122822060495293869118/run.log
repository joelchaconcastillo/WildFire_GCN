2022-12-28 22:06: log dir: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122822060495293869118
2022-12-28 22:06: Experiment log path in: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122822060495293869118
2022-12-28 22:06: Argument: Namespace(batch_size=256, clc='vec', cuda=True, dataset='2020', dataset_root='/home/joel.chacon/tmp/datasets_grl/', debug=False, default_graph=True, device='cpu', dynamic_features=['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh'], early_stop=True, early_stop_patience=8, embed_dim=128, epochs=30, grad_norm=False, horizon=1, input_dim=25, lag=10, link_len=2, log_dir='/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122822060495293869118', log_step=1, loss_func='nllloss', lr_decay=True, lr_decay_rate=0.1, lr_decay_step='15, 20', lr_init=0.0005, max_grad_norm=5, minbatch_size=256, mode='train', model='fire_GCN', nan_fill=0.5, num_layers=1, num_nodes=625, num_workers=20, output_dim=2, patch_height=25, patch_width=25, persistent_workers=True, pin_memory=True, plot=False, positive_weight=0.5, prefetch_factor=2, real_value=True, rnn_units=16, seed=10000, static_features=['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density'], teacher_forcing=False, weight_decay=0.0, window_len=10)
2022-12-28 22:06: Argument batch_size: 256
2022-12-28 22:06: Argument clc: 'vec'
2022-12-28 22:06: Argument cuda: True
2022-12-28 22:06: Argument dataset: '2020'
2022-12-28 22:06: Argument dataset_root: '/home/joel.chacon/tmp/datasets_grl/'
2022-12-28 22:06: Argument debug: False
2022-12-28 22:06: Argument default_graph: True
2022-12-28 22:06: Argument device: 'cpu'
2022-12-28 22:06: Argument dynamic_features: ['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh']
2022-12-28 22:06: Argument early_stop: True
2022-12-28 22:06: Argument early_stop_patience: 8
2022-12-28 22:06: Argument embed_dim: 128
2022-12-28 22:06: Argument epochs: 30
2022-12-28 22:06: Argument grad_norm: False
2022-12-28 22:06: Argument horizon: 1
2022-12-28 22:06: Argument input_dim: 25
2022-12-28 22:06: Argument lag: 10
2022-12-28 22:06: Argument link_len: 2
2022-12-28 22:06: Argument log_dir: '/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122822060495293869118'
2022-12-28 22:06: Argument log_step: 1
2022-12-28 22:06: Argument loss_func: 'nllloss'
2022-12-28 22:06: Argument lr_decay: True
2022-12-28 22:06: Argument lr_decay_rate: 0.1
2022-12-28 22:06: Argument lr_decay_step: '15, 20'
2022-12-28 22:06: Argument lr_init: 0.0005
2022-12-28 22:06: Argument max_grad_norm: 5
2022-12-28 22:06: Argument minbatch_size: 256
2022-12-28 22:06: Argument mode: 'train'
2022-12-28 22:06: Argument model: 'fire_GCN'
2022-12-28 22:06: Argument nan_fill: 0.5
2022-12-28 22:06: Argument num_layers: 1
2022-12-28 22:06: Argument num_nodes: 625
2022-12-28 22:06: Argument num_workers: 20
2022-12-28 22:06: Argument output_dim: 2
2022-12-28 22:06: Argument patch_height: 25
2022-12-28 22:06: Argument patch_width: 25
2022-12-28 22:06: Argument persistent_workers: True
2022-12-28 22:06: Argument pin_memory: True
2022-12-28 22:06: Argument plot: False
2022-12-28 22:06: Argument positive_weight: 0.5
2022-12-28 22:06: Argument prefetch_factor: 2
2022-12-28 22:06: Argument real_value: True
2022-12-28 22:06: Argument rnn_units: 16
2022-12-28 22:06: Argument seed: 10000
2022-12-28 22:06: Argument static_features: ['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density']
2022-12-28 22:06: Argument teacher_forcing: False
2022-12-28 22:06: Argument weight_decay: 0.0
2022-12-28 22:06: Argument window_len: 10
2022-12-28 22:06: Train Epoch 1: 0/61 Loss: 0.666698
2022-12-28 22:06: Train Epoch 1: 1/61 Loss: 1.027993
2022-12-28 22:06: Train Epoch 1: 2/61 Loss: 0.619488
2022-12-28 22:06: Train Epoch 1: 3/61 Loss: 0.634939
2022-12-28 22:07: Train Epoch 1: 4/61 Loss: 0.650613
2022-12-28 22:07: Train Epoch 1: 5/61 Loss: 0.626077
2022-12-28 22:07: Train Epoch 1: 6/61 Loss: 0.573650
2022-12-28 22:07: Train Epoch 1: 7/61 Loss: 0.615692
2022-12-28 22:07: Train Epoch 1: 8/61 Loss: 0.569344
2022-12-28 22:07: Train Epoch 1: 9/61 Loss: 0.627181
2022-12-28 22:07: Train Epoch 1: 10/61 Loss: 0.584810
2022-12-28 22:08: Train Epoch 1: 11/61 Loss: 0.584489
2022-12-28 22:08: Train Epoch 1: 12/61 Loss: 0.582001
2022-12-28 22:08: Train Epoch 1: 13/61 Loss: 0.551617
2022-12-28 22:08: Train Epoch 1: 14/61 Loss: 0.526654
2022-12-28 22:08: Train Epoch 1: 15/61 Loss: 0.537404
2022-12-28 22:08: Train Epoch 1: 16/61 Loss: 0.537995
2022-12-28 22:08: Train Epoch 1: 17/61 Loss: 0.552777
2022-12-28 22:09: Train Epoch 1: 18/61 Loss: 0.504997
2022-12-28 22:09: Train Epoch 1: 19/61 Loss: 0.473122
2022-12-28 22:09: Train Epoch 1: 20/61 Loss: 0.496182
2022-12-28 22:09: Train Epoch 1: 21/61 Loss: 0.470517
2022-12-28 22:09: Train Epoch 1: 22/61 Loss: 0.444916
2022-12-28 22:09: Train Epoch 1: 23/61 Loss: 0.469385
2022-12-28 22:10: Train Epoch 1: 24/61 Loss: 0.439688
2022-12-28 22:10: Train Epoch 1: 25/61 Loss: 0.416271
2022-12-28 22:10: Train Epoch 1: 26/61 Loss: 0.419527
2022-12-28 22:10: Train Epoch 1: 27/61 Loss: 0.382554
2022-12-28 22:10: Train Epoch 1: 28/61 Loss: 0.396404
2022-12-28 22:10: Train Epoch 1: 29/61 Loss: 0.393209
2022-12-28 22:11: Train Epoch 1: 30/61 Loss: 0.354038
2022-12-28 22:11: Train Epoch 1: 31/61 Loss: 0.318739
2022-12-28 22:11: Train Epoch 1: 32/61 Loss: 0.317872
2022-12-28 22:11: Train Epoch 1: 33/61 Loss: 0.347591
2022-12-28 22:11: Train Epoch 1: 34/61 Loss: 0.306904
2022-12-28 22:11: Train Epoch 1: 35/61 Loss: 0.303286
2022-12-28 22:12: Train Epoch 1: 36/61 Loss: 0.279752
2022-12-28 22:12: Train Epoch 1: 37/61 Loss: 0.299193
2022-12-28 22:12: Train Epoch 1: 38/61 Loss: 0.362633
2022-12-28 22:12: Train Epoch 1: 39/61 Loss: 0.263740
2022-12-28 22:12: Train Epoch 1: 40/61 Loss: 0.250558
2022-12-28 22:12: Train Epoch 1: 41/61 Loss: 0.251526
2022-12-28 22:12: Train Epoch 1: 42/61 Loss: 0.279121
2022-12-28 22:13: Train Epoch 1: 43/61 Loss: 0.296223
2022-12-28 22:13: Train Epoch 1: 44/61 Loss: 0.262135
2022-12-28 22:13: Train Epoch 1: 45/61 Loss: 0.279952
2022-12-28 22:13: Train Epoch 1: 46/61 Loss: 0.281514
2022-12-28 22:13: Train Epoch 1: 47/61 Loss: 0.258509
2022-12-28 22:13: Train Epoch 1: 48/61 Loss: 0.300650
2022-12-28 22:14: Train Epoch 1: 49/61 Loss: 0.289930
2022-12-28 22:14: Train Epoch 1: 50/61 Loss: 0.277963
2022-12-28 22:14: Train Epoch 1: 51/61 Loss: 0.282626
2022-12-28 22:14: Train Epoch 1: 52/61 Loss: 0.233802
2022-12-28 22:14: Train Epoch 1: 53/61 Loss: 0.220067
2022-12-28 22:14: Train Epoch 1: 54/61 Loss: 0.291360
2022-12-28 22:14: Train Epoch 1: 55/61 Loss: 0.259484
2022-12-28 22:15: Train Epoch 1: 56/61 Loss: 0.239641
2022-12-28 22:15: Train Epoch 1: 57/61 Loss: 0.199339
2022-12-28 22:15: Train Epoch 1: 58/61 Loss: 0.222724
2022-12-28 22:15: Train Epoch 1: 59/61 Loss: 0.252161
2022-12-28 22:15: Train Epoch 1: 60/61 Loss: 0.182151
2022-12-28 22:15: **********Train Epoch 1: averaged Loss: 0.408875 
2022-12-28 22:15: 
Epoch time elapsed: 584.9268898963928

2022-12-28 22:16: 
 metrics validation: {'precision': 0.8091236494597839, 'recall': 0.5184615384615384, 'f1-score': 0.6319737458977965, 'support': 1300, 'AUC': 0.8008486686390532, 'AUCPR': 0.7244490777460119, 'TP': 674, 'FP': 159, 'TN': 2441, 'FN': 626} 

2022-12-28 22:16: **********Val Epoch 1: average Loss: 0.628305
2022-12-28 22:16: *********************************Current best model saved!
2022-12-28 22:17: 
 Testing metrics {'precision': 0.8357988165680473, 'recall': 0.4600977198697068, 'f1-score': 0.5934873949579832, 'support': 1228, 'AUC': 0.8606396022769471, 'AUCPR': 0.7949954694777288, 'TP': 565, 'FP': 111, 'TN': 2345, 'FN': 663} 

2022-12-28 22:17: Train Epoch 2: 0/61 Loss: 0.330654
2022-12-28 22:17: Train Epoch 2: 1/61 Loss: 0.269353
2022-12-28 22:18: Train Epoch 2: 2/61 Loss: 0.200091
2022-12-28 22:18: Train Epoch 2: 3/61 Loss: 0.265012
2022-12-28 22:18: Train Epoch 2: 4/61 Loss: 0.269526
2022-12-28 22:18: Train Epoch 2: 5/61 Loss: 0.219168
2022-12-28 22:18: Train Epoch 2: 6/61 Loss: 0.235502
2022-12-28 22:19: Train Epoch 2: 7/61 Loss: 0.211366
2022-12-28 22:19: Train Epoch 2: 8/61 Loss: 0.237956
2022-12-28 22:19: Train Epoch 2: 9/61 Loss: 0.245669
2022-12-28 22:19: Train Epoch 2: 10/61 Loss: 0.251464
2022-12-28 22:19: Train Epoch 2: 11/61 Loss: 0.272693
2022-12-28 22:20: Train Epoch 2: 12/61 Loss: 0.223752
2022-12-28 22:20: Train Epoch 2: 13/61 Loss: 0.216431
2022-12-28 22:20: Train Epoch 2: 14/61 Loss: 0.234933
2022-12-28 22:21: Train Epoch 2: 15/61 Loss: 0.197226
2022-12-28 22:21: Train Epoch 2: 16/61 Loss: 0.245014
2022-12-28 22:21: Train Epoch 2: 17/61 Loss: 0.222024
2022-12-28 22:21: Train Epoch 2: 18/61 Loss: 0.217214
2022-12-28 22:22: Train Epoch 2: 19/61 Loss: 0.242781
2022-12-28 22:22: Train Epoch 2: 20/61 Loss: 0.202640
2022-12-28 22:22: Train Epoch 2: 21/61 Loss: 0.202101
2022-12-28 22:22: Train Epoch 2: 22/61 Loss: 0.239179
2022-12-28 22:23: Train Epoch 2: 23/61 Loss: 0.261170
2022-12-28 22:23: Train Epoch 2: 24/61 Loss: 0.195309
2022-12-28 22:23: Train Epoch 2: 25/61 Loss: 0.367214
2022-12-28 22:23: Train Epoch 2: 26/61 Loss: 0.201902
2022-12-28 22:24: Train Epoch 2: 27/61 Loss: 0.297706
2022-12-28 22:24: Train Epoch 2: 28/61 Loss: 0.300889
2022-12-28 22:24: Train Epoch 2: 29/61 Loss: 0.166754
2022-12-28 22:24: Train Epoch 2: 30/61 Loss: 0.295418
2022-12-28 22:24: Train Epoch 2: 31/61 Loss: 0.323423
2022-12-28 22:25: Train Epoch 2: 32/61 Loss: 0.174391
2022-12-28 22:25: Train Epoch 2: 33/61 Loss: 0.410514
2022-12-28 22:25: Train Epoch 2: 34/61 Loss: 0.253000
2022-12-28 22:25: Train Epoch 2: 35/61 Loss: 0.238290
2022-12-28 22:26: Train Epoch 2: 36/61 Loss: 0.350746
2022-12-28 22:26: Train Epoch 2: 37/61 Loss: 0.379623
2022-12-28 22:26: Train Epoch 2: 38/61 Loss: 0.152431
2022-12-28 22:26: Train Epoch 2: 39/61 Loss: 0.289780
2022-12-28 22:27: Train Epoch 2: 40/61 Loss: 0.297854
2022-12-28 22:27: Train Epoch 2: 41/61 Loss: 0.177097
2022-12-28 22:27: Train Epoch 2: 42/61 Loss: 0.292104
2022-12-28 22:27: Train Epoch 2: 43/61 Loss: 0.252338
2022-12-28 22:27: Train Epoch 2: 44/61 Loss: 0.305323
2022-12-28 22:28: Train Epoch 2: 45/61 Loss: 0.236830
2022-12-28 22:28: Train Epoch 2: 46/61 Loss: 0.239153
2022-12-28 22:28: Train Epoch 2: 47/61 Loss: 0.262872
2022-12-28 22:28: Train Epoch 2: 48/61 Loss: 0.214314
2022-12-28 22:29: Train Epoch 2: 49/61 Loss: 0.193170
2022-12-28 22:29: Train Epoch 2: 50/61 Loss: 0.208856
2022-12-28 22:29: Train Epoch 2: 51/61 Loss: 0.249901
2022-12-28 22:29: Train Epoch 2: 52/61 Loss: 0.224186
2022-12-28 22:29: Train Epoch 2: 53/61 Loss: 0.235040
2022-12-28 22:30: Train Epoch 2: 54/61 Loss: 0.192783
2022-12-28 22:30: Train Epoch 2: 55/61 Loss: 0.160641
2022-12-28 22:30: Train Epoch 2: 56/61 Loss: 0.193477
2022-12-28 22:30: Train Epoch 2: 57/61 Loss: 0.143410
2022-12-28 22:31: Train Epoch 2: 58/61 Loss: 0.194312
2022-12-28 22:31: Train Epoch 2: 59/61 Loss: 0.185423
2022-12-28 22:31: Train Epoch 2: 60/61 Loss: 0.227096
2022-12-28 22:31: **********Train Epoch 2: averaged Loss: 0.242565 
2022-12-28 22:31: 
Epoch time elapsed: 847.2057249546051

2022-12-28 22:32: 
 metrics validation: {'precision': 0.7958477508650519, 'recall': 0.5307692307692308, 'f1-score': 0.63682510383018, 'support': 1300, 'AUC': 0.8191156804733728, 'AUCPR': 0.7375746689197764, 'TP': 690, 'FP': 177, 'TN': 2423, 'FN': 610} 

2022-12-28 22:32: **********Val Epoch 2: average Loss: 0.620806
2022-12-28 22:32: *********************************Current best model saved!
2022-12-28 22:34: 
 Testing metrics {'precision': 0.8577023498694517, 'recall': 0.5350162866449512, 'f1-score': 0.6589769307923772, 'support': 1228, 'AUC': 0.8732460026101073, 'AUCPR': 0.814775661250705, 'TP': 657, 'FP': 109, 'TN': 2347, 'FN': 571} 

2022-12-28 22:34: Train Epoch 3: 0/61 Loss: 0.198882
2022-12-28 22:34: Train Epoch 3: 1/61 Loss: 0.187894
2022-12-28 22:35: Train Epoch 3: 2/61 Loss: 0.181233
2022-12-28 22:35: Train Epoch 3: 3/61 Loss: 0.182837
2022-12-28 22:35: Train Epoch 3: 4/61 Loss: 0.203062
2022-12-28 22:35: Train Epoch 3: 5/61 Loss: 0.230576
2022-12-28 22:35: Train Epoch 3: 6/61 Loss: 0.152809
2022-12-28 22:36: Train Epoch 3: 7/61 Loss: 0.169200
2022-12-28 22:36: Train Epoch 3: 8/61 Loss: 0.333169
2022-12-28 22:36: Train Epoch 3: 9/61 Loss: 0.234668
2022-12-28 22:36: Train Epoch 3: 10/61 Loss: 0.242213
2022-12-28 22:37: Train Epoch 3: 11/61 Loss: 0.202368
2022-12-28 22:37: Train Epoch 3: 12/61 Loss: 0.191928
2022-12-28 22:37: Train Epoch 3: 13/61 Loss: 0.247456
2022-12-28 22:37: Train Epoch 3: 14/61 Loss: 0.160921
2022-12-28 22:37: Train Epoch 3: 15/61 Loss: 0.197913
2022-12-28 22:38: Train Epoch 3: 16/61 Loss: 0.198287
2022-12-28 22:38: Train Epoch 3: 17/61 Loss: 0.197907
2022-12-28 22:38: Train Epoch 3: 18/61 Loss: 0.138603
2022-12-28 22:38: Train Epoch 3: 19/61 Loss: 0.180976
2022-12-28 22:39: Train Epoch 3: 20/61 Loss: 0.164190
2022-12-28 22:39: Train Epoch 3: 21/61 Loss: 0.216393
2022-12-28 22:39: Train Epoch 3: 22/61 Loss: 0.212191
2022-12-28 22:39: Train Epoch 3: 23/61 Loss: 0.151111
