2022-12-18 14:50: log dir: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/20221218145048
2022-12-18 14:50: Experiment log path in: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/20221218145048
2022-12-18 14:50: Argument: Namespace(batch_size=256, clc='vec', cuda=True, dataset='2020', dataset_root='/home/joel.chacon/tmp/datasets_grl/', debug=False, default_graph=True, device='cpu', dynamic_features=['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh'], early_stop=True, early_stop_patience=8, embed_dim=50, epochs=1, gamma=1.0, grad_norm=False, horizon=1, input_dim=25, lag=10, link_len=2, log_dir='/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/20221218145048', log_step=1, loss_func='nllloss', lr_decay=True, lr_decay_rate=0.3, lr_decay_step='10, 15, 20, 25', lr_init=0.003, mae_thresh=None, mape_thresh=0.0, max_grad_norm=5, mode='train', model='fire_GCN', nan_fill=0.5, num_layers=1, num_nodes=625, num_workers=12, output_dim=2, patch_height=25, patch_width=25, persistent_workers=True, pin_memory=True, plot=False, positive_weight=0.5, prefetch_factor=2, real_value=True, rnn_units=64, seed=1992, static_features=['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density'], teacher_forcing=False, test_ratio=0.2, val_ratio=0.2, weight_decay=0.01, window_len=10)
2022-12-18 14:50: Argument batch_size: 256
2022-12-18 14:50: Argument clc: 'vec'
2022-12-18 14:50: Argument cuda: True
2022-12-18 14:50: Argument dataset: '2020'
2022-12-18 14:50: Argument dataset_root: '/home/joel.chacon/tmp/datasets_grl/'
2022-12-18 14:50: Argument debug: False
2022-12-18 14:50: Argument default_graph: True
2022-12-18 14:50: Argument device: 'cpu'
2022-12-18 14:50: Argument dynamic_features: ['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh']
2022-12-18 14:50: Argument early_stop: True
2022-12-18 14:50: Argument early_stop_patience: 8
2022-12-18 14:50: Argument embed_dim: 50
2022-12-18 14:50: Argument epochs: 1
2022-12-18 14:50: Argument gamma: 1.0
2022-12-18 14:50: Argument grad_norm: False
2022-12-18 14:50: Argument horizon: 1
2022-12-18 14:50: Argument input_dim: 25
2022-12-18 14:50: Argument lag: 10
2022-12-18 14:50: Argument link_len: 2
2022-12-18 14:50: Argument log_dir: '/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/20221218145048'
2022-12-18 14:50: Argument log_step: 1
2022-12-18 14:50: Argument loss_func: 'nllloss'
2022-12-18 14:50: Argument lr_decay: True
2022-12-18 14:50: Argument lr_decay_rate: 0.3
2022-12-18 14:50: Argument lr_decay_step: '10, 15, 20, 25'
2022-12-18 14:50: Argument lr_init: 0.003
2022-12-18 14:50: Argument mae_thresh: None
2022-12-18 14:50: Argument mape_thresh: 0.0
2022-12-18 14:50: Argument max_grad_norm: 5
2022-12-18 14:50: Argument mode: 'train'
2022-12-18 14:50: Argument model: 'fire_GCN'
2022-12-18 14:50: Argument nan_fill: 0.5
2022-12-18 14:50: Argument num_layers: 1
2022-12-18 14:50: Argument num_nodes: 625
2022-12-18 14:50: Argument num_workers: 12
2022-12-18 14:50: Argument output_dim: 2
2022-12-18 14:50: Argument patch_height: 25
2022-12-18 14:50: Argument patch_width: 25
2022-12-18 14:50: Argument persistent_workers: True
2022-12-18 14:50: Argument pin_memory: True
2022-12-18 14:50: Argument plot: False
2022-12-18 14:50: Argument positive_weight: 0.5
2022-12-18 14:50: Argument prefetch_factor: 2
2022-12-18 14:50: Argument real_value: True
2022-12-18 14:50: Argument rnn_units: 64
2022-12-18 14:50: Argument seed: 1992
2022-12-18 14:50: Argument static_features: ['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density']
2022-12-18 14:50: Argument teacher_forcing: False
2022-12-18 14:50: Argument test_ratio: 0.2
2022-12-18 14:50: Argument val_ratio: 0.2
2022-12-18 14:50: Argument weight_decay: 0.01
2022-12-18 14:50: Argument window_len: 10
2022-12-18 14:51: Train Epoch 1: 0/159 Loss: 0.749898
2022-12-18 14:56: Train Epoch 1: 1/159 Loss: 25.942617
2022-12-18 14:56: Train Epoch 1: 2/159 Loss: 16.293940
2022-12-18 14:56: Train Epoch 1: 3/159 Loss: 1.080029
2022-12-18 14:57: Train Epoch 1: 4/159 Loss: 28.929909
2022-12-18 14:57: Train Epoch 1: 5/159 Loss: 21.632919
2022-12-18 14:58: Train Epoch 1: 6/159 Loss: 9.085131
2022-12-18 14:58: Train Epoch 1: 7/159 Loss: 4.416453
2022-12-18 14:58: Train Epoch 1: 8/159 Loss: 8.558764
2022-12-18 14:59: Train Epoch 1: 9/159 Loss: 9.957477
2022-12-18 14:59: Train Epoch 1: 10/159 Loss: 9.327469
2022-12-18 14:59: Train Epoch 1: 11/159 Loss: 6.219172
2022-12-18 15:00: Train Epoch 1: 12/159 Loss: 1.566095
2022-12-18 15:00: Train Epoch 1: 13/159 Loss: 6.587301
2022-12-18 15:01: Train Epoch 1: 14/159 Loss: 9.190121
2022-12-18 15:01: Train Epoch 1: 15/159 Loss: 6.859452
2022-12-18 15:01: Train Epoch 1: 16/159 Loss: 1.587290
2022-12-18 15:02: Train Epoch 1: 17/159 Loss: 1.971039
2022-12-18 15:02: Train Epoch 1: 18/159 Loss: 3.522469
2022-12-18 15:02: Train Epoch 1: 19/159 Loss: 4.219979
2022-12-18 15:03: Train Epoch 1: 20/159 Loss: 4.116140
2022-12-18 15:03: Train Epoch 1: 21/159 Loss: 3.275733
2022-12-18 15:04: Train Epoch 1: 22/159 Loss: 2.923275
2022-12-18 15:04: Train Epoch 1: 23/159 Loss: 1.864082
2022-12-18 15:04: Train Epoch 1: 24/159 Loss: 0.568228
2022-12-18 15:05: Train Epoch 1: 25/159 Loss: 2.525731
2022-12-18 15:05: Train Epoch 1: 26/159 Loss: 2.879741
2022-12-18 15:05: Train Epoch 1: 27/159 Loss: 2.076191
2022-12-18 15:06: Train Epoch 1: 28/159 Loss: 0.695349
2022-12-18 15:06: Train Epoch 1: 29/159 Loss: 0.616116
2022-12-18 15:06: Train Epoch 1: 30/159 Loss: 1.141611
2022-12-18 15:07: Train Epoch 1: 31/159 Loss: 1.318483
2022-12-18 15:07: Train Epoch 1: 32/159 Loss: 1.182115
2022-12-18 15:08: Train Epoch 1: 33/159 Loss: 1.184145
2022-12-18 15:08: Train Epoch 1: 34/159 Loss: 0.815152
2022-12-18 15:08: Train Epoch 1: 35/159 Loss: 0.566517
2022-12-18 15:09: Train Epoch 1: 36/159 Loss: 0.455890
2022-12-18 15:09: Train Epoch 1: 37/159 Loss: 0.622925
2022-12-18 15:09: Train Epoch 1: 38/159 Loss: 0.889968
2022-12-18 15:10: Train Epoch 1: 39/159 Loss: 0.791222
2022-12-18 15:10: Train Epoch 1: 40/159 Loss: 0.677098
2022-12-18 15:11: Train Epoch 1: 41/159 Loss: 0.470059
2022-12-18 15:11: Train Epoch 1: 42/159 Loss: 0.418478
2022-12-18 15:11: Train Epoch 1: 43/159 Loss: 0.528695
2022-12-18 15:12: Train Epoch 1: 44/159 Loss: 0.587043
2022-12-18 15:12: Train Epoch 1: 45/159 Loss: 0.518414
2022-12-18 15:12: Train Epoch 1: 46/159 Loss: 0.520193
2022-12-18 15:13: Train Epoch 1: 47/159 Loss: 0.576544
2022-12-18 15:13: Train Epoch 1: 48/159 Loss: 0.394616
2022-12-18 15:14: Train Epoch 1: 49/159 Loss: 0.407131
2022-12-18 15:14: Train Epoch 1: 50/159 Loss: 0.358670
2022-12-18 15:14: Train Epoch 1: 51/159 Loss: 0.418141
2022-12-18 15:15: Train Epoch 1: 52/159 Loss: 0.391145
2022-12-18 15:15: Train Epoch 1: 53/159 Loss: 0.414572
2022-12-18 15:15: Train Epoch 1: 54/159 Loss: 0.456575
2022-12-18 15:16: Train Epoch 1: 55/159 Loss: 0.435245
2022-12-18 15:16: Train Epoch 1: 56/159 Loss: 0.352196
2022-12-18 15:17: Train Epoch 1: 57/159 Loss: 0.302674
2022-12-18 15:17: Train Epoch 1: 58/159 Loss: 0.353723
2022-12-18 15:17: Train Epoch 1: 59/159 Loss: 0.374021
2022-12-18 15:18: Train Epoch 1: 60/159 Loss: 0.359105
2022-12-18 15:18: Train Epoch 1: 61/159 Loss: 0.364627
2022-12-18 15:19: Train Epoch 1: 62/159 Loss: 0.418798
2022-12-18 15:19: Train Epoch 1: 63/159 Loss: 0.365175
2022-12-18 15:20: Train Epoch 1: 64/159 Loss: 0.432059
2022-12-18 15:21: Train Epoch 1: 65/159 Loss: 0.355225
2022-12-18 15:21: Train Epoch 1: 66/159 Loss: 0.382559
2022-12-18 15:22: Train Epoch 1: 67/159 Loss: 0.334851
2022-12-18 15:22: Train Epoch 1: 68/159 Loss: 0.354297
2022-12-18 15:23: Train Epoch 1: 69/159 Loss: 0.353190
2022-12-18 15:23: Train Epoch 1: 70/159 Loss: 0.346179
2022-12-18 15:24: Train Epoch 1: 71/159 Loss: 0.319927
2022-12-18 15:24: Train Epoch 1: 72/159 Loss: 0.357846
2022-12-18 15:25: Train Epoch 1: 73/159 Loss: 0.349898
2022-12-18 15:26: Train Epoch 1: 74/159 Loss: 0.327139
2022-12-18 15:26: Train Epoch 1: 75/159 Loss: 0.360658
2022-12-18 15:27: Train Epoch 1: 76/159 Loss: 0.388856
2022-12-18 15:27: Train Epoch 1: 77/159 Loss: 0.330537
2022-12-18 15:28: Train Epoch 1: 78/159 Loss: 0.335642
2022-12-18 15:28: Train Epoch 1: 79/159 Loss: 0.342677
2022-12-18 15:29: Train Epoch 1: 80/159 Loss: 0.294963
2022-12-18 15:29: Train Epoch 1: 81/159 Loss: 0.377933
2022-12-18 15:30: Train Epoch 1: 82/159 Loss: 0.317961
2022-12-18 15:31: Train Epoch 1: 83/159 Loss: 0.275443
2022-12-18 15:31: Train Epoch 1: 84/159 Loss: 0.392436
2022-12-18 15:32: Train Epoch 1: 85/159 Loss: 0.379517
2022-12-18 15:32: Train Epoch 1: 86/159 Loss: 0.310658
2022-12-18 15:33: Train Epoch 1: 87/159 Loss: 0.297485
2022-12-18 15:33: Train Epoch 1: 88/159 Loss: 0.391398
2022-12-18 15:34: Train Epoch 1: 89/159 Loss: 0.306144
2022-12-18 15:35: Train Epoch 1: 90/159 Loss: 0.296512
2022-12-18 15:35: Train Epoch 1: 91/159 Loss: 0.275263
2022-12-18 15:36: Train Epoch 1: 92/159 Loss: 0.341294
2022-12-18 15:36: Train Epoch 1: 93/159 Loss: 0.327931
2022-12-18 15:37: Train Epoch 1: 94/159 Loss: 0.315261
2022-12-18 15:37: Train Epoch 1: 95/159 Loss: 0.280359
2022-12-18 15:38: Train Epoch 1: 96/159 Loss: 0.258263
2022-12-18 15:39: Train Epoch 1: 97/159 Loss: 0.309268
2022-12-18 15:39: Train Epoch 1: 98/159 Loss: 0.304673
2022-12-18 15:40: Train Epoch 1: 99/159 Loss: 0.271997
2022-12-18 15:40: Train Epoch 1: 100/159 Loss: 0.302603
2022-12-18 15:41: Train Epoch 1: 101/159 Loss: 0.328400
2022-12-18 15:41: Train Epoch 1: 102/159 Loss: 0.329246
2022-12-18 15:42: Train Epoch 1: 103/159 Loss: 0.296861
2022-12-18 15:43: Train Epoch 1: 104/159 Loss: 0.287916
2022-12-18 15:43: Train Epoch 1: 105/159 Loss: 0.278990
2022-12-18 15:44: Train Epoch 1: 106/159 Loss: 0.335797
2022-12-18 15:44: Train Epoch 1: 107/159 Loss: 0.306546
2022-12-18 15:45: Train Epoch 1: 108/159 Loss: 0.324933
2022-12-18 15:45: Train Epoch 1: 109/159 Loss: 0.255513
2022-12-18 15:46: Train Epoch 1: 110/159 Loss: 0.295619
2022-12-18 15:47: Train Epoch 1: 111/159 Loss: 0.290852
2022-12-18 15:47: Train Epoch 1: 112/159 Loss: 0.309051
2022-12-18 15:48: Train Epoch 1: 113/159 Loss: 0.307270
2022-12-18 15:48: Train Epoch 1: 114/159 Loss: 0.220227
2022-12-18 15:49: Train Epoch 1: 115/159 Loss: 0.247702
2022-12-18 15:49: Train Epoch 1: 116/159 Loss: 0.333786
2022-12-18 15:50: Train Epoch 1: 117/159 Loss: 0.362533
2022-12-18 15:51: Train Epoch 1: 118/159 Loss: 0.270579
2022-12-18 15:51: Train Epoch 1: 119/159 Loss: 0.291064
2022-12-18 15:52: Train Epoch 1: 120/159 Loss: 0.337977
2022-12-18 15:52: Train Epoch 1: 121/159 Loss: 0.411778
2022-12-18 15:53: Train Epoch 1: 122/159 Loss: 0.310783
2022-12-18 15:53: Train Epoch 1: 123/159 Loss: 0.400242
2022-12-18 15:54: Train Epoch 1: 124/159 Loss: 0.278295
2022-12-18 15:55: Train Epoch 1: 125/159 Loss: 0.263811
2022-12-18 15:55: Train Epoch 1: 126/159 Loss: 0.244415
2022-12-18 15:56: Train Epoch 1: 127/159 Loss: 0.358105
2022-12-18 15:56: Train Epoch 1: 128/159 Loss: 0.297781
2022-12-18 15:57: Train Epoch 1: 129/159 Loss: 0.261603
2022-12-18 15:57: Train Epoch 1: 130/159 Loss: 0.264200
2022-12-18 15:58: Train Epoch 1: 131/159 Loss: 0.369173
2022-12-18 15:59: Train Epoch 1: 132/159 Loss: 0.329683
2022-12-18 15:59: Train Epoch 1: 133/159 Loss: 0.310005
2022-12-18 16:00: Train Epoch 1: 134/159 Loss: 0.274805
2022-12-18 16:00: Train Epoch 1: 135/159 Loss: 0.251843
2022-12-18 16:01: Train Epoch 1: 136/159 Loss: 0.270106
2022-12-18 16:01: Train Epoch 1: 137/159 Loss: 0.309975
2022-12-18 16:02: Train Epoch 1: 138/159 Loss: 0.326097
2022-12-18 16:02: Train Epoch 1: 139/159 Loss: 0.320807
2022-12-18 16:03: Train Epoch 1: 140/159 Loss: 0.237599
2022-12-18 16:04: Train Epoch 1: 141/159 Loss: 0.321470
2022-12-18 16:04: Train Epoch 1: 142/159 Loss: 0.229752
2022-12-18 16:05: Train Epoch 1: 143/159 Loss: 0.280230
2022-12-18 16:05: Train Epoch 1: 144/159 Loss: 0.314643
2022-12-18 16:06: Train Epoch 1: 145/159 Loss: 0.315666
2022-12-18 16:06: Train Epoch 1: 146/159 Loss: 0.324299
2022-12-18 16:07: Train Epoch 1: 147/159 Loss: 0.297022
2022-12-18 16:07: Train Epoch 1: 148/159 Loss: 0.310550
2022-12-18 16:08: Train Epoch 1: 149/159 Loss: 0.334046
2022-12-18 16:09: Train Epoch 1: 150/159 Loss: 0.308626
2022-12-18 16:09: Train Epoch 1: 151/159 Loss: 0.275135
2022-12-18 16:10: Train Epoch 1: 152/159 Loss: 0.311879
2022-12-18 16:10: Train Epoch 1: 153/159 Loss: 0.313926
2022-12-18 16:11: Train Epoch 1: 154/159 Loss: 0.336351
2022-12-18 16:11: Train Epoch 1: 155/159 Loss: 0.326698
2022-12-18 16:12: Train Epoch 1: 156/159 Loss: 0.335264
2022-12-18 16:12: Train Epoch 1: 157/159 Loss: 0.354499
2022-12-18 16:13: Train Epoch 1: 158/159 Loss: 0.217260
2022-12-18 16:13: **********Train Epoch 1: averaged Loss: 1.563379 
2022-12-18 16:13: 
Epoch time elapsed: 4935.1625735759735

2022-12-18 16:15: 
 metrics validation: {'precision': 0.7914893617021277, 'recall': 0.5723076923076923, 'f1-score': 0.6642857142857141, 'support': 1300, 'AUC': 0.8275147928994083, 'AUCPR': 0.7410993389217684, 'TP': 744, 'FP': 196, 'TN': 2404, 'FN': 556} 

2022-12-18 16:15: **********Val Epoch 1: average Loss: 0.559661
2022-12-18 16:15: *********************************Current best model saved!
2022-12-18 16:15: Total training time: 84.4860min, best loss: 0.559661
2022-12-18 16:15: Saving current best model to /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/20221218145048/best_model.pth
2022-12-18 16:17: 
 Testing metrics {'precision': 0.8228299643281808, 'recall': 0.5635179153094463, 'f1-score': 0.6689221846302562, 'support': 1228, 'AUC': 0.8593426720707913, 'AUCPR': 0.7833673581271422, 'TP': 692, 'FP': 149, 'TN': 2307, 'FN': 536} 

