2022-12-29 17:30: log dir: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122917301260752769118
2022-12-29 17:30: Experiment log path in: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122917301260752769118
2022-12-29 17:30: Argument: Namespace(batch_size=256, clc='vec', cuda=True, dataset='2020', dataset_root='/home/joel.chacon/tmp/datasets_grl/', debug=False, default_graph=True, device='cpu', dynamic_features=['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh'], early_stop=True, early_stop_patience=8, embed_dim=128, epochs=30, grad_norm=False, horizon=1, input_dim=25, lag=10, link_len=2, log_dir='/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122917301260752769118', log_step=1, loss_func='nllloss', lr_decay=True, lr_decay_rate=0.1, lr_decay_step='15, 20', lr_init=0.0005, max_grad_norm=5, minbatch_size=64, mode='train', model='fire_GCN', nan_fill=0.5, num_layers=1, num_nodes=625, num_workers=20, output_dim=2, patch_height=25, patch_width=25, persistent_workers=True, pin_memory=True, plot=False, positive_weight=0.5, prefetch_factor=2, real_value=True, rnn_units=16, seed=10000, static_features=['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density'], teacher_forcing=False, weight_decay=0.0, window_len=10)
2022-12-29 17:30: Argument batch_size: 256
2022-12-29 17:30: Argument clc: 'vec'
2022-12-29 17:30: Argument cuda: True
2022-12-29 17:30: Argument dataset: '2020'
2022-12-29 17:30: Argument dataset_root: '/home/joel.chacon/tmp/datasets_grl/'
2022-12-29 17:30: Argument debug: False
2022-12-29 17:30: Argument default_graph: True
2022-12-29 17:30: Argument device: 'cpu'
2022-12-29 17:30: Argument dynamic_features: ['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh']
2022-12-29 17:30: Argument early_stop: True
2022-12-29 17:30: Argument early_stop_patience: 8
2022-12-29 17:30: Argument embed_dim: 128
2022-12-29 17:30: Argument epochs: 30
2022-12-29 17:30: Argument grad_norm: False
2022-12-29 17:30: Argument horizon: 1
2022-12-29 17:30: Argument input_dim: 25
2022-12-29 17:30: Argument lag: 10
2022-12-29 17:30: Argument link_len: 2
2022-12-29 17:30: Argument log_dir: '/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122917301260752769118'
2022-12-29 17:30: Argument log_step: 1
2022-12-29 17:30: Argument loss_func: 'nllloss'
2022-12-29 17:30: Argument lr_decay: True
2022-12-29 17:30: Argument lr_decay_rate: 0.1
2022-12-29 17:30: Argument lr_decay_step: '15, 20'
2022-12-29 17:30: Argument lr_init: 0.0005
2022-12-29 17:30: Argument max_grad_norm: 5
2022-12-29 17:30: Argument minbatch_size: 64
2022-12-29 17:30: Argument mode: 'train'
2022-12-29 17:30: Argument model: 'fire_GCN'
2022-12-29 17:30: Argument nan_fill: 0.5
2022-12-29 17:30: Argument num_layers: 1
2022-12-29 17:30: Argument num_nodes: 625
2022-12-29 17:30: Argument num_workers: 20
2022-12-29 17:30: Argument output_dim: 2
2022-12-29 17:30: Argument patch_height: 25
2022-12-29 17:30: Argument patch_width: 25
2022-12-29 17:30: Argument persistent_workers: True
2022-12-29 17:30: Argument pin_memory: True
2022-12-29 17:30: Argument plot: False
2022-12-29 17:30: Argument positive_weight: 0.5
2022-12-29 17:30: Argument prefetch_factor: 2
2022-12-29 17:30: Argument real_value: True
2022-12-29 17:30: Argument rnn_units: 16
2022-12-29 17:30: Argument seed: 10000
2022-12-29 17:30: Argument static_features: ['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density']
2022-12-29 17:30: Argument teacher_forcing: False
2022-12-29 17:30: Argument weight_decay: 0.0
2022-12-29 17:30: Argument window_len: 10
2022-12-29 17:30: Train Epoch 1: 3/244 Loss: 0.410255
2022-12-29 17:30: Train Epoch 1: 7/244 Loss: 5.113992
2022-12-29 17:30: Train Epoch 1: 11/244 Loss: 1.868857
2022-12-29 17:30: Train Epoch 1: 15/244 Loss: 1.196587
2022-12-29 17:30: Train Epoch 1: 19/244 Loss: 1.769992
2022-12-29 17:30: Train Epoch 1: 23/244 Loss: 1.505098
2022-12-29 17:30: Train Epoch 1: 27/244 Loss: 0.777735
2022-12-29 17:30: Train Epoch 1: 31/244 Loss: 0.865804
2022-12-29 17:30: Train Epoch 1: 35/244 Loss: 0.783044
2022-12-29 17:31: Train Epoch 1: 39/244 Loss: 0.548191
2022-12-29 17:31: Train Epoch 1: 43/244 Loss: 0.320932
2022-12-29 17:31: Train Epoch 1: 47/244 Loss: 0.584930
2022-12-29 17:31: Train Epoch 1: 51/244 Loss: 0.851925
2022-12-29 17:31: Train Epoch 1: 55/244 Loss: 0.792793
2022-12-29 17:31: Train Epoch 1: 59/244 Loss: 0.555321
2022-12-29 17:31: Train Epoch 1: 63/244 Loss: 0.330049
2022-12-29 17:31: Train Epoch 1: 67/244 Loss: 0.366431
2022-12-29 17:31: Train Epoch 1: 71/244 Loss: 0.430407
2022-12-29 17:31: Train Epoch 1: 75/244 Loss: 0.358076
2022-12-29 17:31: Train Epoch 1: 79/244 Loss: 0.407711
2022-12-29 17:31: Train Epoch 1: 83/244 Loss: 0.383858
2022-12-29 17:32: Train Epoch 1: 87/244 Loss: 0.303970
2022-12-29 17:32: Train Epoch 1: 91/244 Loss: 0.252194
2022-12-29 17:32: Train Epoch 1: 95/244 Loss: 0.275941
2022-12-29 17:32: Train Epoch 1: 99/244 Loss: 0.369301
2022-12-29 17:32: Train Epoch 1: 103/244 Loss: 0.299265
2022-12-29 17:32: Train Epoch 1: 107/244 Loss: 0.237694
2022-12-29 17:32: Train Epoch 1: 111/244 Loss: 0.298755
2022-12-29 17:32: Train Epoch 1: 115/244 Loss: 0.282150
2022-12-29 17:32: Train Epoch 1: 119/244 Loss: 0.303486
2022-12-29 17:32: Train Epoch 1: 123/244 Loss: 0.259245
2022-12-29 17:32: Train Epoch 1: 127/244 Loss: 0.269139
2022-12-29 17:32: Train Epoch 1: 131/244 Loss: 0.230653
2022-12-29 17:33: Train Epoch 1: 135/244 Loss: 0.241219
2022-12-29 17:33: Train Epoch 1: 139/244 Loss: 0.247836
2022-12-29 17:33: Train Epoch 1: 143/244 Loss: 0.252882
2022-12-29 17:33: Train Epoch 1: 147/244 Loss: 0.242755
2022-12-29 17:33: Train Epoch 1: 151/244 Loss: 0.207270
2022-12-29 17:33: Train Epoch 1: 155/244 Loss: 0.228077
2022-12-29 17:33: Train Epoch 1: 159/244 Loss: 0.227153
2022-12-29 17:33: Train Epoch 1: 163/244 Loss: 0.238765
2022-12-29 17:33: Train Epoch 1: 167/244 Loss: 0.208524
2022-12-29 17:33: Train Epoch 1: 171/244 Loss: 0.243791
2022-12-29 17:33: Train Epoch 1: 175/244 Loss: 0.230046
2022-12-29 17:33: Train Epoch 1: 179/244 Loss: 0.213197
2022-12-29 17:33: Train Epoch 1: 183/244 Loss: 0.214615
2022-12-29 17:34: Train Epoch 1: 187/244 Loss: 0.206752
2022-12-29 17:34: Train Epoch 1: 191/244 Loss: 0.208527
2022-12-29 17:34: Train Epoch 1: 195/244 Loss: 0.221773
2022-12-29 17:34: Train Epoch 1: 199/244 Loss: 0.189664
2022-12-29 17:34: Train Epoch 1: 203/244 Loss: 0.206803
2022-12-29 17:34: Train Epoch 1: 207/244 Loss: 0.200944
2022-12-29 17:34: Train Epoch 1: 211/244 Loss: 0.200937
2022-12-29 17:34: Train Epoch 1: 215/244 Loss: 0.211850
2022-12-29 17:34: Train Epoch 1: 219/244 Loss: 0.188385
2022-12-29 17:34: Train Epoch 1: 223/244 Loss: 0.216096
2022-12-29 17:34: Train Epoch 1: 227/244 Loss: 0.214426
2022-12-29 17:34: Train Epoch 1: 231/244 Loss: 0.210001
2022-12-29 17:34: Train Epoch 1: 235/244 Loss: 0.180693
2022-12-29 17:34: Train Epoch 1: 239/244 Loss: 0.201241
2022-12-29 17:35: Train Epoch 1: 243/244 Loss: 0.180605
2022-12-29 17:35: **********Train Epoch 1: averaged Loss: 0.485879 
2022-12-29 17:35: 
Epoch time elapsed: 289.1288230419159

2022-12-29 17:35: 
 metrics validation: {'precision': 0.7144396551724138, 'recall': 0.51, 'f1-score': 0.5951526032315978, 'support': 1300, 'AUC': 0.7804189349112426, 'AUCPR': 0.6726294757947213, 'TP': 663, 'FP': 265, 'TN': 2335, 'FN': 637} 

2022-12-29 17:35: **********Val Epoch 1: average Loss: 0.282306
2022-12-29 17:35: *********************************Current best model saved!
2022-12-29 17:35: 
 Testing metrics {'precision': 0.7932367149758454, 'recall': 0.6685667752442996, 'f1-score': 0.7255855059655324, 'support': 1228, 'AUC': 0.8491701503464228, 'AUCPR': 0.7659290542723824, 'TP': 821, 'FP': 214, 'TN': 2242, 'FN': 407} 

2022-12-29 17:35: Train Epoch 2: 3/244 Loss: 0.197616
2022-12-29 17:35: Train Epoch 2: 7/244 Loss: 0.211839
2022-12-29 17:36: Train Epoch 2: 11/244 Loss: 0.181358
2022-12-29 17:36: Train Epoch 2: 15/244 Loss: 0.218551
2022-12-29 17:36: Train Epoch 2: 19/244 Loss: 0.217517
2022-12-29 17:36: Train Epoch 2: 23/244 Loss: 0.208198
2022-12-29 17:36: Train Epoch 2: 27/244 Loss: 0.202990
2022-12-29 17:36: Train Epoch 2: 31/244 Loss: 0.182847
2022-12-29 17:36: Train Epoch 2: 35/244 Loss: 0.206935
2022-12-29 17:36: Train Epoch 2: 39/244 Loss: 0.183881
2022-12-29 17:36: Train Epoch 2: 43/244 Loss: 0.218123
2022-12-29 17:36: Train Epoch 2: 47/244 Loss: 0.273021
2022-12-29 17:36: Train Epoch 2: 51/244 Loss: 0.196313
2022-12-29 17:36: Train Epoch 2: 55/244 Loss: 0.209541
2022-12-29 17:37: Train Epoch 2: 59/244 Loss: 0.199016
2022-12-29 17:37: Train Epoch 2: 63/244 Loss: 0.186834
2022-12-29 17:37: Train Epoch 2: 67/244 Loss: 0.205529
2022-12-29 17:37: Train Epoch 2: 71/244 Loss: 0.213788
2022-12-29 17:37: Train Epoch 2: 75/244 Loss: 0.202181
2022-12-29 17:37: Train Epoch 2: 79/244 Loss: 0.185534
2022-12-29 17:37: Train Epoch 2: 83/244 Loss: 0.175833
2022-12-29 17:37: Train Epoch 2: 87/244 Loss: 0.190593
2022-12-29 17:37: Train Epoch 2: 91/244 Loss: 0.170392
2022-12-29 17:37: Train Epoch 2: 95/244 Loss: 0.202688
2022-12-29 17:37: Train Epoch 2: 99/244 Loss: 0.185782
2022-12-29 17:37: Train Epoch 2: 103/244 Loss: 0.198199
2022-12-29 17:38: Train Epoch 2: 107/244 Loss: 0.198437
2022-12-29 17:38: Train Epoch 2: 111/244 Loss: 0.244941
2022-12-29 17:38: Train Epoch 2: 115/244 Loss: 0.196410
2022-12-29 17:38: Train Epoch 2: 119/244 Loss: 0.215810
2022-12-29 17:38: Train Epoch 2: 123/244 Loss: 0.235292
2022-12-29 17:38: Train Epoch 2: 127/244 Loss: 0.201333
2022-12-29 17:38: Train Epoch 2: 131/244 Loss: 0.204650
2022-12-29 17:38: Train Epoch 2: 135/244 Loss: 0.209620
2022-12-29 17:38: Train Epoch 2: 139/244 Loss: 0.200398
2022-12-29 17:38: Train Epoch 2: 143/244 Loss: 0.218296
2022-12-29 17:38: Train Epoch 2: 147/244 Loss: 0.206392
2022-12-29 17:39: Train Epoch 2: 151/244 Loss: 0.185495
2022-12-29 17:39: Train Epoch 2: 155/244 Loss: 0.213105
2022-12-29 17:39: Train Epoch 2: 159/244 Loss: 0.232365
2022-12-29 17:39: Train Epoch 2: 163/244 Loss: 0.226919
2022-12-29 17:39: Train Epoch 2: 167/244 Loss: 0.252746
2022-12-29 17:39: Train Epoch 2: 171/244 Loss: 0.200927
2022-12-29 17:39: Train Epoch 2: 175/244 Loss: 0.221893
2022-12-29 17:39: Train Epoch 2: 179/244 Loss: 0.197793
2022-12-29 17:39: Train Epoch 2: 183/244 Loss: 0.167799
2022-12-29 17:39: Train Epoch 2: 187/244 Loss: 0.214917
2022-12-29 17:39: Train Epoch 2: 191/244 Loss: 0.205163
2022-12-29 17:39: Train Epoch 2: 195/244 Loss: 0.201069
2022-12-29 17:40: Train Epoch 2: 199/244 Loss: 0.217087
2022-12-29 17:40: Train Epoch 2: 203/244 Loss: 0.170459
2022-12-29 17:40: Train Epoch 2: 207/244 Loss: 0.184375
2022-12-29 17:40: Train Epoch 2: 211/244 Loss: 0.234877
2022-12-29 17:40: Train Epoch 2: 215/244 Loss: 0.189043
2022-12-29 17:40: Train Epoch 2: 219/244 Loss: 0.173015
2022-12-29 17:40: Train Epoch 2: 223/244 Loss: 0.203419
2022-12-29 17:40: Train Epoch 2: 227/244 Loss: 0.177242
2022-12-29 17:40: Train Epoch 2: 231/244 Loss: 0.183646
2022-12-29 17:40: Train Epoch 2: 235/244 Loss: 0.201744
2022-12-29 17:40: Train Epoch 2: 239/244 Loss: 0.210279
2022-12-29 17:40: Train Epoch 2: 243/244 Loss: 0.185243
2022-12-29 17:40: **********Train Epoch 2: averaged Loss: 0.203398 
2022-12-29 17:40: 
Epoch time elapsed: 306.9610879421234

2022-12-29 17:41: 
 metrics validation: {'precision': 0.7240356083086054, 'recall': 0.563076923076923, 'f1-score': 0.6334919948074426, 'support': 1300, 'AUC': 0.7924538461538462, 'AUCPR': 0.7091847206583601, 'TP': 732, 'FP': 279, 'TN': 2321, 'FN': 568} 

2022-12-29 17:41: **********Val Epoch 2: average Loss: 0.308916
2022-12-29 17:41: 
 Testing metrics {'precision': 0.7932367149758454, 'recall': 0.6685667752442996, 'f1-score': 0.7255855059655324, 'support': 1228, 'AUC': 0.8491701503464228, 'AUCPR': 0.7659290542723824, 'TP': 821, 'FP': 214, 'TN': 2242, 'FN': 407} 

2022-12-29 17:41: Train Epoch 3: 3/244 Loss: 0.191592
2022-12-29 17:41: Train Epoch 3: 7/244 Loss: 0.233365
2022-12-29 17:41: Train Epoch 3: 11/244 Loss: 0.208710
2022-12-29 17:41: Train Epoch 3: 15/244 Loss: 0.234708
2022-12-29 17:42: Train Epoch 3: 19/244 Loss: 0.184037
2022-12-29 17:42: Train Epoch 3: 23/244 Loss: 0.205915
2022-12-29 17:42: Train Epoch 3: 27/244 Loss: 0.219050
2022-12-29 17:42: Train Epoch 3: 31/244 Loss: 0.182086
2022-12-29 17:42: Train Epoch 3: 35/244 Loss: 0.192308
2022-12-29 17:42: Train Epoch 3: 39/244 Loss: 0.194504
2022-12-29 17:42: Train Epoch 3: 43/244 Loss: 0.181128
2022-12-29 17:42: Train Epoch 3: 47/244 Loss: 0.195566
2022-12-29 17:42: Train Epoch 3: 51/244 Loss: 0.201063
2022-12-29 17:42: Train Epoch 3: 55/244 Loss: 0.198636
2022-12-29 17:42: Train Epoch 3: 59/244 Loss: 0.197644
2022-12-29 17:42: Train Epoch 3: 63/244 Loss: 0.190494
2022-12-29 17:43: Train Epoch 3: 67/244 Loss: 0.163035
2022-12-29 17:43: Train Epoch 3: 71/244 Loss: 0.217716
2022-12-29 17:43: Train Epoch 3: 75/244 Loss: 0.200220
2022-12-29 17:43: Train Epoch 3: 79/244 Loss: 0.185159
2022-12-29 17:43: Train Epoch 3: 83/244 Loss: 0.195143
2022-12-29 17:43: Train Epoch 3: 87/244 Loss: 0.175300
2022-12-29 17:43: Train Epoch 3: 91/244 Loss: 0.204939
2022-12-29 17:43: Train Epoch 3: 95/244 Loss: 0.203829
2022-12-29 17:43: Train Epoch 3: 99/244 Loss: 0.191892
2022-12-29 17:43: Train Epoch 3: 103/244 Loss: 0.239971
2022-12-29 17:43: Train Epoch 3: 107/244 Loss: 0.206450
2022-12-29 17:44: Train Epoch 3: 111/244 Loss: 0.215735
2022-12-29 17:44: Train Epoch 3: 115/244 Loss: 0.178137
2022-12-29 17:44: Train Epoch 3: 119/244 Loss: 0.187058
2022-12-29 17:44: Train Epoch 3: 123/244 Loss: 0.180307
2022-12-29 17:44: Train Epoch 3: 127/244 Loss: 0.189305
2022-12-29 17:44: Train Epoch 3: 131/244 Loss: 0.204744
2022-12-29 17:44: Train Epoch 3: 135/244 Loss: 0.232756
2022-12-29 17:44: Train Epoch 3: 139/244 Loss: 0.189278
2022-12-29 17:44: Train Epoch 3: 143/244 Loss: 0.215488
2022-12-29 17:44: Train Epoch 3: 147/244 Loss: 0.208821
2022-12-29 17:44: Train Epoch 3: 151/244 Loss: 0.227990
2022-12-29 17:44: Train Epoch 3: 155/244 Loss: 0.171211
2022-12-29 17:45: Train Epoch 3: 159/244 Loss: 0.289277
2022-12-29 17:45: Train Epoch 3: 163/244 Loss: 0.208973
2022-12-29 17:45: Train Epoch 3: 167/244 Loss: 0.237888
2022-12-29 17:45: Train Epoch 3: 171/244 Loss: 0.273576
2022-12-29 17:45: Train Epoch 3: 175/244 Loss: 0.206308
2022-12-29 17:45: Train Epoch 3: 179/244 Loss: 0.236495
2022-12-29 17:45: Train Epoch 3: 183/244 Loss: 0.188557
2022-12-29 17:45: Train Epoch 3: 187/244 Loss: 0.271327
2022-12-29 17:45: Train Epoch 3: 191/244 Loss: 0.245957
2022-12-29 17:45: Train Epoch 3: 195/244 Loss: 0.216017
2022-12-29 17:45: Train Epoch 3: 199/244 Loss: 0.185928
2022-12-29 17:45: Train Epoch 3: 203/244 Loss: 0.182864
2022-12-29 17:46: Train Epoch 3: 207/244 Loss: 0.233045
2022-12-29 17:46: Train Epoch 3: 211/244 Loss: 0.170014
2022-12-29 17:46: Train Epoch 3: 215/244 Loss: 0.211908
2022-12-29 17:46: Train Epoch 3: 219/244 Loss: 0.183886
2022-12-29 17:46: Train Epoch 3: 223/244 Loss: 0.226127
2022-12-29 17:46: Train Epoch 3: 227/244 Loss: 0.169209
2022-12-29 17:46: Train Epoch 3: 231/244 Loss: 0.210156
2022-12-29 17:46: Train Epoch 3: 235/244 Loss: 0.206965
2022-12-29 17:46: Train Epoch 3: 239/244 Loss: 0.194108
2022-12-29 17:46: Train Epoch 3: 243/244 Loss: 0.155550
2022-12-29 17:46: **********Train Epoch 3: averaged Loss: 0.204909 
2022-12-29 17:46: 
Epoch time elapsed: 306.8670449256897

2022-12-29 17:47: 
 metrics validation: {'precision': 0.6701277955271565, 'recall': 0.6453846153846153, 'f1-score': 0.6575235109717869, 'support': 1300, 'AUC': 0.7907423076923078, 'AUCPR': 0.7060706485339441, 'TP': 839, 'FP': 413, 'TN': 2187, 'FN': 461} 

2022-12-29 17:47: **********Val Epoch 3: average Loss: 0.288236
2022-12-29 17:47: 
 Testing metrics {'precision': 0.7932367149758454, 'recall': 0.6685667752442996, 'f1-score': 0.7255855059655324, 'support': 1228, 'AUC': 0.8491701503464228, 'AUCPR': 0.7659290542723824, 'TP': 821, 'FP': 214, 'TN': 2242, 'FN': 407} 

2022-12-29 17:47: Train Epoch 4: 3/244 Loss: 0.208147
2022-12-29 17:47: Train Epoch 4: 7/244 Loss: 0.202397
2022-12-29 17:47: Train Epoch 4: 11/244 Loss: 0.183900
2022-12-29 17:47: Train Epoch 4: 15/244 Loss: 0.195694
2022-12-29 17:47: Train Epoch 4: 19/244 Loss: 0.203772
2022-12-29 17:47: Train Epoch 4: 23/244 Loss: 0.205913
2022-12-29 17:48: Train Epoch 4: 27/244 Loss: 0.188285
2022-12-29 17:48: Train Epoch 4: 31/244 Loss: 0.244731
