2022-12-31 16:47: log dir: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022123116475666372054013
2022-12-31 16:47: Experiment log path in: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022123116475666372054013
2022-12-31 16:47: Argument: Namespace(batch_size=256, clc='vec', cuda=True, dataset='2020', dataset_root='/home/joel.chacon/tmp/datasets_grl/', debug=False, default_graph=True, device='cpu', dynamic_features=['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh'], early_stop=True, early_stop_patience=8, embed_dim=128, epochs=30, grad_norm=False, horizon=1, input_dim=25, lag=10, link_len=2, log_dir='/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022123116475666372054013', log_step=1, loss_func='nllloss', lr_decay=True, lr_decay_rate=0.1, lr_decay_step='15, 20', lr_init=0.0001, max_grad_norm=5, minbatch_size=64, mode='train', model='fire_GCN', nan_fill=-1.0, num_layers=1, num_nodes=625, num_workers=12, output_dim=2, patch_height=25, patch_width=25, persistent_workers=True, pin_memory=True, plot=False, positive_weight=0.5, prefetch_factor=2, real_value=True, rnn_units=32, seed=10000, static_features=['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density'], teacher_forcing=False, weight_decay=0.0, window_len=10)
2022-12-31 16:47: Argument batch_size: 256
2022-12-31 16:47: Argument clc: 'vec'
2022-12-31 16:47: Argument cuda: True
2022-12-31 16:47: Argument dataset: '2020'
2022-12-31 16:47: Argument dataset_root: '/home/joel.chacon/tmp/datasets_grl/'
2022-12-31 16:47: Argument debug: False
2022-12-31 16:47: Argument default_graph: True
2022-12-31 16:47: Argument device: 'cpu'
2022-12-31 16:47: Argument dynamic_features: ['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh']
2022-12-31 16:47: Argument early_stop: True
2022-12-31 16:47: Argument early_stop_patience: 8
2022-12-31 16:47: Argument embed_dim: 128
2022-12-31 16:47: Argument epochs: 30
2022-12-31 16:47: Argument grad_norm: False
2022-12-31 16:47: Argument horizon: 1
2022-12-31 16:47: Argument input_dim: 25
2022-12-31 16:47: Argument lag: 10
2022-12-31 16:47: Argument link_len: 2
2022-12-31 16:47: Argument log_dir: '/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022123116475666372054013'
2022-12-31 16:47: Argument log_step: 1
2022-12-31 16:47: Argument loss_func: 'nllloss'
2022-12-31 16:47: Argument lr_decay: True
2022-12-31 16:47: Argument lr_decay_rate: 0.1
2022-12-31 16:47: Argument lr_decay_step: '15, 20'
2022-12-31 16:47: Argument lr_init: 0.0001
2022-12-31 16:47: Argument max_grad_norm: 5
2022-12-31 16:47: Argument minbatch_size: 64
2022-12-31 16:47: Argument mode: 'train'
2022-12-31 16:47: Argument model: 'fire_GCN'
2022-12-31 16:47: Argument nan_fill: -1.0
2022-12-31 16:47: Argument num_layers: 1
2022-12-31 16:47: Argument num_nodes: 625
2022-12-31 16:47: Argument num_workers: 12
2022-12-31 16:47: Argument output_dim: 2
2022-12-31 16:47: Argument patch_height: 25
2022-12-31 16:47: Argument patch_width: 25
2022-12-31 16:47: Argument persistent_workers: True
2022-12-31 16:47: Argument pin_memory: True
2022-12-31 16:47: Argument plot: False
2022-12-31 16:47: Argument positive_weight: 0.5
2022-12-31 16:47: Argument prefetch_factor: 2
2022-12-31 16:47: Argument real_value: True
2022-12-31 16:47: Argument rnn_units: 32
2022-12-31 16:47: Argument seed: 10000
2022-12-31 16:47: Argument static_features: ['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density']
2022-12-31 16:47: Argument teacher_forcing: False
2022-12-31 16:47: Argument weight_decay: 0.0
2022-12-31 16:47: Argument window_len: 10
2022-12-31 16:48: Train Epoch 1: 3/634 Loss: 0.530994
2022-12-31 16:48: Train Epoch 1: 7/634 Loss: 0.824885
2022-12-31 16:48: Train Epoch 1: 11/634 Loss: 0.562329
2022-12-31 16:49: Train Epoch 1: 15/634 Loss: 0.612547
2022-12-31 16:49: Train Epoch 1: 19/634 Loss: 0.419495
2022-12-31 16:49: Train Epoch 1: 23/634 Loss: 0.275884
2022-12-31 16:49: Train Epoch 1: 27/634 Loss: 0.291413
2022-12-31 16:50: Train Epoch 1: 31/634 Loss: 0.384949
2022-12-31 16:50: Train Epoch 1: 35/634 Loss: 0.378122
2022-12-31 16:50: Train Epoch 1: 39/634 Loss: 0.394988
2022-12-31 16:51: Train Epoch 1: 43/634 Loss: 0.288581
2022-12-31 16:51: Train Epoch 1: 47/634 Loss: 0.266335
2022-12-31 16:51: Train Epoch 1: 51/634 Loss: 0.275725
2022-12-31 16:51: Train Epoch 1: 55/634 Loss: 0.241929
2022-12-31 16:52: Train Epoch 1: 59/634 Loss: 0.271477
2022-12-31 16:52: Train Epoch 1: 63/634 Loss: 0.349029
2022-12-31 16:52: Train Epoch 1: 67/634 Loss: 0.281434
2022-12-31 16:52: Train Epoch 1: 71/634 Loss: 0.267481
2022-12-31 16:53: Train Epoch 1: 75/634 Loss: 0.250486
2022-12-31 16:53: Train Epoch 1: 79/634 Loss: 0.230668
2022-12-31 16:53: Train Epoch 1: 83/634 Loss: 0.247030
2022-12-31 16:54: Train Epoch 1: 87/634 Loss: 0.268270
2022-12-31 16:54: Train Epoch 1: 91/634 Loss: 0.266600
2022-12-31 16:54: Train Epoch 1: 95/634 Loss: 0.286725
2022-12-31 16:54: Train Epoch 1: 99/634 Loss: 0.227674
2022-12-31 16:55: Train Epoch 1: 103/634 Loss: 0.259565
2022-12-31 16:55: Train Epoch 1: 107/634 Loss: 0.219937
2022-12-31 16:55: Train Epoch 1: 111/634 Loss: 0.234346
2022-12-31 16:55: Train Epoch 1: 115/634 Loss: 0.192062
2022-12-31 16:56: Train Epoch 1: 119/634 Loss: 0.230404
2022-12-31 16:56: Train Epoch 1: 123/634 Loss: 0.254487
2022-12-31 16:56: Train Epoch 1: 127/634 Loss: 0.213532
2022-12-31 16:57: Train Epoch 1: 131/634 Loss: 0.225646
2022-12-31 16:57: Train Epoch 1: 135/634 Loss: 0.210722
2022-12-31 16:57: Train Epoch 1: 139/634 Loss: 0.220480
2022-12-31 16:57: Train Epoch 1: 143/634 Loss: 0.207955
2022-12-31 16:58: Train Epoch 1: 147/634 Loss: 0.229355
2022-12-31 16:58: Train Epoch 1: 151/634 Loss: 0.213936
2022-12-31 16:58: Train Epoch 1: 155/634 Loss: 0.210451
2022-12-31 16:58: Train Epoch 1: 159/634 Loss: 0.228663
2022-12-31 16:59: Train Epoch 1: 163/634 Loss: 0.264788
2022-12-31 16:59: Train Epoch 1: 167/634 Loss: 0.230798
2022-12-31 16:59: Train Epoch 1: 171/634 Loss: 0.209955
2022-12-31 17:00: Train Epoch 1: 175/634 Loss: 0.235144
2022-12-31 17:00: Train Epoch 1: 179/634 Loss: 0.218491
2022-12-31 17:00: Train Epoch 1: 183/634 Loss: 0.206996
2022-12-31 17:00: Train Epoch 1: 187/634 Loss: 0.248737
2022-12-31 17:01: Train Epoch 1: 191/634 Loss: 0.216119
2022-12-31 17:01: Train Epoch 1: 195/634 Loss: 0.237329
2022-12-31 17:01: Train Epoch 1: 199/634 Loss: 0.176617
2022-12-31 17:01: Train Epoch 1: 203/634 Loss: 0.200360
2022-12-31 17:02: Train Epoch 1: 207/634 Loss: 0.204066
2022-12-31 17:02: Train Epoch 1: 211/634 Loss: 0.209639
2022-12-31 17:02: Train Epoch 1: 215/634 Loss: 0.226614
2022-12-31 17:02: Train Epoch 1: 219/634 Loss: 0.200569
2022-12-31 17:03: Train Epoch 1: 223/634 Loss: 0.207463
2022-12-31 17:03: Train Epoch 1: 227/634 Loss: 0.218025
2022-12-31 17:03: Train Epoch 1: 231/634 Loss: 0.249008
2022-12-31 17:03: Train Epoch 1: 235/634 Loss: 0.193255
2022-12-31 17:04: Train Epoch 1: 239/634 Loss: 0.212083
2022-12-31 17:04: Train Epoch 1: 243/634 Loss: 0.195507
2022-12-31 17:04: Train Epoch 1: 247/634 Loss: 0.186806
2022-12-31 17:04: Train Epoch 1: 251/634 Loss: 0.203659
2022-12-31 17:05: Train Epoch 1: 255/634 Loss: 0.204542
2022-12-31 17:05: Train Epoch 1: 259/634 Loss: 0.221358
2022-12-31 17:05: Train Epoch 1: 263/634 Loss: 0.215053
2022-12-31 17:06: Train Epoch 1: 267/634 Loss: 0.217843
2022-12-31 17:06: Train Epoch 1: 271/634 Loss: 0.216269
2022-12-31 17:06: Train Epoch 1: 275/634 Loss: 0.198729
2022-12-31 17:06: Train Epoch 1: 279/634 Loss: 0.186128
2022-12-31 17:07: Train Epoch 1: 283/634 Loss: 0.199747
2022-12-31 17:07: Train Epoch 1: 287/634 Loss: 0.228357
2022-12-31 17:07: Train Epoch 1: 291/634 Loss: 0.189241
2022-12-31 17:07: Train Epoch 1: 295/634 Loss: 0.207428
2022-12-31 17:08: Train Epoch 1: 299/634 Loss: 0.205823
2022-12-31 17:08: Train Epoch 1: 303/634 Loss: 0.219987
2022-12-31 17:08: Train Epoch 1: 307/634 Loss: 0.199621
2022-12-31 17:09: Train Epoch 1: 311/634 Loss: 0.195676
2022-12-31 17:09: Train Epoch 1: 315/634 Loss: 0.205263
2022-12-31 17:09: Train Epoch 1: 319/634 Loss: 0.189870
2022-12-31 17:09: Train Epoch 1: 323/634 Loss: 0.218230
2022-12-31 17:10: Train Epoch 1: 327/634 Loss: 0.210600
2022-12-31 17:10: Train Epoch 1: 331/634 Loss: 0.222973
2022-12-31 17:10: Train Epoch 1: 335/634 Loss: 0.186065
2022-12-31 17:10: Train Epoch 1: 339/634 Loss: 0.179502
2022-12-31 17:11: Train Epoch 1: 343/634 Loss: 0.202835
2022-12-31 17:11: Train Epoch 1: 347/634 Loss: 0.204374
2022-12-31 17:11: Train Epoch 1: 351/634 Loss: 0.226312
2022-12-31 17:11: Train Epoch 1: 355/634 Loss: 0.192416
2022-12-31 17:12: Train Epoch 1: 359/634 Loss: 0.186835
2022-12-31 17:12: Train Epoch 1: 363/634 Loss: 0.206518
2022-12-31 17:12: Train Epoch 1: 367/634 Loss: 0.224226
2022-12-31 17:12: Train Epoch 1: 371/634 Loss: 0.230767
2022-12-31 17:13: Train Epoch 1: 375/634 Loss: 0.209026
2022-12-31 17:13: Train Epoch 1: 379/634 Loss: 0.179244
2022-12-31 17:13: Train Epoch 1: 383/634 Loss: 0.192734
2022-12-31 17:14: Train Epoch 1: 387/634 Loss: 0.184795
2022-12-31 17:14: Train Epoch 1: 391/634 Loss: 0.207877
2022-12-31 17:14: Train Epoch 1: 395/634 Loss: 0.193985
2022-12-31 17:14: Train Epoch 1: 399/634 Loss: 0.192754
2022-12-31 17:15: Train Epoch 1: 403/634 Loss: 0.200528
2022-12-31 17:15: Train Epoch 1: 407/634 Loss: 0.193597
2022-12-31 17:15: Train Epoch 1: 411/634 Loss: 0.181140
2022-12-31 17:15: Train Epoch 1: 415/634 Loss: 0.216480
2022-12-31 17:16: Train Epoch 1: 419/634 Loss: 0.237735
2022-12-31 17:16: Train Epoch 1: 423/634 Loss: 0.166489
2022-12-31 17:16: Train Epoch 1: 427/634 Loss: 0.212014
2022-12-31 17:16: Train Epoch 1: 431/634 Loss: 0.210680
2022-12-31 17:17: Train Epoch 1: 435/634 Loss: 0.177543
2022-12-31 17:17: Train Epoch 1: 439/634 Loss: 0.178405
2022-12-31 17:17: Train Epoch 1: 443/634 Loss: 0.211957
2022-12-31 17:17: Train Epoch 1: 447/634 Loss: 0.184667
2022-12-31 17:18: Train Epoch 1: 451/634 Loss: 0.185688
2022-12-31 17:18: Train Epoch 1: 455/634 Loss: 0.185787
2022-12-31 17:18: Train Epoch 1: 459/634 Loss: 0.235439
2022-12-31 17:19: Train Epoch 1: 463/634 Loss: 0.209233
2022-12-31 17:19: Train Epoch 1: 467/634 Loss: 0.188205
2022-12-31 17:19: Train Epoch 1: 471/634 Loss: 0.197706
2022-12-31 17:19: Train Epoch 1: 475/634 Loss: 0.219800
2022-12-31 17:20: Train Epoch 1: 479/634 Loss: 0.243357
2022-12-31 17:20: Train Epoch 1: 483/634 Loss: 0.175938
2022-12-31 17:20: Train Epoch 1: 487/634 Loss: 0.207785
2022-12-31 17:20: Train Epoch 1: 491/634 Loss: 0.230191
2022-12-31 17:21: Train Epoch 1: 495/634 Loss: 0.211432
2022-12-31 17:21: Train Epoch 1: 499/634 Loss: 0.225327
2022-12-31 17:21: Train Epoch 1: 503/634 Loss: 0.195367
2022-12-31 17:22: Train Epoch 1: 507/634 Loss: 0.206506
2022-12-31 17:22: Train Epoch 1: 511/634 Loss: 0.238561
2022-12-31 17:22: Train Epoch 1: 515/634 Loss: 0.212199
2022-12-31 17:22: Train Epoch 1: 519/634 Loss: 0.201022
2022-12-31 17:23: Train Epoch 1: 523/634 Loss: 0.187974
2022-12-31 17:23: Train Epoch 1: 527/634 Loss: 0.205575
2022-12-31 17:23: Train Epoch 1: 531/634 Loss: 0.178172
2022-12-31 17:23: Train Epoch 1: 535/634 Loss: 0.224001
2022-12-31 17:24: Train Epoch 1: 539/634 Loss: 0.199914
2022-12-31 17:24: Train Epoch 1: 543/634 Loss: 0.184522
2022-12-31 17:24: Train Epoch 1: 547/634 Loss: 0.187841
2022-12-31 17:24: Train Epoch 1: 551/634 Loss: 0.220568
2022-12-31 17:25: Train Epoch 1: 555/634 Loss: 0.203118
2022-12-31 17:25: Train Epoch 1: 559/634 Loss: 0.198675
2022-12-31 17:25: Train Epoch 1: 563/634 Loss: 0.234395
2022-12-31 17:26: Train Epoch 1: 567/634 Loss: 0.237302
2022-12-31 17:26: Train Epoch 1: 571/634 Loss: 0.198618
2022-12-31 17:26: Train Epoch 1: 575/634 Loss: 0.228986
2022-12-31 17:26: Train Epoch 1: 579/634 Loss: 0.194688
2022-12-31 17:27: Train Epoch 1: 583/634 Loss: 0.222340
2022-12-31 17:27: Train Epoch 1: 587/634 Loss: 0.197419
