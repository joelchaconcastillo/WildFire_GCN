2022-12-28 22:58: log dir: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122822581582665569118
2022-12-28 22:58: Experiment log path in: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122822581582665569118
2022-12-28 22:58: Argument: Namespace(batch_size=256, clc='vec', cuda=True, dataset='2020', dataset_root='/home/joel.chacon/tmp/datasets_grl/', debug=False, default_graph=True, device='cpu', dynamic_features=['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh'], early_stop=True, early_stop_patience=8, embed_dim=128, epochs=30, grad_norm=False, horizon=1, input_dim=25, lag=10, link_len=2, log_dir='/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122822581582665569118', log_step=1, loss_func='nllloss', lr_decay=True, lr_decay_rate=0.1, lr_decay_step='15, 20', lr_init=0.0005, max_grad_norm=5, minbatch_size=64, mode='train', model='fire_GCN', nan_fill=0.5, num_layers=1, num_nodes=625, num_workers=20, output_dim=2, patch_height=25, patch_width=25, persistent_workers=True, pin_memory=True, plot=False, positive_weight=0.5, prefetch_factor=2, real_value=True, rnn_units=16, seed=10000, static_features=['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density'], teacher_forcing=False, weight_decay=0.0, window_len=10)
2022-12-28 22:58: Argument batch_size: 256
2022-12-28 22:58: Argument clc: 'vec'
2022-12-28 22:58: Argument cuda: True
2022-12-28 22:58: Argument dataset: '2020'
2022-12-28 22:58: Argument dataset_root: '/home/joel.chacon/tmp/datasets_grl/'
2022-12-28 22:58: Argument debug: False
2022-12-28 22:58: Argument default_graph: True
2022-12-28 22:58: Argument device: 'cpu'
2022-12-28 22:58: Argument dynamic_features: ['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh']
2022-12-28 22:58: Argument early_stop: True
2022-12-28 22:58: Argument early_stop_patience: 8
2022-12-28 22:58: Argument embed_dim: 128
2022-12-28 22:58: Argument epochs: 30
2022-12-28 22:58: Argument grad_norm: False
2022-12-28 22:58: Argument horizon: 1
2022-12-28 22:58: Argument input_dim: 25
2022-12-28 22:58: Argument lag: 10
2022-12-28 22:58: Argument link_len: 2
2022-12-28 22:58: Argument log_dir: '/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122822581582665569118'
2022-12-28 22:58: Argument log_step: 1
2022-12-28 22:58: Argument loss_func: 'nllloss'
2022-12-28 22:58: Argument lr_decay: True
2022-12-28 22:58: Argument lr_decay_rate: 0.1
2022-12-28 22:58: Argument lr_decay_step: '15, 20'
2022-12-28 22:58: Argument lr_init: 0.0005
2022-12-28 22:58: Argument max_grad_norm: 5
2022-12-28 22:58: Argument minbatch_size: 64
2022-12-28 22:58: Argument mode: 'train'
2022-12-28 22:58: Argument model: 'fire_GCN'
2022-12-28 22:58: Argument nan_fill: 0.5
2022-12-28 22:58: Argument num_layers: 1
2022-12-28 22:58: Argument num_nodes: 625
2022-12-28 22:58: Argument num_workers: 20
2022-12-28 22:58: Argument output_dim: 2
2022-12-28 22:58: Argument patch_height: 25
2022-12-28 22:58: Argument patch_width: 25
2022-12-28 22:58: Argument persistent_workers: True
2022-12-28 22:58: Argument pin_memory: True
2022-12-28 22:58: Argument plot: False
2022-12-28 22:58: Argument positive_weight: 0.5
2022-12-28 22:58: Argument prefetch_factor: 2
2022-12-28 22:58: Argument real_value: True
2022-12-28 22:58: Argument rnn_units: 16
2022-12-28 22:58: Argument seed: 10000
2022-12-28 22:58: Argument static_features: ['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density']
2022-12-28 22:58: Argument teacher_forcing: False
2022-12-28 22:58: Argument weight_decay: 0.0
2022-12-28 22:58: Argument window_len: 10
2022-12-28 22:58: Train Epoch 1: 3/244 Loss: 2.240158
2022-12-28 22:58: Train Epoch 1: 7/244 Loss: 3.109055
2022-12-28 22:58: Train Epoch 1: 11/244 Loss: 1.782924
2022-12-28 22:58: Train Epoch 1: 15/244 Loss: 1.382248
2022-12-28 22:58: Train Epoch 1: 19/244 Loss: 0.565962
2022-12-28 22:58: Train Epoch 1: 23/244 Loss: 1.008825
2022-12-28 22:58: Train Epoch 1: 27/244 Loss: 0.663991
2022-12-28 22:58: Train Epoch 1: 31/244 Loss: 0.562860
2022-12-28 22:59: Train Epoch 1: 35/244 Loss: 0.729470
2022-12-28 22:59: Train Epoch 1: 39/244 Loss: 0.708371
2022-12-28 22:59: Train Epoch 1: 43/244 Loss: 0.575660
2022-12-28 22:59: Train Epoch 1: 47/244 Loss: 0.462768
2022-12-28 22:59: Train Epoch 1: 51/244 Loss: 0.582528
2022-12-28 22:59: Train Epoch 1: 55/244 Loss: 0.558431
2022-12-28 22:59: Train Epoch 1: 59/244 Loss: 0.448658
2022-12-28 22:59: Train Epoch 1: 63/244 Loss: 0.408568
2022-12-28 22:59: Train Epoch 1: 67/244 Loss: 0.555866
2022-12-28 22:59: Train Epoch 1: 71/244 Loss: 0.578117
2022-12-28 22:59: Train Epoch 1: 75/244 Loss: 0.502367
2022-12-28 22:59: Train Epoch 1: 79/244 Loss: 0.377172
2022-12-28 22:59: Train Epoch 1: 83/244 Loss: 0.501761
2022-12-28 22:59: Train Epoch 1: 87/244 Loss: 0.515915
2022-12-28 23:00: Train Epoch 1: 91/244 Loss: 0.473950
2022-12-28 23:00: Train Epoch 1: 95/244 Loss: 0.337866
2022-12-28 23:00: Train Epoch 1: 99/244 Loss: 0.426955
2022-12-28 23:00: Train Epoch 1: 103/244 Loss: 0.492898
2022-12-28 23:00: Train Epoch 1: 107/244 Loss: 0.423541
2022-12-28 23:00: Train Epoch 1: 111/244 Loss: 0.354899
2022-12-28 23:00: Train Epoch 1: 115/244 Loss: 0.366838
2022-12-28 23:00: Train Epoch 1: 119/244 Loss: 0.425871
2022-12-28 23:00: Train Epoch 1: 123/244 Loss: 0.417350
2022-12-28 23:00: Train Epoch 1: 127/244 Loss: 0.346013
2022-12-28 23:00: Train Epoch 1: 131/244 Loss: 0.358721
2022-12-28 23:00: Train Epoch 1: 135/244 Loss: 0.426522
2022-12-28 23:00: Train Epoch 1: 139/244 Loss: 0.318399
2022-12-28 23:01: Train Epoch 1: 143/244 Loss: 0.356530
2022-12-28 23:01: Train Epoch 1: 147/244 Loss: 0.329879
2022-12-28 23:01: Train Epoch 1: 151/244 Loss: 0.332998
2022-12-28 23:01: Train Epoch 1: 155/244 Loss: 0.352223
2022-12-28 23:01: Train Epoch 1: 159/244 Loss: 0.284179
2022-12-28 23:01: Train Epoch 1: 163/244 Loss: 0.318824
2022-12-28 23:01: Train Epoch 1: 167/244 Loss: 0.297083
2022-12-28 23:01: Train Epoch 1: 171/244 Loss: 0.294395
2022-12-28 23:01: Train Epoch 1: 175/244 Loss: 0.281713
2022-12-28 23:01: Train Epoch 1: 179/244 Loss: 0.284983
2022-12-28 23:01: Train Epoch 1: 183/244 Loss: 0.301097
2022-12-28 23:01: Train Epoch 1: 187/244 Loss: 0.307967
2022-12-28 23:01: Train Epoch 1: 191/244 Loss: 0.250342
2022-12-28 23:01: Train Epoch 1: 195/244 Loss: 0.270011
2022-12-28 23:02: Train Epoch 1: 199/244 Loss: 0.277384
2022-12-28 23:02: Train Epoch 1: 203/244 Loss: 0.229932
2022-12-28 23:02: Train Epoch 1: 207/244 Loss: 0.237441
2022-12-28 23:02: Train Epoch 1: 211/244 Loss: 0.272713
2022-12-28 23:02: Train Epoch 1: 215/244 Loss: 0.251823
2022-12-28 23:02: Train Epoch 1: 219/244 Loss: 0.226909
2022-12-28 23:02: Train Epoch 1: 223/244 Loss: 0.264585
2022-12-28 23:02: Train Epoch 1: 227/244 Loss: 0.250864
2022-12-28 23:02: Train Epoch 1: 231/244 Loss: 0.268400
2022-12-28 23:02: Train Epoch 1: 235/244 Loss: 0.242721
2022-12-28 23:02: Train Epoch 1: 239/244 Loss: 0.235413
2022-12-28 23:02: Train Epoch 1: 243/244 Loss: 0.257001
2022-12-28 23:02: **********Train Epoch 1: averaged Loss: 0.512605 
2022-12-28 23:02: 
Epoch time elapsed: 269.54502153396606

2022-12-28 23:03: 
 metrics validation: {'precision': 0.7557692307692307, 'recall': 0.6046153846153847, 'f1-score': 0.6717948717948717, 'support': 1300, 'AUC': 0.8113852071005917, 'AUCPR': 0.7356068197049024, 'TP': 786, 'FP': 254, 'TN': 2346, 'FN': 514} 

2022-12-28 23:03: **********Val Epoch 1: average Loss: 0.541543
2022-12-28 23:03: *********************************Current best model saved!
2022-12-28 23:03: 
 Testing metrics {'precision': 0.7862433862433862, 'recall': 0.6050488599348535, 'f1-score': 0.6838472158306489, 'support': 1228, 'AUC': 0.8556864661694024, 'AUCPR': 0.7843885424748428, 'TP': 743, 'FP': 202, 'TN': 2254, 'FN': 485} 

2022-12-28 23:03: Train Epoch 2: 3/244 Loss: 0.171815
2022-12-28 23:03: Train Epoch 2: 7/244 Loss: 0.230928
2022-12-28 23:04: Train Epoch 2: 11/244 Loss: 0.240915
2022-12-28 23:04: Train Epoch 2: 15/244 Loss: 0.265442
2022-12-28 23:04: Train Epoch 2: 19/244 Loss: 0.233504
2022-12-28 23:04: Train Epoch 2: 23/244 Loss: 0.203515
2022-12-28 23:04: Train Epoch 2: 27/244 Loss: 0.183213
2022-12-28 23:04: Train Epoch 2: 31/244 Loss: 0.238003
2022-12-28 23:04: Train Epoch 2: 35/244 Loss: 0.194905
2022-12-28 23:04: Train Epoch 2: 39/244 Loss: 0.218177
2022-12-28 23:04: Train Epoch 2: 43/244 Loss: 0.243666
2022-12-28 23:04: Train Epoch 2: 47/244 Loss: 0.223269
2022-12-28 23:04: Train Epoch 2: 51/244 Loss: 0.243607
2022-12-28 23:04: Train Epoch 2: 55/244 Loss: 0.205554
2022-12-28 23:04: Train Epoch 2: 59/244 Loss: 0.177375
2022-12-28 23:04: Train Epoch 2: 63/244 Loss: 0.265388
2022-12-28 23:05: Train Epoch 2: 67/244 Loss: 0.264395
2022-12-28 23:05: Train Epoch 2: 71/244 Loss: 0.203913
2022-12-28 23:05: Train Epoch 2: 75/244 Loss: 0.227508
2022-12-28 23:05: Train Epoch 2: 79/244 Loss: 0.312368
2022-12-28 23:05: Train Epoch 2: 83/244 Loss: 0.230604
2022-12-28 23:05: Train Epoch 2: 87/244 Loss: 0.252730
2022-12-28 23:05: Train Epoch 2: 91/244 Loss: 0.215732
2022-12-28 23:05: Train Epoch 2: 95/244 Loss: 0.277263
2022-12-28 23:05: Train Epoch 2: 99/244 Loss: 0.216573
2022-12-28 23:05: Train Epoch 2: 103/244 Loss: 0.235048
2022-12-28 23:05: Train Epoch 2: 107/244 Loss: 0.178130
2022-12-28 23:05: Train Epoch 2: 111/244 Loss: 0.197465
2022-12-28 23:05: Train Epoch 2: 115/244 Loss: 0.233074
2022-12-28 23:05: Train Epoch 2: 119/244 Loss: 0.222046
2022-12-28 23:06: Train Epoch 2: 123/244 Loss: 0.267100
2022-12-28 23:06: Train Epoch 2: 127/244 Loss: 0.217255
2022-12-28 23:06: Train Epoch 2: 131/244 Loss: 0.225586
2022-12-28 23:06: Train Epoch 2: 135/244 Loss: 0.211793
2022-12-28 23:06: Train Epoch 2: 139/244 Loss: 0.209277
2022-12-28 23:06: Train Epoch 2: 143/244 Loss: 0.217399
2022-12-28 23:06: Train Epoch 2: 147/244 Loss: 0.213476
2022-12-28 23:06: Train Epoch 2: 151/244 Loss: 0.280318
2022-12-28 23:06: Train Epoch 2: 155/244 Loss: 0.216995
2022-12-28 23:06: Train Epoch 2: 159/244 Loss: 0.287488
2022-12-28 23:06: Train Epoch 2: 163/244 Loss: 0.303060
2022-12-28 23:06: Train Epoch 2: 167/244 Loss: 0.287966
2022-12-28 23:06: Train Epoch 2: 171/244 Loss: 0.181519
2022-12-28 23:07: Train Epoch 2: 175/244 Loss: 0.282630
2022-12-28 23:07: Train Epoch 2: 179/244 Loss: 0.234983
2022-12-28 23:07: Train Epoch 2: 183/244 Loss: 0.212737
2022-12-28 23:07: Train Epoch 2: 187/244 Loss: 0.289603
2022-12-28 23:07: Train Epoch 2: 191/244 Loss: 0.300971
2022-12-28 23:07: Train Epoch 2: 195/244 Loss: 0.257100
2022-12-28 23:07: Train Epoch 2: 199/244 Loss: 0.242977
2022-12-28 23:07: Train Epoch 2: 203/244 Loss: 0.310099
2022-12-28 23:07: Train Epoch 2: 207/244 Loss: 0.304830
2022-12-28 23:07: Train Epoch 2: 211/244 Loss: 0.369275
2022-12-28 23:07: Train Epoch 2: 215/244 Loss: 0.197739
2022-12-28 23:07: Train Epoch 2: 219/244 Loss: 0.262325
