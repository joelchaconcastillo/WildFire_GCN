2022-12-29 15:06: log dir: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122915061272358569118
2022-12-29 15:06: Experiment log path in: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122915061272358569118
2022-12-29 15:06: Argument: Namespace(batch_size=256, clc='vec', cuda=True, dataset='2020', dataset_root='/home/joel.chacon/tmp/datasets_grl/', debug=False, default_graph=True, device='cpu', dynamic_features=['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh'], early_stop=True, early_stop_patience=8, embed_dim=128, epochs=30, grad_norm=False, horizon=1, input_dim=25, lag=10, link_len=2, log_dir='/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122915061272358569118', log_step=1, loss_func='nllloss', lr_decay=True, lr_decay_rate=0.1, lr_decay_step='15, 20', lr_init=0.0005, max_grad_norm=5, minbatch_size=64, mode='train', model='fire_GCN', nan_fill=0.5, num_layers=1, num_nodes=625, num_workers=20, output_dim=2, patch_height=25, patch_width=25, persistent_workers=True, pin_memory=True, plot=False, positive_weight=0.5, prefetch_factor=2, real_value=True, rnn_units=16, seed=10000, static_features=['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density'], teacher_forcing=False, weight_decay=0.0, window_len=10)
2022-12-29 15:06: Argument batch_size: 256
2022-12-29 15:06: Argument clc: 'vec'
2022-12-29 15:06: Argument cuda: True
2022-12-29 15:06: Argument dataset: '2020'
2022-12-29 15:06: Argument dataset_root: '/home/joel.chacon/tmp/datasets_grl/'
2022-12-29 15:06: Argument debug: False
2022-12-29 15:06: Argument default_graph: True
2022-12-29 15:06: Argument device: 'cpu'
2022-12-29 15:06: Argument dynamic_features: ['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh']
2022-12-29 15:06: Argument early_stop: True
2022-12-29 15:06: Argument early_stop_patience: 8
2022-12-29 15:06: Argument embed_dim: 128
2022-12-29 15:06: Argument epochs: 30
2022-12-29 15:06: Argument grad_norm: False
2022-12-29 15:06: Argument horizon: 1
2022-12-29 15:06: Argument input_dim: 25
2022-12-29 15:06: Argument lag: 10
2022-12-29 15:06: Argument link_len: 2
2022-12-29 15:06: Argument log_dir: '/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122915061272358569118'
2022-12-29 15:06: Argument log_step: 1
2022-12-29 15:06: Argument loss_func: 'nllloss'
2022-12-29 15:06: Argument lr_decay: True
2022-12-29 15:06: Argument lr_decay_rate: 0.1
2022-12-29 15:06: Argument lr_decay_step: '15, 20'
2022-12-29 15:06: Argument lr_init: 0.0005
2022-12-29 15:06: Argument max_grad_norm: 5
2022-12-29 15:06: Argument minbatch_size: 64
2022-12-29 15:06: Argument mode: 'train'
2022-12-29 15:06: Argument model: 'fire_GCN'
2022-12-29 15:06: Argument nan_fill: 0.5
2022-12-29 15:06: Argument num_layers: 1
2022-12-29 15:06: Argument num_nodes: 625
2022-12-29 15:06: Argument num_workers: 20
2022-12-29 15:06: Argument output_dim: 2
2022-12-29 15:06: Argument patch_height: 25
2022-12-29 15:06: Argument patch_width: 25
2022-12-29 15:06: Argument persistent_workers: True
2022-12-29 15:06: Argument pin_memory: True
2022-12-29 15:06: Argument plot: False
2022-12-29 15:06: Argument positive_weight: 0.5
2022-12-29 15:06: Argument prefetch_factor: 2
2022-12-29 15:06: Argument real_value: True
2022-12-29 15:06: Argument rnn_units: 16
2022-12-29 15:06: Argument seed: 10000
2022-12-29 15:06: Argument static_features: ['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density']
2022-12-29 15:06: Argument teacher_forcing: False
2022-12-29 15:06: Argument weight_decay: 0.0
2022-12-29 15:06: Argument window_len: 10
2022-12-29 15:06: Train Epoch 1: 3/244 Loss: 1.120079
2022-12-29 15:06: Train Epoch 1: 7/244 Loss: 1.554527
2022-12-29 15:06: Train Epoch 1: 11/244 Loss: 0.891462
2022-12-29 15:06: Train Epoch 1: 15/244 Loss: 0.691124
2022-12-29 15:06: Train Epoch 1: 19/244 Loss: 0.282981
2022-12-29 15:06: Train Epoch 1: 23/244 Loss: 0.504412
2022-12-29 15:06: Train Epoch 1: 27/244 Loss: 0.331998
2022-12-29 15:06: Train Epoch 1: 31/244 Loss: 0.281428
2022-12-29 15:07: Train Epoch 1: 35/244 Loss: 0.364732
2022-12-29 15:07: Train Epoch 1: 39/244 Loss: 0.354181
2022-12-29 15:07: Train Epoch 1: 43/244 Loss: 0.287825
2022-12-29 15:07: Train Epoch 1: 47/244 Loss: 0.231380
2022-12-29 15:07: Train Epoch 1: 51/244 Loss: 0.291264
2022-12-29 15:07: Train Epoch 1: 55/244 Loss: 0.279219
2022-12-29 15:07: Train Epoch 1: 59/244 Loss: 0.224328
2022-12-29 15:07: Train Epoch 1: 63/244 Loss: 0.204282
2022-12-29 15:07: Train Epoch 1: 67/244 Loss: 0.277931
2022-12-29 15:07: Train Epoch 1: 71/244 Loss: 0.289067
2022-12-29 15:07: Train Epoch 1: 75/244 Loss: 0.251190
2022-12-29 15:07: Train Epoch 1: 79/244 Loss: 0.188579
2022-12-29 15:07: Train Epoch 1: 83/244 Loss: 0.250871
2022-12-29 15:08: Train Epoch 1: 87/244 Loss: 0.257957
2022-12-29 15:08: Train Epoch 1: 91/244 Loss: 0.236983
2022-12-29 15:08: Train Epoch 1: 95/244 Loss: 0.168926
2022-12-29 15:08: Train Epoch 1: 99/244 Loss: 0.213461
2022-12-29 15:08: Train Epoch 1: 103/244 Loss: 0.246431
2022-12-29 15:08: Train Epoch 1: 107/244 Loss: 0.211759
2022-12-29 15:08: Train Epoch 1: 111/244 Loss: 0.177446
2022-12-29 15:08: Train Epoch 1: 115/244 Loss: 0.183413
2022-12-29 15:08: Train Epoch 1: 119/244 Loss: 0.212937
2022-12-29 15:08: Train Epoch 1: 123/244 Loss: 0.208691
2022-12-29 15:08: Train Epoch 1: 127/244 Loss: 0.173018
2022-12-29 15:08: Train Epoch 1: 131/244 Loss: 0.179345
2022-12-29 15:08: Train Epoch 1: 135/244 Loss: 0.213258
2022-12-29 15:08: Train Epoch 1: 139/244 Loss: 0.159218
2022-12-29 15:09: Train Epoch 1: 143/244 Loss: 0.178276
2022-12-29 15:09: Train Epoch 1: 147/244 Loss: 0.164935
2022-12-29 15:09: Train Epoch 1: 151/244 Loss: 0.166522
2022-12-29 15:09: Train Epoch 1: 155/244 Loss: 0.176151
2022-12-29 15:09: Train Epoch 1: 159/244 Loss: 0.142082
2022-12-29 15:09: Train Epoch 1: 163/244 Loss: 0.159416
2022-12-29 15:09: Train Epoch 1: 167/244 Loss: 0.148556
2022-12-29 15:09: Train Epoch 1: 171/244 Loss: 0.147218
2022-12-29 15:09: Train Epoch 1: 175/244 Loss: 0.140864
2022-12-29 15:09: Train Epoch 1: 179/244 Loss: 0.142489
2022-12-29 15:09: Train Epoch 1: 183/244 Loss: 0.150587
2022-12-29 15:09: Train Epoch 1: 187/244 Loss: 0.153975
2022-12-29 15:09: Train Epoch 1: 191/244 Loss: 0.125173
2022-12-29 15:10: Train Epoch 1: 195/244 Loss: 0.135030
2022-12-29 15:10: Train Epoch 1: 199/244 Loss: 0.138703
2022-12-29 15:10: Train Epoch 1: 203/244 Loss: 0.114959
2022-12-29 15:10: Train Epoch 1: 207/244 Loss: 0.118707
2022-12-29 15:10: Train Epoch 1: 211/244 Loss: 0.136356
2022-12-29 15:10: Train Epoch 1: 215/244 Loss: 0.125925
2022-12-29 15:10: Train Epoch 1: 219/244 Loss: 0.113460
2022-12-29 15:10: Train Epoch 1: 223/244 Loss: 0.132282
2022-12-29 15:10: Train Epoch 1: 227/244 Loss: 0.125444
2022-12-29 15:10: Train Epoch 1: 231/244 Loss: 0.134111
2022-12-29 15:10: Train Epoch 1: 235/244 Loss: 0.121283
2022-12-29 15:10: Train Epoch 1: 239/244 Loss: 0.117664
2022-12-29 15:10: Train Epoch 1: 243/244 Loss: 0.124641
2022-12-29 15:10: **********Train Epoch 1: averaged Loss: 0.256238 
2022-12-29 15:10: 
Epoch time elapsed: 275.0150887966156

2022-12-29 15:11: 
 metrics validation: {'precision': 0.7560038424591738, 'recall': 0.6053846153846154, 'f1-score': 0.6723622383596753, 'support': 1300, 'AUC': 0.8114903846153846, 'AUCPR': 0.7359532549046968, 'TP': 787, 'FP': 254, 'TN': 2346, 'FN': 513} 

2022-12-29 15:11: **********Val Epoch 1: average Loss: 17.306440
2022-12-29 15:11: *********************************Current best model saved!
2022-12-29 15:11: 
 Testing metrics {'precision': 0.7860169491525424, 'recall': 0.6042345276872965, 'f1-score': 0.6832412523020258, 'support': 1228, 'AUC': 0.8555293026981718, 'AUCPR': 0.7842862799210863, 'TP': 742, 'FP': 202, 'TN': 2254, 'FN': 486} 

2022-12-29 15:11: Train Epoch 2: 3/244 Loss: 0.085502
2022-12-29 15:12: Train Epoch 2: 7/244 Loss: 0.114657
2022-12-29 15:12: Train Epoch 2: 11/244 Loss: 0.119581
2022-12-29 15:12: Train Epoch 2: 15/244 Loss: 0.132032
2022-12-29 15:12: Train Epoch 2: 19/244 Loss: 0.116087
2022-12-29 15:12: Train Epoch 2: 23/244 Loss: 0.099896
2022-12-29 15:12: Train Epoch 2: 27/244 Loss: 0.090235
2022-12-29 15:12: Train Epoch 2: 31/244 Loss: 0.118825
2022-12-29 15:12: Train Epoch 2: 35/244 Loss: 0.096694
2022-12-29 15:12: Train Epoch 2: 39/244 Loss: 0.107705
2022-12-29 15:12: Train Epoch 2: 43/244 Loss: 0.120370
2022-12-29 15:12: Train Epoch 2: 47/244 Loss: 0.109772
2022-12-29 15:12: Train Epoch 2: 51/244 Loss: 0.121372
2022-12-29 15:12: Train Epoch 2: 55/244 Loss: 0.103443
2022-12-29 15:13: Train Epoch 2: 59/244 Loss: 0.087978
2022-12-29 15:13: Train Epoch 2: 63/244 Loss: 0.133452
2022-12-29 15:13: Train Epoch 2: 67/244 Loss: 0.132897
2022-12-29 15:13: Train Epoch 2: 71/244 Loss: 0.102362
2022-12-29 15:13: Train Epoch 2: 75/244 Loss: 0.113499
2022-12-29 15:13: Train Epoch 2: 79/244 Loss: 0.155340
2022-12-29 15:13: Train Epoch 2: 83/244 Loss: 0.115966
2022-12-29 15:13: Train Epoch 2: 87/244 Loss: 0.126453
2022-12-29 15:13: Train Epoch 2: 91/244 Loss: 0.108886
2022-12-29 15:13: Train Epoch 2: 95/244 Loss: 0.139378
2022-12-29 15:13: Train Epoch 2: 99/244 Loss: 0.106078
2022-12-29 15:13: Train Epoch 2: 103/244 Loss: 0.118118
2022-12-29 15:13: Train Epoch 2: 107/244 Loss: 0.088359
2022-12-29 15:14: Train Epoch 2: 111/244 Loss: 0.100783
2022-12-29 15:14: Train Epoch 2: 115/244 Loss: 0.118863
2022-12-29 15:14: Train Epoch 2: 119/244 Loss: 0.111890
2022-12-29 15:14: Train Epoch 2: 123/244 Loss: 0.135074
2022-12-29 15:14: Train Epoch 2: 127/244 Loss: 0.108751
2022-12-29 15:14: Train Epoch 2: 131/244 Loss: 0.113361
2022-12-29 15:14: Train Epoch 2: 135/244 Loss: 0.105449
2022-12-29 15:14: Train Epoch 2: 139/244 Loss: 0.106064
2022-12-29 15:14: Train Epoch 2: 143/244 Loss: 0.109968
2022-12-29 15:14: Train Epoch 2: 147/244 Loss: 0.107069
2022-12-29 15:14: Train Epoch 2: 151/244 Loss: 0.148457
2022-12-29 15:14: Train Epoch 2: 155/244 Loss: 0.109102
2022-12-29 15:14: Train Epoch 2: 159/244 Loss: 0.143158
2022-12-29 15:15: Train Epoch 2: 163/244 Loss: 0.155826
2022-12-29 15:15: Train Epoch 2: 167/244 Loss: 0.147321
2022-12-29 15:15: Train Epoch 2: 171/244 Loss: 0.088412
2022-12-29 15:15: Train Epoch 2: 175/244 Loss: 0.139310
2022-12-29 15:15: Train Epoch 2: 179/244 Loss: 0.118245
2022-12-29 15:15: Train Epoch 2: 183/244 Loss: 0.100528
2022-12-29 15:15: Train Epoch 2: 187/244 Loss: 0.140435
2022-12-29 15:15: Train Epoch 2: 191/244 Loss: 0.156504
2022-12-29 15:15: Train Epoch 2: 195/244 Loss: 0.123479
2022-12-29 15:15: Train Epoch 2: 199/244 Loss: 0.119541
2022-12-29 15:15: Train Epoch 2: 203/244 Loss: 0.160442
2022-12-29 15:15: Train Epoch 2: 207/244 Loss: 0.147832
2022-12-29 15:15: Train Epoch 2: 211/244 Loss: 0.181592
2022-12-29 15:15: Train Epoch 2: 215/244 Loss: 0.096927
2022-12-29 15:15: Train Epoch 2: 219/244 Loss: 0.127966
2022-12-29 15:16: Train Epoch 2: 223/244 Loss: 0.139204
2022-12-29 15:16: Train Epoch 2: 227/244 Loss: 0.130411
2022-12-29 15:16: Train Epoch 2: 231/244 Loss: 0.100191
2022-12-29 15:16: Train Epoch 2: 235/244 Loss: 0.121666
2022-12-29 15:16: Train Epoch 2: 239/244 Loss: 0.085517
2022-12-29 15:16: Train Epoch 2: 243/244 Loss: 0.122560
2022-12-29 15:16: **********Train Epoch 2: averaged Loss: 0.119456 
2022-12-29 15:16: 
Epoch time elapsed: 273.5181505680084

2022-12-29 15:16: 
 metrics validation: {'precision': 0.7712486883525709, 'recall': 0.5653846153846154, 'f1-score': 0.6524633821571239, 'support': 1300, 'AUC': 0.819341124260355, 'AUCPR': 0.7418527539247206, 'TP': 735, 'FP': 218, 'TN': 2382, 'FN': 565} 

2022-12-29 15:16: **********Val Epoch 2: average Loss: 18.714702
2022-12-29 15:17: 
 Testing metrics {'precision': 0.7860169491525424, 'recall': 0.6042345276872965, 'f1-score': 0.6832412523020258, 'support': 1228, 'AUC': 0.8555293026981718, 'AUCPR': 0.7842862799210863, 'TP': 742, 'FP': 202, 'TN': 2254, 'FN': 486} 

2022-12-29 15:17: Train Epoch 3: 3/244 Loss: 0.141569
2022-12-29 15:17: Train Epoch 3: 7/244 Loss: 0.128985
2022-12-29 15:17: Train Epoch 3: 11/244 Loss: 0.126985
2022-12-29 15:17: Train Epoch 3: 15/244 Loss: 0.111078
2022-12-29 15:17: Train Epoch 3: 19/244 Loss: 0.139000
2022-12-29 15:17: Train Epoch 3: 23/244 Loss: 0.121854
2022-12-29 15:17: Train Epoch 3: 27/244 Loss: 0.107825
2022-12-29 15:17: Train Epoch 3: 31/244 Loss: 0.118682
2022-12-29 15:17: Train Epoch 3: 35/244 Loss: 0.115326
2022-12-29 15:17: Train Epoch 3: 39/244 Loss: 0.119518
2022-12-29 15:18: Train Epoch 3: 43/244 Loss: 0.101654
2022-12-29 15:18: Train Epoch 3: 47/244 Loss: 0.127081
2022-12-29 15:18: Train Epoch 3: 51/244 Loss: 0.107123
2022-12-29 15:18: Train Epoch 3: 55/244 Loss: 0.099871
2022-12-29 15:18: Train Epoch 3: 59/244 Loss: 0.126529
2022-12-29 15:18: Train Epoch 3: 63/244 Loss: 0.136595
2022-12-29 15:18: Train Epoch 3: 67/244 Loss: 0.093387
2022-12-29 15:18: Train Epoch 3: 71/244 Loss: 0.121751
2022-12-29 15:18: Train Epoch 3: 75/244 Loss: 0.105786
2022-12-29 15:18: Train Epoch 3: 79/244 Loss: 0.104708
2022-12-29 15:18: Train Epoch 3: 83/244 Loss: 0.146661
2022-12-29 15:18: Train Epoch 3: 87/244 Loss: 0.103027
2022-12-29 15:18: Train Epoch 3: 91/244 Loss: 0.089830
2022-12-29 15:19: Train Epoch 3: 95/244 Loss: 0.124933
2022-12-29 15:19: Train Epoch 3: 99/244 Loss: 0.119533
2022-12-29 15:19: Train Epoch 3: 103/244 Loss: 0.102235
2022-12-29 15:19: Train Epoch 3: 107/244 Loss: 0.120897
2022-12-29 15:19: Train Epoch 3: 111/244 Loss: 0.113847
2022-12-29 15:19: Train Epoch 3: 115/244 Loss: 0.109375
2022-12-29 15:19: Train Epoch 3: 119/244 Loss: 0.090575
2022-12-29 15:19: Train Epoch 3: 123/244 Loss: 0.088807
2022-12-29 15:19: Train Epoch 3: 127/244 Loss: 0.114546
2022-12-29 15:19: Train Epoch 3: 131/244 Loss: 0.091296
2022-12-29 15:19: Train Epoch 3: 135/244 Loss: 0.087304
2022-12-29 15:19: Train Epoch 3: 139/244 Loss: 0.108532
2022-12-29 15:19: Train Epoch 3: 143/244 Loss: 0.125896
2022-12-29 15:20: Train Epoch 3: 147/244 Loss: 0.112112
2022-12-29 15:20: Train Epoch 3: 151/244 Loss: 0.120350
2022-12-29 15:20: Train Epoch 3: 155/244 Loss: 0.131840
2022-12-29 15:20: Train Epoch 3: 159/244 Loss: 0.110738
2022-12-29 15:20: Train Epoch 3: 163/244 Loss: 0.107408
2022-12-29 15:20: Train Epoch 3: 167/244 Loss: 0.110121
2022-12-29 15:20: Train Epoch 3: 171/244 Loss: 0.126110
2022-12-29 15:20: Train Epoch 3: 175/244 Loss: 0.127115
2022-12-29 15:20: Train Epoch 3: 179/244 Loss: 0.124471
2022-12-29 15:20: Train Epoch 3: 183/244 Loss: 0.119883
2022-12-29 15:20: Train Epoch 3: 187/244 Loss: 0.110158
2022-12-29 15:20: Train Epoch 3: 191/244 Loss: 0.127496
2022-12-29 15:20: Train Epoch 3: 195/244 Loss: 0.105036
2022-12-29 15:21: Train Epoch 3: 199/244 Loss: 0.093217
