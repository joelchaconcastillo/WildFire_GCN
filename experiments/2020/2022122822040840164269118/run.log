2022-12-28 22:04: log dir: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122822040840164269118
2022-12-28 22:04: Experiment log path in: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122822040840164269118
2022-12-28 22:04: Argument: Namespace(batch_size=256, clc='vec', cuda=True, dataset='2020', dataset_root='/home/joel.chacon/tmp/datasets_grl/', debug=False, default_graph=True, device='cpu', dynamic_features=['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh'], early_stop=True, early_stop_patience=8, embed_dim=128, epochs=30, grad_norm=False, horizon=1, input_dim=25, lag=10, link_len=2, log_dir='/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122822040840164269118', log_step=1, loss_func='nllloss', lr_decay=True, lr_decay_rate=0.1, lr_decay_step='15, 20', lr_init=0.0005, max_grad_norm=5, minbatch_size=64, mode='train', model='fire_GCN', nan_fill=0.5, num_layers=1, num_nodes=625, num_workers=20, output_dim=2, patch_height=25, patch_width=25, persistent_workers=True, pin_memory=True, plot=False, positive_weight=0.5, prefetch_factor=2, real_value=True, rnn_units=16, seed=10000, static_features=['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density'], teacher_forcing=False, weight_decay=0.0, window_len=10)
2022-12-28 22:04: Argument batch_size: 256
2022-12-28 22:04: Argument clc: 'vec'
2022-12-28 22:04: Argument cuda: True
2022-12-28 22:04: Argument dataset: '2020'
2022-12-28 22:04: Argument dataset_root: '/home/joel.chacon/tmp/datasets_grl/'
2022-12-28 22:04: Argument debug: False
2022-12-28 22:04: Argument default_graph: True
2022-12-28 22:04: Argument device: 'cpu'
2022-12-28 22:04: Argument dynamic_features: ['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh']
2022-12-28 22:04: Argument early_stop: True
2022-12-28 22:04: Argument early_stop_patience: 8
2022-12-28 22:04: Argument embed_dim: 128
2022-12-28 22:04: Argument epochs: 30
2022-12-28 22:04: Argument grad_norm: False
2022-12-28 22:04: Argument horizon: 1
2022-12-28 22:04: Argument input_dim: 25
2022-12-28 22:04: Argument lag: 10
2022-12-28 22:04: Argument link_len: 2
2022-12-28 22:04: Argument log_dir: '/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122822040840164269118'
2022-12-28 22:04: Argument log_step: 1
2022-12-28 22:04: Argument loss_func: 'nllloss'
2022-12-28 22:04: Argument lr_decay: True
2022-12-28 22:04: Argument lr_decay_rate: 0.1
2022-12-28 22:04: Argument lr_decay_step: '15, 20'
2022-12-28 22:04: Argument lr_init: 0.0005
2022-12-28 22:04: Argument max_grad_norm: 5
2022-12-28 22:04: Argument minbatch_size: 64
2022-12-28 22:04: Argument mode: 'train'
2022-12-28 22:04: Argument model: 'fire_GCN'
2022-12-28 22:04: Argument nan_fill: 0.5
2022-12-28 22:04: Argument num_layers: 1
2022-12-28 22:04: Argument num_nodes: 625
2022-12-28 22:04: Argument num_workers: 20
2022-12-28 22:04: Argument output_dim: 2
2022-12-28 22:04: Argument patch_height: 25
2022-12-28 22:04: Argument patch_width: 25
2022-12-28 22:04: Argument persistent_workers: True
2022-12-28 22:04: Argument pin_memory: True
2022-12-28 22:04: Argument plot: False
2022-12-28 22:04: Argument positive_weight: 0.5
2022-12-28 22:04: Argument prefetch_factor: 2
2022-12-28 22:04: Argument real_value: True
2022-12-28 22:04: Argument rnn_units: 16
2022-12-28 22:04: Argument seed: 10000
2022-12-28 22:04: Argument static_features: ['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density']
2022-12-28 22:04: Argument teacher_forcing: False
2022-12-28 22:04: Argument weight_decay: 0.0
2022-12-28 22:04: Argument window_len: 10
2022-12-28 22:04: Train Epoch 1: 3/244 Loss: 1.417423
2022-12-28 22:04: Train Epoch 1: 7/244 Loss: 1.632654
2022-12-28 22:04: Train Epoch 1: 11/244 Loss: 0.824187
2022-12-28 22:04: Train Epoch 1: 15/244 Loss: 0.621655
2022-12-28 22:04: Train Epoch 1: 19/244 Loss: 0.658074
2022-12-28 22:04: Train Epoch 1: 23/244 Loss: 0.674494
2022-12-28 22:04: Train Epoch 1: 27/244 Loss: 0.618278
2022-12-28 22:04: Train Epoch 1: 31/244 Loss: 0.570066
2022-12-28 22:04: Train Epoch 1: 35/244 Loss: 0.612444
2022-12-28 22:04: Train Epoch 1: 39/244 Loss: 0.663570
2022-12-28 22:05: Train Epoch 1: 43/244 Loss: 0.650776
2022-12-28 22:05: Train Epoch 1: 47/244 Loss: 0.598310
2022-12-28 22:05: Train Epoch 1: 51/244 Loss: 0.553988
2022-12-28 22:05: Train Epoch 1: 55/244 Loss: 0.569314
2022-12-28 22:05: Train Epoch 1: 59/244 Loss: 0.594864
2022-12-28 22:05: Train Epoch 1: 63/244 Loss: 0.565012
2022-12-28 22:05: Train Epoch 1: 67/244 Loss: 0.515807
2022-12-28 22:05: Train Epoch 1: 71/244 Loss: 0.524521
2022-12-28 22:05: Train Epoch 1: 75/244 Loss: 0.536664
2022-12-28 22:05: Train Epoch 1: 79/244 Loss: 0.566180
2022-12-28 22:05: Train Epoch 1: 83/244 Loss: 0.588188
2022-12-28 22:05: Train Epoch 1: 87/244 Loss: 0.548069
2022-12-28 22:05: Train Epoch 1: 91/244 Loss: 0.520901
2022-12-28 22:05: Train Epoch 1: 95/244 Loss: 0.552114
2022-12-28 22:06: Train Epoch 1: 99/244 Loss: 0.560072
2022-12-28 22:06: Train Epoch 1: 103/244 Loss: 0.461960
2022-12-28 22:06: Train Epoch 1: 107/244 Loss: 0.466257
2022-12-28 22:06: Train Epoch 1: 111/244 Loss: 0.527266
2022-12-28 22:06: Train Epoch 1: 115/244 Loss: 0.460006
2022-12-28 22:06: Train Epoch 1: 119/244 Loss: 0.506020
2022-12-28 22:07: Train Epoch 1: 123/244 Loss: 0.415336
2022-12-28 22:08: Train Epoch 1: 127/244 Loss: 0.427220
2022-12-28 22:08: Train Epoch 1: 131/244 Loss: 0.461730
2022-12-28 22:09: Train Epoch 1: 135/244 Loss: 0.437877
2022-12-28 22:09: Train Epoch 1: 139/244 Loss: 0.439795
2022-12-28 22:10: Train Epoch 1: 143/244 Loss: 0.407304
2022-12-28 22:10: Train Epoch 1: 147/244 Loss: 0.377520
2022-12-28 22:11: Train Epoch 1: 151/244 Loss: 0.424197
2022-12-28 22:11: Train Epoch 1: 155/244 Loss: 0.389507
2022-12-28 22:12: Train Epoch 1: 159/244 Loss: 0.393159
2022-12-28 22:12: Train Epoch 1: 163/244 Loss: 0.372947
2022-12-28 22:13: Train Epoch 1: 167/244 Loss: 0.355672
2022-12-28 22:13: Train Epoch 1: 171/244 Loss: 0.335125
2022-12-28 22:14: Train Epoch 1: 175/244 Loss: 0.342946
2022-12-28 22:14: Train Epoch 1: 179/244 Loss: 0.353358
2022-12-28 22:15: Train Epoch 1: 183/244 Loss: 0.311241
2022-12-28 22:15: Train Epoch 1: 187/244 Loss: 0.314819
2022-12-28 22:15: Train Epoch 1: 191/244 Loss: 0.306691
2022-12-28 22:16: Train Epoch 1: 195/244 Loss: 0.294276
2022-12-28 22:16: Train Epoch 1: 199/244 Loss: 0.347090
2022-12-28 22:16: Train Epoch 1: 203/244 Loss: 0.281602
2022-12-28 22:16: Train Epoch 1: 207/244 Loss: 0.320617
2022-12-28 22:17: Train Epoch 1: 211/244 Loss: 0.285909
2022-12-28 22:17: Train Epoch 1: 215/244 Loss: 0.234462
2022-12-28 22:18: Train Epoch 1: 219/244 Loss: 0.241321
2022-12-28 22:18: Train Epoch 1: 223/244 Loss: 0.234394
2022-12-28 22:19: Train Epoch 1: 227/244 Loss: 0.266371
2022-12-28 22:19: Train Epoch 1: 231/244 Loss: 0.262426
2022-12-28 22:19: Train Epoch 1: 235/244 Loss: 0.251181
2022-12-28 22:19: Train Epoch 1: 239/244 Loss: 0.274491
2022-12-28 22:19: Train Epoch 1: 243/244 Loss: 0.321080
2022-12-28 22:19: **********Train Epoch 1: averaged Loss: 0.485915 
2022-12-28 22:19: 
Epoch time elapsed: 937.6813950538635

2022-12-28 22:20: 
 metrics validation: {'precision': 0.7112068965517241, 'recall': 0.6346153846153846, 'f1-score': 0.6707317073170731, 'support': 1300, 'AUC': 0.8027742603550295, 'AUCPR': 0.7242474139511578, 'TP': 825, 'FP': 335, 'TN': 2265, 'FN': 475} 

2022-12-28 22:20: **********Val Epoch 1: average Loss: 0.534902
2022-12-28 22:20: *********************************Current best model saved!
2022-12-28 22:21: 
 Testing metrics {'precision': 0.7555147058823529, 'recall': 0.6693811074918566, 'f1-score': 0.7098445595854922, 'support': 1228, 'AUC': 0.8589892200447748, 'AUCPR': 0.7923041827420025, 'TP': 822, 'FP': 266, 'TN': 2190, 'FN': 406} 

2022-12-28 22:21: Train Epoch 2: 3/244 Loss: 0.216333
2022-12-28 22:22: Train Epoch 2: 7/244 Loss: 0.277898
2022-12-28 22:22: Train Epoch 2: 11/244 Loss: 0.258402
2022-12-28 22:22: Train Epoch 2: 15/244 Loss: 0.257193
2022-12-28 22:22: Train Epoch 2: 19/244 Loss: 0.252872
2022-12-28 22:22: Train Epoch 2: 23/244 Loss: 0.245289
2022-12-28 22:22: Train Epoch 2: 27/244 Loss: 0.251041
2022-12-28 22:23: Train Epoch 2: 31/244 Loss: 0.246157
2022-12-28 22:23: Train Epoch 2: 35/244 Loss: 0.203730
2022-12-28 22:23: Train Epoch 2: 39/244 Loss: 0.230625
2022-12-28 22:23: Train Epoch 2: 43/244 Loss: 0.267870
2022-12-28 22:23: Train Epoch 2: 47/244 Loss: 0.308014
2022-12-28 22:24: Train Epoch 2: 51/244 Loss: 0.265998
2022-12-28 22:24: Train Epoch 2: 55/244 Loss: 0.241424
2022-12-28 22:24: Train Epoch 2: 59/244 Loss: 0.178013
2022-12-28 22:24: Train Epoch 2: 63/244 Loss: 0.182301
2022-12-28 22:24: Train Epoch 2: 67/244 Loss: 0.210084
2022-12-28 22:24: Train Epoch 2: 71/244 Loss: 0.293274
2022-12-28 22:25: Train Epoch 2: 75/244 Loss: 0.257153
2022-12-28 22:25: Train Epoch 2: 79/244 Loss: 0.200364
2022-12-28 22:25: Train Epoch 2: 83/244 Loss: 0.257925
2022-12-28 22:25: Train Epoch 2: 87/244 Loss: 0.239867
2022-12-28 22:25: Train Epoch 2: 91/244 Loss: 0.236873
2022-12-28 22:26: Train Epoch 2: 95/244 Loss: 0.239524
2022-12-28 22:26: Train Epoch 2: 99/244 Loss: 0.201265
2022-12-28 22:26: Train Epoch 2: 103/244 Loss: 0.247140
2022-12-28 22:26: Train Epoch 2: 107/244 Loss: 0.212807
2022-12-28 22:26: Train Epoch 2: 111/244 Loss: 0.219735
2022-12-28 22:27: Train Epoch 2: 115/244 Loss: 0.205488
2022-12-28 22:27: Train Epoch 2: 119/244 Loss: 0.211139
2022-12-28 22:27: Train Epoch 2: 123/244 Loss: 0.230845
2022-12-28 22:27: Train Epoch 2: 127/244 Loss: 0.277897
2022-12-28 22:27: Train Epoch 2: 131/244 Loss: 0.253399
2022-12-28 22:27: Train Epoch 2: 135/244 Loss: 0.219049
2022-12-28 22:28: Train Epoch 2: 139/244 Loss: 0.225908
2022-12-28 22:28: Train Epoch 2: 143/244 Loss: 0.222848
2022-12-28 22:28: Train Epoch 2: 147/244 Loss: 0.211949
2022-12-28 22:28: Train Epoch 2: 151/244 Loss: 0.242392
2022-12-28 22:28: Train Epoch 2: 155/244 Loss: 0.223572
2022-12-28 22:29: Train Epoch 2: 159/244 Loss: 0.225646
2022-12-28 22:29: Train Epoch 2: 163/244 Loss: 0.232878
2022-12-28 22:29: Train Epoch 2: 167/244 Loss: 0.209090
2022-12-28 22:29: Train Epoch 2: 171/244 Loss: 0.175557
2022-12-28 22:29: Train Epoch 2: 175/244 Loss: 0.206306
2022-12-28 22:29: Train Epoch 2: 179/244 Loss: 0.213380
2022-12-28 22:30: Train Epoch 2: 183/244 Loss: 0.185648
2022-12-28 22:30: Train Epoch 2: 187/244 Loss: 0.196971
2022-12-28 22:30: Train Epoch 2: 191/244 Loss: 0.193733
2022-12-28 22:30: Train Epoch 2: 195/244 Loss: 0.193223
2022-12-28 22:30: Train Epoch 2: 199/244 Loss: 0.264806
2022-12-28 22:30: Train Epoch 2: 203/244 Loss: 0.212623
2022-12-28 22:31: Train Epoch 2: 207/244 Loss: 0.309886
2022-12-28 22:31: Train Epoch 2: 211/244 Loss: 0.206349
2022-12-28 22:31: Train Epoch 2: 215/244 Loss: 0.283872
2022-12-28 22:31: Train Epoch 2: 219/244 Loss: 0.306090
2022-12-28 22:31: Train Epoch 2: 223/244 Loss: 0.197024
2022-12-28 22:31: Train Epoch 2: 227/244 Loss: 0.252227
2022-12-28 22:31: Train Epoch 2: 231/244 Loss: 0.263474
2022-12-28 22:31: Train Epoch 2: 235/244 Loss: 0.235301
2022-12-28 22:32: Train Epoch 2: 239/244 Loss: 0.246257
2022-12-28 22:32: Train Epoch 2: 243/244 Loss: 0.294161
2022-12-28 22:32: **********Train Epoch 2: averaged Loss: 0.234888 
2022-12-28 22:32: 
Epoch time elapsed: 648.2417016029358

2022-12-28 22:33: 
 metrics validation: {'precision': 0.6983606557377049, 'recall': 0.6553846153846153, 'f1-score': 0.6761904761904762, 'support': 1300, 'AUC': 0.8269304733727809, 'AUCPR': 0.7465350427343026, 'TP': 852, 'FP': 368, 'TN': 2232, 'FN': 448} 

2022-12-28 22:33: **********Val Epoch 2: average Loss: 0.535794
2022-12-28 22:34: 
 Testing metrics {'precision': 0.7555147058823529, 'recall': 0.6693811074918566, 'f1-score': 0.7098445595854922, 'support': 1228, 'AUC': 0.8589892200447748, 'AUCPR': 0.7923041827420025, 'TP': 822, 'FP': 266, 'TN': 2190, 'FN': 406} 

2022-12-28 22:34: Train Epoch 3: 3/244 Loss: 0.261746
2022-12-28 22:34: Train Epoch 3: 7/244 Loss: 0.258430
2022-12-28 22:34: Train Epoch 3: 11/244 Loss: 0.319212
2022-12-28 22:35: Train Epoch 3: 15/244 Loss: 0.281626
2022-12-28 22:35: Train Epoch 3: 19/244 Loss: 0.258589
2022-12-28 22:35: Train Epoch 3: 23/244 Loss: 0.272462
2022-12-28 22:35: Train Epoch 3: 27/244 Loss: 0.248723
2022-12-28 22:35: Train Epoch 3: 31/244 Loss: 0.216739
2022-12-28 22:36: Train Epoch 3: 35/244 Loss: 0.271138
2022-12-28 22:36: Train Epoch 3: 39/244 Loss: 0.223969
2022-12-28 22:36: Train Epoch 3: 43/244 Loss: 0.231604
2022-12-28 22:36: Train Epoch 3: 47/244 Loss: 0.247550
2022-12-28 22:36: Train Epoch 3: 51/244 Loss: 0.227109
2022-12-28 22:37: Train Epoch 3: 55/244 Loss: 0.235631
2022-12-28 22:37: Train Epoch 3: 59/244 Loss: 0.169964
2022-12-28 22:37: Train Epoch 3: 63/244 Loss: 0.247507
2022-12-28 22:37: Train Epoch 3: 67/244 Loss: 0.284280
2022-12-28 22:37: Train Epoch 3: 71/244 Loss: 0.290059
2022-12-28 22:38: Train Epoch 3: 75/244 Loss: 0.212944
2022-12-28 22:38: Train Epoch 3: 79/244 Loss: 0.235857
2022-12-28 22:38: Train Epoch 3: 83/244 Loss: 0.214823
2022-12-28 22:38: Train Epoch 3: 87/244 Loss: 0.305957
2022-12-28 22:38: Train Epoch 3: 91/244 Loss: 0.309025
2022-12-28 22:39: Train Epoch 3: 95/244 Loss: 0.218791
2022-12-28 22:39: Train Epoch 3: 99/244 Loss: 0.189672
2022-12-28 22:39: Train Epoch 3: 103/244 Loss: 0.225272
2022-12-28 22:39: Train Epoch 3: 107/244 Loss: 0.197187
