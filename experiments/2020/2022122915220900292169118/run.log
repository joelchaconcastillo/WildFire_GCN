2022-12-29 15:22: log dir: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122915220900292169118
2022-12-29 15:22: Experiment log path in: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122915220900292169118
2022-12-29 15:22: Argument: Namespace(batch_size=256, clc='vec', cuda=True, dataset='2020', dataset_root='/home/joel.chacon/tmp/datasets_grl/', debug=False, default_graph=True, device='cpu', dynamic_features=['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh'], early_stop=True, early_stop_patience=8, embed_dim=128, epochs=30, grad_norm=False, horizon=1, input_dim=25, lag=10, link_len=2, log_dir='/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122915220900292169118', log_step=1, loss_func='nllloss', lr_decay=True, lr_decay_rate=0.1, lr_decay_step='15, 20', lr_init=0.0005, max_grad_norm=5, minbatch_size=256, mode='train', model='fire_GCN', nan_fill=0.5, num_layers=1, num_nodes=625, num_workers=20, output_dim=2, patch_height=25, patch_width=25, persistent_workers=True, pin_memory=True, plot=False, positive_weight=0.5, prefetch_factor=2, real_value=True, rnn_units=16, seed=10000, static_features=['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density'], teacher_forcing=False, weight_decay=0.0, window_len=10)
2022-12-29 15:22: Argument batch_size: 256
2022-12-29 15:22: Argument clc: 'vec'
2022-12-29 15:22: Argument cuda: True
2022-12-29 15:22: Argument dataset: '2020'
2022-12-29 15:22: Argument dataset_root: '/home/joel.chacon/tmp/datasets_grl/'
2022-12-29 15:22: Argument debug: False
2022-12-29 15:22: Argument default_graph: True
2022-12-29 15:22: Argument device: 'cpu'
2022-12-29 15:22: Argument dynamic_features: ['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh']
2022-12-29 15:22: Argument early_stop: True
2022-12-29 15:22: Argument early_stop_patience: 8
2022-12-29 15:22: Argument embed_dim: 128
2022-12-29 15:22: Argument epochs: 30
2022-12-29 15:22: Argument grad_norm: False
2022-12-29 15:22: Argument horizon: 1
2022-12-29 15:22: Argument input_dim: 25
2022-12-29 15:22: Argument lag: 10
2022-12-29 15:22: Argument link_len: 2
2022-12-29 15:22: Argument log_dir: '/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122915220900292169118'
2022-12-29 15:22: Argument log_step: 1
2022-12-29 15:22: Argument loss_func: 'nllloss'
2022-12-29 15:22: Argument lr_decay: True
2022-12-29 15:22: Argument lr_decay_rate: 0.1
2022-12-29 15:22: Argument lr_decay_step: '15, 20'
2022-12-29 15:22: Argument lr_init: 0.0005
2022-12-29 15:22: Argument max_grad_norm: 5
2022-12-29 15:22: Argument minbatch_size: 256
2022-12-29 15:22: Argument mode: 'train'
2022-12-29 15:22: Argument model: 'fire_GCN'
2022-12-29 15:22: Argument nan_fill: 0.5
2022-12-29 15:22: Argument num_layers: 1
2022-12-29 15:22: Argument num_nodes: 625
2022-12-29 15:22: Argument num_workers: 20
2022-12-29 15:22: Argument output_dim: 2
2022-12-29 15:22: Argument patch_height: 25
2022-12-29 15:22: Argument patch_width: 25
2022-12-29 15:22: Argument persistent_workers: True
2022-12-29 15:22: Argument pin_memory: True
2022-12-29 15:22: Argument plot: False
2022-12-29 15:22: Argument positive_weight: 0.5
2022-12-29 15:22: Argument prefetch_factor: 2
2022-12-29 15:22: Argument real_value: True
2022-12-29 15:22: Argument rnn_units: 16
2022-12-29 15:22: Argument seed: 10000
2022-12-29 15:22: Argument static_features: ['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density']
2022-12-29 15:22: Argument teacher_forcing: False
2022-12-29 15:22: Argument weight_decay: 0.0
2022-12-29 15:22: Argument window_len: 10
2022-12-29 15:22: Train Epoch 1: 0/61 Loss: 1.120079
2022-12-29 15:22: Train Epoch 1: 1/61 Loss: 1.554527
2022-12-29 15:22: Train Epoch 1: 2/61 Loss: 0.891462
2022-12-29 15:22: Train Epoch 1: 3/61 Loss: 0.691124
2022-12-29 15:22: Train Epoch 1: 4/61 Loss: 0.282981
2022-12-29 15:22: Train Epoch 1: 5/61 Loss: 0.504412
2022-12-29 15:23: Train Epoch 1: 6/61 Loss: 0.331998
2022-12-29 15:23: Train Epoch 1: 7/61 Loss: 0.281428
2022-12-29 15:23: Train Epoch 1: 8/61 Loss: 0.364732
2022-12-29 15:23: Train Epoch 1: 9/61 Loss: 0.354182
2022-12-29 15:23: Train Epoch 1: 10/61 Loss: 0.287828
2022-12-29 15:23: Train Epoch 1: 11/61 Loss: 0.231380
2022-12-29 15:23: Train Epoch 1: 12/61 Loss: 0.291262
2022-12-29 15:23: Train Epoch 1: 13/61 Loss: 0.279219
2022-12-29 15:23: Train Epoch 1: 14/61 Loss: 0.224328
2022-12-29 15:24: Train Epoch 1: 15/61 Loss: 0.204282
2022-12-29 15:24: Train Epoch 1: 16/61 Loss: 0.277931
2022-12-29 15:24: Train Epoch 1: 17/61 Loss: 0.289066
2022-12-29 15:24: Train Epoch 1: 18/61 Loss: 0.251189
2022-12-29 15:24: Train Epoch 1: 19/61 Loss: 0.188580
2022-12-29 15:24: Train Epoch 1: 20/61 Loss: 0.250875
2022-12-29 15:24: Train Epoch 1: 21/61 Loss: 0.257963
2022-12-29 15:24: Train Epoch 1: 22/61 Loss: 0.236988
2022-12-29 15:24: Train Epoch 1: 23/61 Loss: 0.168926
2022-12-29 15:25: Train Epoch 1: 24/61 Loss: 0.213463
2022-12-29 15:25: Train Epoch 1: 25/61 Loss: 0.246437
2022-12-29 15:25: Train Epoch 1: 26/61 Loss: 0.211775
2022-12-29 15:25: Train Epoch 1: 27/61 Loss: 0.177457
2022-12-29 15:25: Train Epoch 1: 28/61 Loss: 0.183406
2022-12-29 15:25: Train Epoch 1: 29/61 Loss: 0.212940
2022-12-29 15:25: Train Epoch 1: 30/61 Loss: 0.208704
2022-12-29 15:25: Train Epoch 1: 31/61 Loss: 0.173029
2022-12-29 15:25: Train Epoch 1: 32/61 Loss: 0.179351
2022-12-29 15:25: Train Epoch 1: 33/61 Loss: 0.213261
2022-12-29 15:26: Train Epoch 1: 34/61 Loss: 0.159232
2022-12-29 15:26: Train Epoch 1: 35/61 Loss: 0.178298
2022-12-29 15:26: Train Epoch 1: 36/61 Loss: 0.164945
2022-12-29 15:26: Train Epoch 1: 37/61 Loss: 0.166507
2022-12-29 15:26: Train Epoch 1: 38/61 Loss: 0.176162
2022-12-29 15:26: Train Epoch 1: 39/61 Loss: 0.142096
2022-12-29 15:26: Train Epoch 1: 40/61 Loss: 0.159413
2022-12-29 15:26: Train Epoch 1: 41/61 Loss: 0.148554
2022-12-29 15:26: Train Epoch 1: 42/61 Loss: 0.147226
2022-12-29 15:26: Train Epoch 1: 43/61 Loss: 0.140868
2022-12-29 15:26: Train Epoch 1: 44/61 Loss: 0.142485
2022-12-29 15:26: Train Epoch 1: 45/61 Loss: 0.150582
2022-12-29 15:27: Train Epoch 1: 46/61 Loss: 0.153969
2022-12-29 15:27: Train Epoch 1: 47/61 Loss: 0.125166
2022-12-29 15:27: Train Epoch 1: 48/61 Loss: 0.135030
2022-12-29 15:27: Train Epoch 1: 49/61 Loss: 0.138712
2022-12-29 15:27: Train Epoch 1: 50/61 Loss: 0.114982
2022-12-29 15:27: Train Epoch 1: 51/61 Loss: 0.118696
2022-12-29 15:27: Train Epoch 1: 52/61 Loss: 0.136324
2022-12-29 15:27: Train Epoch 1: 53/61 Loss: 0.125935
2022-12-29 15:27: Train Epoch 1: 54/61 Loss: 0.113406
2022-12-29 15:27: Train Epoch 1: 55/61 Loss: 0.132298
2022-12-29 15:27: Train Epoch 1: 56/61 Loss: 0.125414
2022-12-29 15:27: Train Epoch 1: 57/61 Loss: 0.134026
2022-12-29 15:28: Train Epoch 1: 58/61 Loss: 0.121354
2022-12-29 15:28: Train Epoch 1: 59/61 Loss: 0.117636
2022-12-29 15:28: Train Epoch 1: 60/61 Loss: 0.124378
2022-12-29 15:28: **********Train Epoch 1: averaged Loss: 0.256234 
2022-12-29 15:28: 
Epoch time elapsed: 368.55153727531433

2022-12-29 15:28: 
 metrics validation: {'precision': 0.7560038424591738, 'recall': 0.6053846153846154, 'f1-score': 0.6723622383596753, 'support': 1300, 'AUC': 0.8115298816568046, 'AUCPR': 0.7357602730342812, 'TP': 787, 'FP': 254, 'TN': 2346, 'FN': 513} 

2022-12-29 15:28: **********Val Epoch 1: average Loss: 0.270277
2022-12-29 15:28: *********************************Current best model saved!
2022-12-29 15:29: 
 Testing metrics {'precision': 0.7858649789029536, 'recall': 0.6066775244299675, 'f1-score': 0.6847426470588236, 'support': 1228, 'AUC': 0.8557958837759551, 'AUCPR': 0.7850181160840264, 'TP': 745, 'FP': 203, 'TN': 2253, 'FN': 483} 

2022-12-29 15:29: Train Epoch 2: 0/61 Loss: 0.085768
2022-12-29 15:29: Train Epoch 2: 1/61 Loss: 0.115099
2022-12-29 15:29: Train Epoch 2: 2/61 Loss: 0.120252
2022-12-29 15:29: Train Epoch 2: 3/61 Loss: 0.132103
2022-12-29 15:30: Train Epoch 2: 4/61 Loss: 0.115027
2022-12-29 15:30: Train Epoch 2: 5/61 Loss: 0.100424
2022-12-29 15:30: Train Epoch 2: 6/61 Loss: 0.090384
2022-12-29 15:30: Train Epoch 2: 7/61 Loss: 0.119944
2022-12-29 15:30: Train Epoch 2: 8/61 Loss: 0.096556
2022-12-29 15:30: Train Epoch 2: 9/61 Loss: 0.108660
2022-12-29 15:31: Train Epoch 2: 10/61 Loss: 0.120681
2022-12-29 15:31: Train Epoch 2: 11/61 Loss: 0.111426
2022-12-29 15:31: Train Epoch 2: 12/61 Loss: 0.121191
2022-12-29 15:31: Train Epoch 2: 13/61 Loss: 0.100923
2022-12-29 15:31: Train Epoch 2: 14/61 Loss: 0.088491
2022-12-29 15:31: Train Epoch 2: 15/61 Loss: 0.131712
2022-12-29 15:31: Train Epoch 2: 16/61 Loss: 0.129240
2022-12-29 15:31: Train Epoch 2: 17/61 Loss: 0.100345
2022-12-29 15:31: Train Epoch 2: 18/61 Loss: 0.111922
2022-12-29 15:32: Train Epoch 2: 19/61 Loss: 0.155336
2022-12-29 15:32: Train Epoch 2: 20/61 Loss: 0.115106
2022-12-29 15:32: Train Epoch 2: 21/61 Loss: 0.125607
2022-12-29 15:32: Train Epoch 2: 22/61 Loss: 0.106947
2022-12-29 15:32: Train Epoch 2: 23/61 Loss: 0.138195
2022-12-29 15:32: Train Epoch 2: 24/61 Loss: 0.104979
2022-12-29 15:32: Train Epoch 2: 25/61 Loss: 0.117580
2022-12-29 15:32: Train Epoch 2: 26/61 Loss: 0.085837
2022-12-29 15:32: Train Epoch 2: 27/61 Loss: 0.100383
2022-12-29 15:32: Train Epoch 2: 28/61 Loss: 0.117583
2022-12-29 15:33: Train Epoch 2: 29/61 Loss: 0.112139
2022-12-29 15:33: Train Epoch 2: 30/61 Loss: 0.133380
2022-12-29 15:33: Train Epoch 2: 31/61 Loss: 0.108470
2022-12-29 15:33: Train Epoch 2: 32/61 Loss: 0.113362
2022-12-29 15:33: Train Epoch 2: 33/61 Loss: 0.104816
2022-12-29 15:33: Train Epoch 2: 34/61 Loss: 0.104767
2022-12-29 15:33: Train Epoch 2: 35/61 Loss: 0.109639
2022-12-29 15:33: Train Epoch 2: 36/61 Loss: 0.106583
2022-12-29 15:33: Train Epoch 2: 37/61 Loss: 0.143180
2022-12-29 15:33: Train Epoch 2: 38/61 Loss: 0.111281
2022-12-29 15:34: Train Epoch 2: 39/61 Loss: 0.141891
2022-12-29 15:34: Train Epoch 2: 40/61 Loss: 0.150784
2022-12-29 15:34: Train Epoch 2: 41/61 Loss: 0.146170
2022-12-29 15:34: Train Epoch 2: 42/61 Loss: 0.087023
2022-12-29 15:34: Train Epoch 2: 43/61 Loss: 0.140214
2022-12-29 15:34: Train Epoch 2: 44/61 Loss: 0.121250
2022-12-29 15:34: Train Epoch 2: 45/61 Loss: 0.101842
2022-12-29 15:34: Train Epoch 2: 46/61 Loss: 0.140863
2022-12-29 15:34: Train Epoch 2: 47/61 Loss: 0.154347
2022-12-29 15:35: Train Epoch 2: 48/61 Loss: 0.125231
2022-12-29 15:35: Train Epoch 2: 49/61 Loss: 0.116173
2022-12-29 15:35: Train Epoch 2: 50/61 Loss: 0.157935
2022-12-29 15:35: Train Epoch 2: 51/61 Loss: 0.146796
2022-12-29 15:35: Train Epoch 2: 52/61 Loss: 0.177174
2022-12-29 15:35: Train Epoch 2: 53/61 Loss: 0.097942
2022-12-29 15:35: Train Epoch 2: 54/61 Loss: 0.129919
2022-12-29 15:35: Train Epoch 2: 55/61 Loss: 0.137440
2022-12-29 15:35: Train Epoch 2: 56/61 Loss: 0.131142
2022-12-29 15:35: Train Epoch 2: 57/61 Loss: 0.100687
2022-12-29 15:35: Train Epoch 2: 58/61 Loss: 0.120043
2022-12-29 15:36: Train Epoch 2: 59/61 Loss: 0.084328
2022-12-29 15:36: Train Epoch 2: 60/61 Loss: 0.121670
2022-12-29 15:36: **********Train Epoch 2: averaged Loss: 0.118790 
2022-12-29 15:36: 
Epoch time elapsed: 414.90433979034424

2022-12-29 15:36: 
 metrics validation: {'precision': 0.7769633507853403, 'recall': 0.5707692307692308, 'f1-score': 0.6580931263858093, 'support': 1300, 'AUC': 0.8224069526627218, 'AUCPR': 0.7446123781933967, 'TP': 742, 'FP': 213, 'TN': 2387, 'FN': 558} 

2022-12-29 15:36: **********Val Epoch 2: average Loss: 0.290269
2022-12-29 15:37: 
 Testing metrics {'precision': 0.7858649789029536, 'recall': 0.6066775244299675, 'f1-score': 0.6847426470588236, 'support': 1228, 'AUC': 0.8557958837759551, 'AUCPR': 0.7850181160840264, 'TP': 745, 'FP': 203, 'TN': 2253, 'FN': 483} 

2022-12-29 15:37: Train Epoch 3: 0/61 Loss: 0.141434
2022-12-29 15:37: Train Epoch 3: 1/61 Loss: 0.128288
2022-12-29 15:37: Train Epoch 3: 2/61 Loss: 0.123901
2022-12-29 15:37: Train Epoch 3: 3/61 Loss: 0.110692
2022-12-29 15:37: Train Epoch 3: 4/61 Loss: 0.137648
2022-12-29 15:37: Train Epoch 3: 5/61 Loss: 0.121168
2022-12-29 15:37: Train Epoch 3: 6/61 Loss: 0.107524
2022-12-29 15:38: Train Epoch 3: 7/61 Loss: 0.115333
2022-12-29 15:38: Train Epoch 3: 8/61 Loss: 0.113903
2022-12-29 15:38: Train Epoch 3: 9/61 Loss: 0.117948
2022-12-29 15:38: Train Epoch 3: 10/61 Loss: 0.099873
2022-12-29 15:38: Train Epoch 3: 11/61 Loss: 0.124780
2022-12-29 15:38: Train Epoch 3: 12/61 Loss: 0.105537
2022-12-29 15:38: Train Epoch 3: 13/61 Loss: 0.097555
2022-12-29 15:38: Train Epoch 3: 14/61 Loss: 0.125017
2022-12-29 15:38: Train Epoch 3: 15/61 Loss: 0.136169
