2022-12-22 09:21: log dir: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/20221222092157
2022-12-22 09:21: Experiment log path in: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/20221222092157
2022-12-22 09:21: Argument: Namespace(batch_size=256, clc='vec', cuda=True, dataset='2020', dataset_root='/home/joel.chacon/tmp/datasets_grl/', debug=False, default_graph=True, device='cpu', dynamic_features=['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh'], early_stop=True, early_stop_patience=5, embed_dim=64, epochs=30, gamma=1.0, grad_norm=False, horizon=1, input_dim=25, lag=10, link_len=2, log_dir='/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/20221222092157', log_step=1, loss_func='nllloss', lr_decay=True, lr_decay_rate=0.1, lr_decay_step='10, 15, 20, 25', lr_init=0.0001, mae_thresh=None, mape_thresh=0.0, max_grad_norm=5, mode='train', model='fire_GCN', nan_fill=0.5, num_layers=1, num_nodes=625, num_workers=12, output_dim=1, patch_height=25, patch_width=25, persistent_workers=True, pin_memory=True, plot=False, positive_weight=0.5, prefetch_factor=2, real_value=True, rnn_units=32, seed=1992, static_features=['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density'], teacher_forcing=False, test_ratio=0.2, val_ratio=0.2, weight_decay=0.01, window_len=10)
2022-12-22 09:21: Argument batch_size: 256
2022-12-22 09:21: Argument clc: 'vec'
2022-12-22 09:21: Argument cuda: True
2022-12-22 09:21: Argument dataset: '2020'
2022-12-22 09:21: Argument dataset_root: '/home/joel.chacon/tmp/datasets_grl/'
2022-12-22 09:21: Argument debug: False
2022-12-22 09:21: Argument default_graph: True
2022-12-22 09:21: Argument device: 'cpu'
2022-12-22 09:21: Argument dynamic_features: ['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh']
2022-12-22 09:21: Argument early_stop: True
2022-12-22 09:21: Argument early_stop_patience: 5
2022-12-22 09:21: Argument embed_dim: 64
2022-12-22 09:21: Argument epochs: 30
2022-12-22 09:21: Argument gamma: 1.0
2022-12-22 09:21: Argument grad_norm: False
2022-12-22 09:21: Argument horizon: 1
2022-12-22 09:21: Argument input_dim: 25
2022-12-22 09:21: Argument lag: 10
2022-12-22 09:21: Argument link_len: 2
2022-12-22 09:21: Argument log_dir: '/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/20221222092157'
2022-12-22 09:21: Argument log_step: 1
2022-12-22 09:21: Argument loss_func: 'nllloss'
2022-12-22 09:21: Argument lr_decay: True
2022-12-22 09:21: Argument lr_decay_rate: 0.1
2022-12-22 09:21: Argument lr_decay_step: '10, 15, 20, 25'
2022-12-22 09:21: Argument lr_init: 0.0001
2022-12-22 09:21: Argument mae_thresh: None
2022-12-22 09:21: Argument mape_thresh: 0.0
2022-12-22 09:21: Argument max_grad_norm: 5
2022-12-22 09:21: Argument mode: 'train'
2022-12-22 09:21: Argument model: 'fire_GCN'
2022-12-22 09:21: Argument nan_fill: 0.5
2022-12-22 09:21: Argument num_layers: 1
2022-12-22 09:21: Argument num_nodes: 625
2022-12-22 09:21: Argument num_workers: 12
2022-12-22 09:21: Argument output_dim: 1
2022-12-22 09:21: Argument patch_height: 25
2022-12-22 09:21: Argument patch_width: 25
2022-12-22 09:21: Argument persistent_workers: True
2022-12-22 09:21: Argument pin_memory: True
2022-12-22 09:21: Argument plot: False
2022-12-22 09:21: Argument positive_weight: 0.5
2022-12-22 09:21: Argument prefetch_factor: 2
2022-12-22 09:21: Argument real_value: True
2022-12-22 09:21: Argument rnn_units: 32
2022-12-22 09:21: Argument seed: 1992
2022-12-22 09:21: Argument static_features: ['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density']
2022-12-22 09:21: Argument teacher_forcing: False
2022-12-22 09:21: Argument test_ratio: 0.2
2022-12-22 09:21: Argument val_ratio: 0.2
2022-12-22 09:21: Argument weight_decay: 0.01
2022-12-22 09:21: Argument window_len: 10
2022-12-22 09:22: Train Epoch 1: 0/61 Loss: 1.240824
2022-12-22 09:22: Train Epoch 1: 1/61 Loss: 1.036751
2022-12-22 09:22: Train Epoch 1: 2/61 Loss: 1.150916
2022-12-22 09:22: Train Epoch 1: 3/61 Loss: 0.847511
2022-12-22 09:22: Train Epoch 1: 4/61 Loss: 1.027941
2022-12-22 09:22: Train Epoch 1: 5/61 Loss: 1.000034
2022-12-22 09:22: Train Epoch 1: 6/61 Loss: 0.889449
2022-12-22 09:22: Train Epoch 1: 7/61 Loss: 0.935603
2022-12-22 09:22: Train Epoch 1: 8/61 Loss: 0.817895
2022-12-22 09:22: Train Epoch 1: 9/61 Loss: 0.799499
2022-12-22 09:22: Train Epoch 1: 10/61 Loss: 0.757167
2022-12-22 09:23: Train Epoch 1: 11/61 Loss: 0.774579
2022-12-22 09:23: Train Epoch 1: 12/61 Loss: 0.797503
2022-12-22 09:23: Train Epoch 1: 13/61 Loss: 0.812580
2022-12-22 09:23: Train Epoch 1: 14/61 Loss: 0.732818
2022-12-22 09:23: Train Epoch 1: 15/61 Loss: 0.722487
2022-12-22 09:23: Train Epoch 1: 16/61 Loss: 0.774364
2022-12-22 09:23: Train Epoch 1: 17/61 Loss: 0.796876
2022-12-22 09:23: Train Epoch 1: 18/61 Loss: 0.717390
2022-12-22 09:23: Train Epoch 1: 19/61 Loss: 0.670724
2022-12-22 09:23: Train Epoch 1: 20/61 Loss: 0.753164
2022-12-22 09:23: Train Epoch 1: 21/61 Loss: 0.718729
2022-12-22 09:23: Train Epoch 1: 22/61 Loss: 0.715347
2022-12-22 09:24: Train Epoch 1: 23/61 Loss: 0.714136
2022-12-22 09:24: Train Epoch 1: 24/61 Loss: 0.682927
2022-12-22 09:24: Train Epoch 1: 25/61 Loss: 0.665427
2022-12-22 09:24: Train Epoch 1: 26/61 Loss: 0.706205
2022-12-22 09:24: Train Epoch 1: 27/61 Loss: 0.744344
2022-12-22 09:24: Train Epoch 1: 28/61 Loss: 0.775376
2022-12-22 09:24: Train Epoch 1: 29/61 Loss: 0.739093
2022-12-22 09:24: Train Epoch 1: 30/61 Loss: 0.708542
2022-12-22 09:24: Train Epoch 1: 31/61 Loss: 0.662340
2022-12-22 09:24: Train Epoch 1: 32/61 Loss: 0.715192
2022-12-22 09:24: Train Epoch 1: 33/61 Loss: 0.657552
2022-12-22 09:24: Train Epoch 1: 34/61 Loss: 0.657145
2022-12-22 09:25: Train Epoch 1: 35/61 Loss: 0.659839
2022-12-22 09:25: Train Epoch 1: 36/61 Loss: 0.681437
2022-12-22 09:25: Train Epoch 1: 37/61 Loss: 0.696446
2022-12-22 09:25: Train Epoch 1: 38/61 Loss: 0.670759
2022-12-22 09:25: Train Epoch 1: 39/61 Loss: 0.628404
2022-12-22 09:25: Train Epoch 1: 40/61 Loss: 0.628220
2022-12-22 09:25: Train Epoch 1: 41/61 Loss: 0.637563
2022-12-22 09:25: Train Epoch 1: 42/61 Loss: 0.643944
2022-12-22 09:25: Train Epoch 1: 43/61 Loss: 0.675247
2022-12-22 09:25: Train Epoch 1: 44/61 Loss: 0.615845
2022-12-22 09:25: Train Epoch 1: 45/61 Loss: 0.590210
2022-12-22 09:25: Train Epoch 1: 46/61 Loss: 0.582666
2022-12-22 09:26: Train Epoch 1: 47/61 Loss: 0.606052
2022-12-22 09:26: Train Epoch 1: 48/61 Loss: 0.647523
2022-12-22 09:26: Train Epoch 1: 49/61 Loss: 0.584188
2022-12-22 09:26: Train Epoch 1: 50/61 Loss: 0.544050
2022-12-22 09:26: Train Epoch 1: 51/61 Loss: 0.604169
2022-12-22 09:26: Train Epoch 1: 52/61 Loss: 0.618655
2022-12-22 09:26: Train Epoch 1: 53/61 Loss: 0.575861
2022-12-22 09:26: Train Epoch 1: 54/61 Loss: 0.531643
2022-12-22 09:26: Train Epoch 1: 55/61 Loss: 0.568200
2022-12-22 09:26: Train Epoch 1: 56/61 Loss: 0.519700
2022-12-22 09:26: Train Epoch 1: 57/61 Loss: 0.542159
2022-12-22 09:27: Train Epoch 1: 58/61 Loss: 0.505026
2022-12-22 09:27: Train Epoch 1: 59/61 Loss: 0.479091
2022-12-22 09:27: Train Epoch 1: 60/61 Loss: 0.468337
2022-12-22 09:27: **********Train Epoch 1: averaged Loss: 0.711863 
2022-12-22 09:27: 
Epoch time elapsed: 313.1600549221039

2022-12-22 09:27: 
 metrics validation: {'precision': 0.7149817295980512, 'recall': 0.45153846153846156, 'f1-score': 0.5535124941065536, 'support': 1300, 'AUC': 0.7617204142011834, 'AUCPR': 0.6584018486272059, 'TP': 587, 'FP': 234, 'TN': 2366, 'FN': 713} 

2022-12-22 09:27: **********Val Epoch 1: average Loss: 0.526592
2022-12-22 09:27: *********************************Current best model saved!
2022-12-22 09:28: 
 Testing metrics {'precision': 0.7957658779576587, 'recall': 0.5203583061889251, 'f1-score': 0.6292466765140324, 'support': 1228, 'AUC': 0.8431588133561099, 'AUCPR': 0.7569570842390373, 'TP': 639, 'FP': 164, 'TN': 2292, 'FN': 589} 

2022-12-22 09:28: Train Epoch 2: 0/61 Loss: 0.544885
2022-12-22 09:28: Train Epoch 2: 1/61 Loss: 0.466997
2022-12-22 09:28: Train Epoch 2: 2/61 Loss: 0.483521
2022-12-22 09:28: Train Epoch 2: 3/61 Loss: 0.497310
2022-12-22 09:28: Train Epoch 2: 4/61 Loss: 0.496455
2022-12-22 09:28: Train Epoch 2: 5/61 Loss: 0.488300
2022-12-22 09:29: Train Epoch 2: 6/61 Loss: 0.455760
2022-12-22 09:29: Train Epoch 2: 7/61 Loss: 0.477416
2022-12-22 09:29: Train Epoch 2: 8/61 Loss: 0.384321
2022-12-22 09:29: Train Epoch 2: 9/61 Loss: 0.435680
2022-12-22 09:29: Train Epoch 2: 10/61 Loss: 0.387017
2022-12-22 09:29: Train Epoch 2: 11/61 Loss: 0.426341
2022-12-22 09:29: Train Epoch 2: 12/61 Loss: 0.403614
2022-12-22 09:29: Train Epoch 2: 13/61 Loss: 0.381117
2022-12-22 09:29: Train Epoch 2: 14/61 Loss: 0.522203
2022-12-22 09:29: Train Epoch 2: 15/61 Loss: 0.493584
2022-12-22 09:29: Train Epoch 2: 16/61 Loss: 0.448352
2022-12-22 09:29: Train Epoch 2: 17/61 Loss: 0.446654
2022-12-22 09:30: Train Epoch 2: 18/61 Loss: 0.432169
2022-12-22 09:30: Train Epoch 2: 19/61 Loss: 0.411619
2022-12-22 09:30: Train Epoch 2: 20/61 Loss: 0.403639
2022-12-22 09:30: Train Epoch 2: 21/61 Loss: 0.339012
2022-12-22 09:30: Train Epoch 2: 22/61 Loss: 0.518132
2022-12-22 09:30: Train Epoch 2: 23/61 Loss: 0.452866
2022-12-22 09:30: Train Epoch 2: 24/61 Loss: 0.379994
2022-12-22 09:30: Train Epoch 2: 25/61 Loss: 0.471892
2022-12-22 09:30: Train Epoch 2: 26/61 Loss: 0.489321
2022-12-22 09:30: Train Epoch 2: 27/61 Loss: 0.392569
2022-12-22 09:30: Train Epoch 2: 28/61 Loss: 0.358585
2022-12-22 09:31: Train Epoch 2: 29/61 Loss: 0.398972
2022-12-22 09:31: Train Epoch 2: 30/61 Loss: 0.352782
2022-12-22 09:31: Train Epoch 2: 31/61 Loss: 0.395904
2022-12-22 09:31: Train Epoch 2: 32/61 Loss: 0.396793
2022-12-22 09:31: Train Epoch 2: 33/61 Loss: 0.374972
2022-12-22 09:31: Train Epoch 2: 34/61 Loss: 0.400959
2022-12-22 09:31: Train Epoch 2: 35/61 Loss: 0.436214
2022-12-22 09:31: Train Epoch 2: 36/61 Loss: 0.359531
2022-12-22 09:31: Train Epoch 2: 37/61 Loss: 0.306502
2022-12-22 09:31: Train Epoch 2: 38/61 Loss: 0.380363
2022-12-22 09:31: Train Epoch 2: 39/61 Loss: 0.389263
2022-12-22 09:31: Train Epoch 2: 40/61 Loss: 0.353132
2022-12-22 09:32: Train Epoch 2: 41/61 Loss: 0.408970
2022-12-22 09:32: Train Epoch 2: 42/61 Loss: 0.318820
2022-12-22 09:32: Train Epoch 2: 43/61 Loss: 0.356264
2022-12-22 09:32: Train Epoch 2: 44/61 Loss: 0.380293
2022-12-22 09:32: Train Epoch 2: 45/61 Loss: 0.426315
2022-12-22 09:32: Train Epoch 2: 46/61 Loss: 0.326937
2022-12-22 09:32: Train Epoch 2: 47/61 Loss: 0.405346
2022-12-22 09:32: Train Epoch 2: 48/61 Loss: 0.448872
2022-12-22 09:32: Train Epoch 2: 49/61 Loss: 0.369805
2022-12-22 09:32: Train Epoch 2: 50/61 Loss: 0.405972
2022-12-22 09:32: Train Epoch 2: 51/61 Loss: 0.404414
2022-12-22 09:32: Train Epoch 2: 52/61 Loss: 0.392922
2022-12-22 09:33: Train Epoch 2: 53/61 Loss: 0.358429
2022-12-22 09:33: Train Epoch 2: 54/61 Loss: 0.334137
2022-12-22 09:33: Train Epoch 2: 55/61 Loss: 0.344157
2022-12-22 09:33: Train Epoch 2: 56/61 Loss: 0.429617
2022-12-22 09:33: Train Epoch 2: 57/61 Loss: 0.279618
2022-12-22 09:33: Train Epoch 2: 58/61 Loss: 0.390342
2022-12-22 09:33: Train Epoch 2: 59/61 Loss: 0.411287
2022-12-22 09:33: Train Epoch 2: 60/61 Loss: 0.375918
2022-12-22 09:33: **********Train Epoch 2: averaged Loss: 0.409888 
2022-12-22 09:33: 
Epoch time elapsed: 319.68171191215515

2022-12-22 09:34: 
 metrics validation: {'precision': 0.6701119724375538, 'recall': 0.5984615384615385, 'f1-score': 0.6322633075985372, 'support': 1300, 'AUC': 0.7820316568047337, 'AUCPR': 0.7137333128560749, 'TP': 778, 'FP': 383, 'TN': 2217, 'FN': 522} 

2022-12-22 09:34: **********Val Epoch 2: average Loss: 0.655137
2022-12-22 09:34: 
 Testing metrics {'precision': 0.7524229074889868, 'recall': 0.6954397394136808, 'f1-score': 0.7228099873042741, 'support': 1228, 'AUC': 0.8468385606213328, 'AUCPR': 0.7736731261054627, 'TP': 854, 'FP': 281, 'TN': 2175, 'FN': 374} 

2022-12-22 09:34: Train Epoch 3: 0/61 Loss: 0.347240
2022-12-22 09:34: Train Epoch 3: 1/61 Loss: 0.400636
2022-12-22 09:34: Train Epoch 3: 2/61 Loss: 0.356472
2022-12-22 09:35: Train Epoch 3: 3/61 Loss: 0.344019
2022-12-22 09:35: Train Epoch 3: 4/61 Loss: 0.349177
2022-12-22 09:35: Train Epoch 3: 5/61 Loss: 0.390679
2022-12-22 09:35: Train Epoch 3: 6/61 Loss: 0.281897
2022-12-22 09:35: Train Epoch 3: 7/61 Loss: 0.335191
2022-12-22 09:35: Train Epoch 3: 8/61 Loss: 0.369470
2022-12-22 09:35: Train Epoch 3: 9/61 Loss: 0.357029
2022-12-22 09:35: Train Epoch 3: 10/61 Loss: 0.386558
2022-12-22 09:35: Train Epoch 3: 11/61 Loss: 0.367872
2022-12-22 09:35: Train Epoch 3: 12/61 Loss: 0.357843
2022-12-22 09:35: Train Epoch 3: 13/61 Loss: 0.349205
2022-12-22 09:36: Train Epoch 3: 14/61 Loss: 0.317288
2022-12-22 09:36: Train Epoch 3: 15/61 Loss: 0.324595
2022-12-22 09:36: Train Epoch 3: 16/61 Loss: 0.343862
2022-12-22 09:36: Train Epoch 3: 17/61 Loss: 0.296896
2022-12-22 09:36: Train Epoch 3: 18/61 Loss: 0.228717
2022-12-22 09:36: Train Epoch 3: 19/61 Loss: 0.424185
2022-12-22 09:36: Train Epoch 3: 20/61 Loss: 0.326176
2022-12-22 09:36: Train Epoch 3: 21/61 Loss: 0.389364
2022-12-22 09:36: Train Epoch 3: 22/61 Loss: 0.291640
2022-12-22 09:36: Train Epoch 3: 23/61 Loss: 0.354469
2022-12-22 09:36: Train Epoch 3: 24/61 Loss: 0.349446
2022-12-22 09:36: Train Epoch 3: 25/61 Loss: 0.339576
2022-12-22 09:37: Train Epoch 3: 26/61 Loss: 0.327376
2022-12-22 09:37: Train Epoch 3: 27/61 Loss: 0.466835
2022-12-22 09:37: Train Epoch 3: 28/61 Loss: 0.362595
2022-12-22 09:37: Train Epoch 3: 29/61 Loss: 0.407306
2022-12-22 09:37: Train Epoch 3: 30/61 Loss: 0.319235
2022-12-22 09:37: Train Epoch 3: 31/61 Loss: 0.329286
2022-12-22 09:37: Train Epoch 3: 32/61 Loss: 0.299602
2022-12-22 09:37: Train Epoch 3: 33/61 Loss: 0.451546
2022-12-22 09:37: Train Epoch 3: 34/61 Loss: 0.302912
2022-12-22 09:37: Train Epoch 3: 35/61 Loss: 0.296151
2022-12-22 09:37: Train Epoch 3: 36/61 Loss: 0.400663
2022-12-22 09:38: Train Epoch 3: 37/61 Loss: 0.266893
2022-12-22 09:38: Train Epoch 3: 38/61 Loss: 0.376793
2022-12-22 09:38: Train Epoch 3: 39/61 Loss: 0.326638
2022-12-22 09:38: Train Epoch 3: 40/61 Loss: 0.383291
2022-12-22 09:38: Train Epoch 3: 41/61 Loss: 0.312050
2022-12-22 09:38: Train Epoch 3: 42/61 Loss: 0.294776
2022-12-22 09:38: Train Epoch 3: 43/61 Loss: 0.290883
2022-12-22 09:38: Train Epoch 3: 44/61 Loss: 0.338389
2022-12-22 09:38: Train Epoch 3: 45/61 Loss: 0.330962
2022-12-22 09:38: Train Epoch 3: 46/61 Loss: 0.340622
2022-12-22 09:38: Train Epoch 3: 47/61 Loss: 0.384347
2022-12-22 09:38: Train Epoch 3: 48/61 Loss: 0.370461
2022-12-22 09:39: Train Epoch 3: 49/61 Loss: 0.421892
2022-12-22 09:39: Train Epoch 3: 50/61 Loss: 0.377482
2022-12-22 09:39: Train Epoch 3: 51/61 Loss: 0.345872
2022-12-22 09:39: Train Epoch 3: 52/61 Loss: 0.347678
2022-12-22 09:39: Train Epoch 3: 53/61 Loss: 0.382646
2022-12-22 09:39: Train Epoch 3: 54/61 Loss: 0.340202
2022-12-22 09:39: Train Epoch 3: 55/61 Loss: 0.290878
2022-12-22 09:39: Train Epoch 3: 56/61 Loss: 0.319023
2022-12-22 09:39: Train Epoch 3: 57/61 Loss: 0.316163
2022-12-22 09:39: Train Epoch 3: 58/61 Loss: 0.367147
2022-12-22 09:39: Train Epoch 3: 59/61 Loss: 0.328082
2022-12-22 09:40: Train Epoch 3: 60/61 Loss: 0.259865
2022-12-22 09:40: **********Train Epoch 3: averaged Loss: 0.345181 
2022-12-22 09:40: 
Epoch time elapsed: 324.89670610427856

2022-12-22 09:40: 
 metrics validation: {'precision': 0.75, 'recall': 0.5169230769230769, 'f1-score': 0.6120218579234972, 'support': 1300, 'AUC': 0.7888263313609467, 'AUCPR': 0.7265557258361363, 'TP': 672, 'FP': 224, 'TN': 2376, 'FN': 628} 

2022-12-22 09:40: **********Val Epoch 3: average Loss: 0.687420
2022-12-22 09:41: 
 Testing metrics {'precision': 0.7997685185185185, 'recall': 0.5627035830618893, 'f1-score': 0.6606118546845124, 'support': 1228, 'AUC': 0.8459356995830195, 'AUCPR': 0.7800045709186241, 'TP': 691, 'FP': 173, 'TN': 2283, 'FN': 537} 

2022-12-22 09:41: Train Epoch 4: 0/61 Loss: 0.332727
2022-12-22 09:41: Train Epoch 4: 1/61 Loss: 0.376204
2022-12-22 09:41: Train Epoch 4: 2/61 Loss: 0.384550
2022-12-22 09:41: Train Epoch 4: 3/61 Loss: 0.340426
2022-12-22 09:41: Train Epoch 4: 4/61 Loss: 0.308664
2022-12-22 09:41: Train Epoch 4: 5/61 Loss: 0.322590
2022-12-22 09:41: Train Epoch 4: 6/61 Loss: 0.352137
2022-12-22 09:41: Train Epoch 4: 7/61 Loss: 0.289231
2022-12-22 09:41: Train Epoch 4: 8/61 Loss: 0.299924
2022-12-22 09:41: Train Epoch 4: 9/61 Loss: 0.379190
2022-12-22 09:42: Train Epoch 4: 10/61 Loss: 0.408923
2022-12-22 09:42: Train Epoch 4: 11/61 Loss: 0.375029
