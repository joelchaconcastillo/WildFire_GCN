2023-01-04 22:00: log dir: /home/joel.chacon/tmp/convLSTM/WildFire_GCN/experiments/2020/2023010422002859609419251
2023-01-04 22:00: Experiment log path in: /home/joel.chacon/tmp/convLSTM/WildFire_GCN/experiments/2020/2023010422002859609419251
2023-01-04 22:00: Argument: Namespace(batch_size=256, clc='vec', cuda=True, dataset='2020', dataset_root='/home/joel.chacon/tmp/datasets_grl/', debug=False, default_graph=True, device='cpu', dynamic_features=['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh'], early_stop=True, early_stop_patience=5, embed_dim=32, epochs=30, grad_norm=False, horizon=1, input_dim=25, lag=10, link_len=2, log_dir='/home/joel.chacon/tmp/convLSTM/WildFire_GCN/experiments/2020/2023010422002859609419251', log_step=1, loss_func='nllloss', lr_decay=True, lr_decay_rate=0.1, lr_decay_step='15', lr_init=0.0001, max_grad_norm=5, minbatch_size=64, mode='train', model='fire_GCN', nan_fill=-1.0, num_layers=1, num_nodes=625, num_workers=12, output_dim=2, patch_height=25, patch_width=25, persistent_workers=True, pin_memory=True, plot=False, positive_weight=0.5, prefetch_factor=2, real_value=True, rnn_units=32, seed=100000, static_features=['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density'], teacher_forcing=False, weight_decay=0.01, window_len=10)
2023-01-04 22:00: Argument batch_size: 256
2023-01-04 22:00: Argument clc: 'vec'
2023-01-04 22:00: Argument cuda: True
2023-01-04 22:00: Argument dataset: '2020'
2023-01-04 22:00: Argument dataset_root: '/home/joel.chacon/tmp/datasets_grl/'
2023-01-04 22:00: Argument debug: False
2023-01-04 22:00: Argument default_graph: True
2023-01-04 22:00: Argument device: 'cpu'
2023-01-04 22:00: Argument dynamic_features: ['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh']
2023-01-04 22:00: Argument early_stop: True
2023-01-04 22:00: Argument early_stop_patience: 5
2023-01-04 22:00: Argument embed_dim: 32
2023-01-04 22:00: Argument epochs: 30
2023-01-04 22:00: Argument grad_norm: False
2023-01-04 22:00: Argument horizon: 1
2023-01-04 22:00: Argument input_dim: 25
2023-01-04 22:00: Argument lag: 10
2023-01-04 22:00: Argument link_len: 2
2023-01-04 22:00: Argument log_dir: '/home/joel.chacon/tmp/convLSTM/WildFire_GCN/experiments/2020/2023010422002859609419251'
2023-01-04 22:00: Argument log_step: 1
2023-01-04 22:00: Argument loss_func: 'nllloss'
2023-01-04 22:00: Argument lr_decay: True
2023-01-04 22:00: Argument lr_decay_rate: 0.1
2023-01-04 22:00: Argument lr_decay_step: '15'
2023-01-04 22:00: Argument lr_init: 0.0001
2023-01-04 22:00: Argument max_grad_norm: 5
2023-01-04 22:00: Argument minbatch_size: 64
2023-01-04 22:00: Argument mode: 'train'
2023-01-04 22:00: Argument model: 'fire_GCN'
2023-01-04 22:00: Argument nan_fill: -1.0
2023-01-04 22:00: Argument num_layers: 1
2023-01-04 22:00: Argument num_nodes: 625
2023-01-04 22:00: Argument num_workers: 12
2023-01-04 22:00: Argument output_dim: 2
2023-01-04 22:00: Argument patch_height: 25
2023-01-04 22:00: Argument patch_width: 25
2023-01-04 22:00: Argument persistent_workers: True
2023-01-04 22:00: Argument pin_memory: True
2023-01-04 22:00: Argument plot: False
2023-01-04 22:00: Argument positive_weight: 0.5
2023-01-04 22:00: Argument prefetch_factor: 2
2023-01-04 22:00: Argument real_value: True
2023-01-04 22:00: Argument rnn_units: 32
2023-01-04 22:00: Argument seed: 100000
2023-01-04 22:00: Argument static_features: ['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density']
2023-01-04 22:00: Argument teacher_forcing: False
2023-01-04 22:00: Argument weight_decay: 0.01
2023-01-04 22:00: Argument window_len: 10
2023-01-04 22:00: 52.03942108154297 <-----
2023-01-04 22:00: 50.28440856933594 <-----
2023-01-04 22:00: 54.125465393066406 <-----
2023-01-04 22:00: 48.6310920715332 <-----
2023-01-04 22:00: Train Epoch 1: 3/634 Loss: 0.801095
2023-01-04 22:00: 44.74711608886719 <-----
2023-01-04 22:00: 29.45116424560547 <-----
2023-01-04 22:00: 35.99675750732422 <-----
2023-01-04 22:00: 34.66605758666992 <-----
2023-01-04 22:00: Train Epoch 1: 7/634 Loss: 0.565864
2023-01-04 22:00: 31.480375289916992 <-----
2023-01-04 22:00: 34.216793060302734 <-----
2023-01-04 22:00: 39.83625793457031 <-----
2023-01-04 22:00: 35.84617614746094 <-----
2023-01-04 22:00: Train Epoch 1: 11/634 Loss: 0.552264
2023-01-04 22:00: 27.929031372070312 <-----
2023-01-04 22:00: 27.853158950805664 <-----
2023-01-04 22:00: 39.13248062133789 <-----
2023-01-04 22:00: 30.40286636352539 <-----
2023-01-04 22:01: Train Epoch 1: 15/634 Loss: 0.489522
2023-01-04 22:01: 28.146446228027344 <-----
2023-01-04 22:01: 30.90232276916504 <-----
2023-01-04 22:01: 32.72511672973633 <-----
2023-01-04 22:01: 29.31389808654785 <-----
2023-01-04 22:01: Train Epoch 1: 19/634 Loss: 0.472999
2023-01-04 22:01: 28.27203941345215 <-----
2023-01-04 22:01: 30.972469329833984 <-----
2023-01-04 22:01: 28.972900390625 <-----
2023-01-04 22:01: 28.519615173339844 <-----
2023-01-04 22:01: Train Epoch 1: 23/634 Loss: 0.456004
2023-01-04 22:01: 27.985078811645508 <-----
2023-01-04 22:01: 24.236347198486328 <-----
2023-01-04 22:01: 32.60114288330078 <-----
2023-01-04 22:01: 25.8778076171875 <-----
2023-01-04 22:01: Train Epoch 1: 27/634 Loss: 0.432423
2023-01-04 22:01: 27.225522994995117 <-----
2023-01-04 22:01: 26.03938865661621 <-----
2023-01-04 22:01: 30.632280349731445 <-----
2023-01-04 22:01: 30.05232048034668 <-----
2023-01-04 22:01: Train Epoch 1: 31/634 Loss: 0.445115
2023-01-04 22:01: 24.58550262451172 <-----
2023-01-04 22:01: 22.235628128051758 <-----
2023-01-04 22:01: 25.627620697021484 <-----
2023-01-04 22:01: 29.452482223510742 <-----
2023-01-04 22:01: Train Epoch 1: 35/634 Loss: 0.398052
2023-01-04 22:01: 26.402999877929688 <-----
2023-01-04 22:01: 21.03809928894043 <-----
2023-01-04 22:01: 24.799978256225586 <-----
2023-01-04 22:01: 26.5240478515625 <-----
2023-01-04 22:01: Train Epoch 1: 39/634 Loss: 0.385801
2023-01-04 22:01: 27.65564727783203 <-----
2023-01-04 22:01: 25.88275718688965 <-----
2023-01-04 22:01: 24.947425842285156 <-----
2023-01-04 22:01: 22.38699722290039 <-----
2023-01-04 22:01: Train Epoch 1: 43/634 Loss: 0.394034
2023-01-04 22:01: 23.108638763427734 <-----
2023-01-04 22:01: 22.169658660888672 <-----
2023-01-04 22:01: 21.80840492248535 <-----
2023-01-04 22:01: 23.55487060546875 <-----
2023-01-04 22:01: Train Epoch 1: 47/634 Loss: 0.354069
2023-01-04 22:01: 25.2518367767334 <-----
2023-01-04 22:01: 20.84451675415039 <-----
2023-01-04 22:01: 24.436527252197266 <-----
2023-01-04 22:01: 27.215076446533203 <-----
2023-01-04 22:01: Train Epoch 1: 51/634 Loss: 0.381828
2023-01-04 22:01: 25.32014274597168 <-----
2023-01-04 22:02: 19.041418075561523 <-----
2023-01-04 22:02: 20.815357208251953 <-----
2023-01-04 22:02: 26.981409072875977 <-----
2023-01-04 22:02: Train Epoch 1: 55/634 Loss: 0.359993
2023-01-04 22:02: 22.932619094848633 <-----
2023-01-04 22:02: 22.708974838256836 <-----
2023-01-04 22:02: 25.96363067626953 <-----
2023-01-04 22:02: 26.477609634399414 <-----
2023-01-04 22:02: Train Epoch 1: 59/634 Loss: 0.383136
2023-01-04 22:02: 21.974592208862305 <-----
2023-01-04 22:02: 19.395559310913086 <-----
2023-01-04 22:02: 17.374303817749023 <-----
2023-01-04 22:02: 23.35558319091797 <-----
2023-01-04 22:02: Train Epoch 1: 63/634 Loss: 0.320703
2023-01-04 22:02: 23.89691162109375 <-----
2023-01-04 22:02: 23.021488189697266 <-----
2023-01-04 22:02: 18.870317459106445 <-----
2023-01-04 22:02: 18.568540573120117 <-----
2023-01-04 22:02: Train Epoch 1: 67/634 Loss: 0.329521
2023-01-04 22:02: 17.631671905517578 <-----
2023-01-04 22:02: 22.723922729492188 <-----
2023-01-04 22:02: 17.568571090698242 <-----
2023-01-04 22:02: 21.65091323852539 <-----
2023-01-04 22:02: Train Epoch 1: 71/634 Loss: 0.310840
2023-01-04 22:02: 21.953144073486328 <-----
2023-01-04 22:02: 16.1121883392334 <-----
2023-01-04 22:02: 19.98563003540039 <-----
2023-01-04 22:02: 18.67598533630371 <-----
2023-01-04 22:02: Train Epoch 1: 75/634 Loss: 0.299715
2023-01-04 22:02: 22.716598510742188 <-----
2023-01-04 22:02: 21.518619537353516 <-----
2023-01-04 22:02: 21.528228759765625 <-----
2023-01-04 22:02: 20.735158920288086 <-----
2023-01-04 22:02: Train Epoch 1: 79/634 Loss: 0.337885
2023-01-04 22:02: 19.056194305419922 <-----
2023-01-04 22:02: 18.922950744628906 <-----
2023-01-04 22:02: 22.626087188720703 <-----
2023-01-04 22:02: 18.479267120361328 <-----
2023-01-04 22:02: Train Epoch 1: 83/634 Loss: 0.308924
2023-01-04 22:02: 19.061180114746094 <-----
2023-01-04 22:02: 18.67850685119629 <-----
2023-01-04 22:02: 16.822860717773438 <-----
2023-01-04 22:02: 20.66062355041504 <-----
2023-01-04 22:02: Train Epoch 1: 87/634 Loss: 0.293841
2023-01-04 22:02: 22.521194458007812 <-----
2023-01-04 22:02: 15.563955307006836 <-----
2023-01-04 22:03: 20.000463485717773 <-----
2023-01-04 22:03: 15.601340293884277 <-----
2023-01-04 22:03: Train Epoch 1: 91/634 Loss: 0.287840
2023-01-04 22:03: 19.15447235107422 <-----
2023-01-04 22:03: 20.303564071655273 <-----
2023-01-04 22:03: 15.228410720825195 <-----
2023-01-04 22:03: 19.32024574279785 <-----
2023-01-04 22:03: Train Epoch 1: 95/634 Loss: 0.289089
2023-01-04 22:03: 20.096799850463867 <-----
2023-01-04 22:03: 14.757577896118164 <-----
2023-01-04 22:03: 17.726318359375 <-----
2023-01-04 22:03: 21.161399841308594 <-----
2023-01-04 22:03: Train Epoch 1: 99/634 Loss: 0.288055
2023-01-04 22:03: 15.967869758605957 <-----
2023-01-04 22:03: 18.834983825683594 <-----
2023-01-04 22:03: 14.39353084564209 <-----
2023-01-04 22:03: 15.689016342163086 <-----
2023-01-04 22:03: Train Epoch 1: 103/634 Loss: 0.253459
2023-01-04 22:03: 18.088163375854492 <-----
2023-01-04 22:03: 18.20115852355957 <-----
2023-01-04 22:03: 15.549793243408203 <-----
2023-01-04 22:03: 21.564434051513672 <-----
2023-01-04 22:03: Train Epoch 1: 107/634 Loss: 0.286733
2023-01-04 22:03: 18.62738609313965 <-----
2023-01-04 22:03: 22.395666122436523 <-----
2023-01-04 22:03: 12.496789932250977 <-----
2023-01-04 22:03: 18.364255905151367 <-----
2023-01-04 22:03: Train Epoch 1: 111/634 Loss: 0.280797
2023-01-04 22:03: 15.561149597167969 <-----
2023-01-04 22:03: 16.37869644165039 <-----
2023-01-04 22:03: 18.807647705078125 <-----
2023-01-04 22:03: 18.58168601989746 <-----
2023-01-04 22:03: Train Epoch 1: 115/634 Loss: 0.270817
2023-01-04 22:03: 19.480087280273438 <-----
2023-01-04 22:03: 20.00631332397461 <-----
2023-01-04 22:03: 16.520362854003906 <-----
2023-01-04 22:03: 23.17530632019043 <-----
2023-01-04 22:03: Train Epoch 1: 119/634 Loss: 0.309305
2023-01-04 22:03: 18.59707260131836 <-----
2023-01-04 22:03: 20.019866943359375 <-----
2023-01-04 22:03: 16.051143646240234 <-----
2023-01-04 22:03: 18.222108840942383 <-----
2023-01-04 22:03: Train Epoch 1: 123/634 Loss: 0.284727
2023-01-04 22:03: 15.648988723754883 <-----
2023-01-04 22:03: 19.984607696533203 <-----
2023-01-04 22:03: 18.336530685424805 <-----
2023-01-04 22:03: 20.018234252929688 <-----
2023-01-04 22:03: Train Epoch 1: 127/634 Loss: 0.289017
2023-01-04 22:03: 16.677492141723633 <-----
2023-01-04 22:04: 15.695612907409668 <-----
2023-01-04 22:04: 15.413607597351074 <-----
2023-01-04 22:04: 13.633724212646484 <-----
2023-01-04 22:04: Train Epoch 1: 131/634 Loss: 0.239924
2023-01-04 22:04: 14.977301597595215 <-----
2023-01-04 22:04: 17.447195053100586 <-----
2023-01-04 22:04: 16.135061264038086 <-----
2023-01-04 22:04: 18.480989456176758 <-----
2023-01-04 22:04: Train Epoch 1: 135/634 Loss: 0.261877
2023-01-04 22:04: 18.509469985961914 <-----
2023-01-04 22:04: 18.20078468322754 <-----
2023-01-04 22:04: 14.84748649597168 <-----
2023-01-04 22:04: 15.643495559692383 <-----
2023-01-04 22:04: Train Epoch 1: 139/634 Loss: 0.262505
2023-01-04 22:04: 15.502775192260742 <-----
2023-01-04 22:04: 15.51079273223877 <-----
2023-01-04 22:04: 15.883912086486816 <-----
2023-01-04 22:04: 19.029508590698242 <-----
2023-01-04 22:04: Train Epoch 1: 143/634 Loss: 0.257527
2023-01-04 22:04: 14.997620582580566 <-----
2023-01-04 22:04: 14.383753776550293 <-----
2023-01-04 22:04: 13.937777519226074 <-----
2023-01-04 22:04: 15.753618240356445 <-----
2023-01-04 22:04: Train Epoch 1: 147/634 Loss: 0.230753
2023-01-04 22:04: 18.51300811767578 <-----
2023-01-04 22:04: 13.30009651184082 <-----
2023-01-04 22:04: 15.447480201721191 <-----
2023-01-04 22:04: 15.596658706665039 <-----
2023-01-04 22:04: Train Epoch 1: 151/634 Loss: 0.245536
2023-01-04 22:04: 18.853099822998047 <-----
2023-01-04 22:04: 16.298755645751953 <-----
2023-01-04 22:04: 12.307781219482422 <-----
2023-01-04 22:04: 10.660587310791016 <-----
2023-01-04 22:04: Train Epoch 1: 155/634 Loss: 0.227032
2023-01-04 22:04: 13.659900665283203 <-----
2023-01-04 22:04: 11.718249320983887 <-----
2023-01-04 22:04: 12.296835899353027 <-----
2023-01-04 22:04: 16.348800659179688 <-----
2023-01-04 22:04: Train Epoch 1: 159/634 Loss: 0.211030
2023-01-04 22:04: 12.519083976745605 <-----
2023-01-04 22:04: 14.723072052001953 <-----
2023-01-04 22:04: 12.992363929748535 <-----
2023-01-04 22:04: 18.66090965270996 <-----
2023-01-04 22:04: Train Epoch 1: 163/634 Loss: 0.230060
2023-01-04 22:04: 15.815925598144531 <-----
2023-01-04 22:04: 17.19828224182129 <-----
2023-01-04 22:04: 11.563998222351074 <-----
2023-01-04 22:04: 17.189865112304688 <-----
2023-01-04 22:04: Train Epoch 1: 167/634 Loss: 0.241282
2023-01-04 22:04: 15.655923843383789 <-----
2023-01-04 22:05: 17.950288772583008 <-----
2023-01-04 22:05: 15.51333999633789 <-----
2023-01-04 22:05: 16.500446319580078 <-----
2023-01-04 22:05: Train Epoch 1: 171/634 Loss: 0.256328
2023-01-04 22:05: 11.708767890930176 <-----
2023-01-04 22:05: 18.11041831970215 <-----
2023-01-04 22:05: 15.00623607635498 <-----
2023-01-04 22:05: 12.91696834564209 <-----
2023-01-04 22:05: Train Epoch 1: 175/634 Loss: 0.225556
2023-01-04 22:05: 15.335614204406738 <-----
2023-01-04 22:05: 14.033832550048828 <-----
2023-01-04 22:05: 10.2935152053833 <-----
2023-01-04 22:05: 14.424857139587402 <-----
2023-01-04 22:05: Train Epoch 1: 179/634 Loss: 0.211281
