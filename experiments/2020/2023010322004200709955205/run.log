2023-01-03 22:00: log dir: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2023010322004200709955205
2023-01-03 22:00: Experiment log path in: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2023010322004200709955205
2023-01-03 22:00: Argument: Namespace(alpha=1, batch_size=256, clc='vec', cuda=True, dataset='2020', dataset_root='/home/joel.chacon/tmp/datasets_grl/', debug=False, default_graph=True, device='cpu', dynamic_features=['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh'], early_stop=True, early_stop_patience=8, embed_dim=32, epochs=30, grad_norm=False, horizon=1, input_dim=25, lag=10, link_len=2, log_dir='/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2023010322004200709955205', log_step=1, loss_func='nllloss', lr_decay=True, lr_decay_rate=0.1, lr_decay_step='15, 20', lr_init=0.0001, maxDimHoles=1, max_grad_norm=5, minbatch_size=64, mode='train', model='fire_GCN', nan_fill=-1.0, num_layers=1, num_nodes=625, num_workers=12, output_dim=2, patch_height=25, patch_width=25, persistent_workers=True, pin_memory=True, plot=False, positive_weight=0.5, prefetch_factor=2, real_value=True, rnn_units=16, scaleParameter=1.0, seed=1992, sizeBorder=1, static_features=['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density'], teacher_forcing=False, weight_decay=0.0, window_len=10)
2023-01-03 22:00: Argument alpha: 1
2023-01-03 22:00: Argument batch_size: 256
2023-01-03 22:00: Argument clc: 'vec'
2023-01-03 22:00: Argument cuda: True
2023-01-03 22:00: Argument dataset: '2020'
2023-01-03 22:00: Argument dataset_root: '/home/joel.chacon/tmp/datasets_grl/'
2023-01-03 22:00: Argument debug: False
2023-01-03 22:00: Argument default_graph: True
2023-01-03 22:00: Argument device: 'cpu'
2023-01-03 22:00: Argument dynamic_features: ['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh']
2023-01-03 22:00: Argument early_stop: True
2023-01-03 22:00: Argument early_stop_patience: 8
2023-01-03 22:00: Argument embed_dim: 32
2023-01-03 22:00: Argument epochs: 30
2023-01-03 22:00: Argument grad_norm: False
2023-01-03 22:00: Argument horizon: 1
2023-01-03 22:00: Argument input_dim: 25
2023-01-03 22:00: Argument lag: 10
2023-01-03 22:00: Argument link_len: 2
2023-01-03 22:00: Argument log_dir: '/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2023010322004200709955205'
2023-01-03 22:00: Argument log_step: 1
2023-01-03 22:00: Argument loss_func: 'nllloss'
2023-01-03 22:00: Argument lr_decay: True
2023-01-03 22:00: Argument lr_decay_rate: 0.1
2023-01-03 22:00: Argument lr_decay_step: '15, 20'
2023-01-03 22:00: Argument lr_init: 0.0001
2023-01-03 22:00: Argument maxDimHoles: 1
2023-01-03 22:00: Argument max_grad_norm: 5
2023-01-03 22:00: Argument minbatch_size: 64
2023-01-03 22:00: Argument mode: 'train'
2023-01-03 22:00: Argument model: 'fire_GCN'
2023-01-03 22:00: Argument nan_fill: -1.0
2023-01-03 22:00: Argument num_layers: 1
2023-01-03 22:00: Argument num_nodes: 625
2023-01-03 22:00: Argument num_workers: 12
2023-01-03 22:00: Argument output_dim: 2
2023-01-03 22:00: Argument patch_height: 25
2023-01-03 22:00: Argument patch_width: 25
2023-01-03 22:00: Argument persistent_workers: True
2023-01-03 22:00: Argument pin_memory: True
2023-01-03 22:00: Argument plot: False
2023-01-03 22:00: Argument positive_weight: 0.5
2023-01-03 22:00: Argument prefetch_factor: 2
2023-01-03 22:00: Argument real_value: True
2023-01-03 22:00: Argument rnn_units: 16
2023-01-03 22:00: Argument scaleParameter: 1.0
2023-01-03 22:00: Argument seed: 1992
2023-01-03 22:00: Argument sizeBorder: 1
2023-01-03 22:00: Argument static_features: ['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density']
2023-01-03 22:00: Argument teacher_forcing: False
2023-01-03 22:00: Argument weight_decay: 0.0
2023-01-03 22:00: Argument window_len: 10
2023-01-03 22:01: Train Epoch 1: 3/634 Loss: 0.328895
2023-01-03 22:01: Train Epoch 1: 7/634 Loss: 0.341583
2023-01-03 22:01: Train Epoch 1: 11/634 Loss: 0.290144
2023-01-03 22:01: Train Epoch 1: 15/634 Loss: 0.302690
2023-01-03 22:02: Train Epoch 1: 19/634 Loss: 0.304672
2023-01-03 22:02: Train Epoch 1: 23/634 Loss: 0.284132
2023-01-03 22:02: Train Epoch 1: 27/634 Loss: 0.267136
2023-01-03 22:02: Train Epoch 1: 31/634 Loss: 0.262896
2023-01-03 22:03: Train Epoch 1: 35/634 Loss: 0.273788
2023-01-03 22:03: Train Epoch 1: 39/634 Loss: 0.271457
2023-01-03 22:03: Train Epoch 1: 43/634 Loss: 0.266384
2023-01-03 22:03: Train Epoch 1: 47/634 Loss: 0.252136
2023-01-03 22:03: Train Epoch 1: 51/634 Loss: 0.256350
2023-01-03 22:04: Train Epoch 1: 55/634 Loss: 0.256814
2023-01-03 22:04: Train Epoch 1: 59/634 Loss: 0.268181
2023-01-03 22:04: Train Epoch 1: 63/634 Loss: 0.261270
2023-01-03 22:04: Train Epoch 1: 67/634 Loss: 0.243510
2023-01-03 22:05: Train Epoch 1: 71/634 Loss: 0.246679
2023-01-03 22:05: Train Epoch 1: 75/634 Loss: 0.260301
2023-01-03 22:05: Train Epoch 1: 79/634 Loss: 0.249884
2023-01-03 22:05: Train Epoch 1: 83/634 Loss: 0.215528
2023-01-03 22:05: Train Epoch 1: 87/634 Loss: 0.247817
2023-01-03 22:06: Train Epoch 1: 91/634 Loss: 0.232614
2023-01-03 22:06: Train Epoch 1: 95/634 Loss: 0.236336
2023-01-03 22:06: Train Epoch 1: 99/634 Loss: 0.244314
2023-01-03 22:06: Train Epoch 1: 103/634 Loss: 0.267937
2023-01-03 22:07: Train Epoch 1: 107/634 Loss: 0.246828
2023-01-03 22:07: Train Epoch 1: 111/634 Loss: 0.236443
2023-01-03 22:07: Train Epoch 1: 115/634 Loss: 0.228506
2023-01-03 22:07: Train Epoch 1: 119/634 Loss: 0.234875
2023-01-03 22:08: Train Epoch 1: 123/634 Loss: 0.246592
2023-01-03 22:08: Train Epoch 1: 127/634 Loss: 0.253908
2023-01-03 22:08: Train Epoch 1: 131/634 Loss: 0.225627
2023-01-03 22:08: Train Epoch 1: 135/634 Loss: 0.224963
2023-01-03 22:08: Train Epoch 1: 139/634 Loss: 0.240152
2023-01-03 22:09: Train Epoch 1: 143/634 Loss: 0.234748
2023-01-03 22:09: Train Epoch 1: 147/634 Loss: 0.237658
2023-01-03 22:09: Train Epoch 1: 151/634 Loss: 0.222989
2023-01-03 22:09: Train Epoch 1: 155/634 Loss: 0.228900
2023-01-03 22:10: Train Epoch 1: 159/634 Loss: 0.212437
2023-01-03 22:10: Train Epoch 1: 163/634 Loss: 0.208608
2023-01-03 22:10: Train Epoch 1: 167/634 Loss: 0.255620
2023-01-03 22:10: Train Epoch 1: 171/634 Loss: 0.214569
2023-01-03 22:11: Train Epoch 1: 175/634 Loss: 0.264078
2023-01-03 22:11: Train Epoch 1: 179/634 Loss: 0.235712
2023-01-03 22:11: Train Epoch 1: 183/634 Loss: 0.214583
2023-01-03 22:11: Train Epoch 1: 187/634 Loss: 0.209732
2023-01-03 22:11: Train Epoch 1: 191/634 Loss: 0.231441
2023-01-03 22:12: Train Epoch 1: 195/634 Loss: 0.257418
2023-01-03 22:12: Train Epoch 1: 199/634 Loss: 0.251469
2023-01-03 22:12: Train Epoch 1: 203/634 Loss: 0.217914
2023-01-03 22:12: Train Epoch 1: 207/634 Loss: 0.213085
2023-01-03 22:13: Train Epoch 1: 211/634 Loss: 0.214335
2023-01-03 22:13: Train Epoch 1: 215/634 Loss: 0.228611
2023-01-03 22:13: Train Epoch 1: 219/634 Loss: 0.215522
2023-01-03 22:13: Train Epoch 1: 223/634 Loss: 0.212044
2023-01-03 22:13: Train Epoch 1: 227/634 Loss: 0.240135
2023-01-03 22:14: Train Epoch 1: 231/634 Loss: 0.209871
2023-01-03 22:14: Train Epoch 1: 235/634 Loss: 0.213189
2023-01-03 22:14: Train Epoch 1: 239/634 Loss: 0.221555
2023-01-03 22:14: Train Epoch 1: 243/634 Loss: 0.208106
2023-01-03 22:15: Train Epoch 1: 247/634 Loss: 0.228400
2023-01-03 22:15: Train Epoch 1: 251/634 Loss: 0.219402
2023-01-03 22:15: Train Epoch 1: 255/634 Loss: 0.184502
2023-01-03 22:15: Train Epoch 1: 259/634 Loss: 0.219439
2023-01-03 22:16: Train Epoch 1: 263/634 Loss: 0.217973
2023-01-03 22:16: Train Epoch 1: 267/634 Loss: 0.232658
2023-01-03 22:16: Train Epoch 1: 271/634 Loss: 0.234654
2023-01-03 22:16: Train Epoch 1: 275/634 Loss: 0.205969
2023-01-03 22:16: Train Epoch 1: 279/634 Loss: 0.216225
2023-01-03 22:17: Train Epoch 1: 283/634 Loss: 0.203675
2023-01-03 22:17: Train Epoch 1: 287/634 Loss: 0.213839
2023-01-03 22:17: Train Epoch 1: 291/634 Loss: 0.198244
2023-01-03 22:17: Train Epoch 1: 295/634 Loss: 0.204615
2023-01-03 22:18: Train Epoch 1: 299/634 Loss: 0.207778
2023-01-03 22:18: Train Epoch 1: 303/634 Loss: 0.228386
2023-01-03 22:18: Train Epoch 1: 307/634 Loss: 0.228501
2023-01-03 22:18: Train Epoch 1: 311/634 Loss: 0.224812
2023-01-03 22:18: Train Epoch 1: 315/634 Loss: 0.236434
2023-01-03 22:19: Train Epoch 1: 319/634 Loss: 0.216854
2023-01-03 22:19: Train Epoch 1: 323/634 Loss: 0.225920
2023-01-03 22:19: Train Epoch 1: 327/634 Loss: 0.192745
2023-01-03 22:19: Train Epoch 1: 331/634 Loss: 0.197958
2023-01-03 22:20: Train Epoch 1: 335/634 Loss: 0.201589
2023-01-03 22:20: Train Epoch 1: 339/634 Loss: 0.197638
2023-01-03 22:20: Train Epoch 1: 343/634 Loss: 0.214634
2023-01-03 22:20: Train Epoch 1: 347/634 Loss: 0.227017
2023-01-03 22:21: Train Epoch 1: 351/634 Loss: 0.223192
2023-01-03 22:21: Train Epoch 1: 355/634 Loss: 0.228247
2023-01-03 22:21: Train Epoch 1: 359/634 Loss: 0.207734
2023-01-03 22:21: Train Epoch 1: 363/634 Loss: 0.205984
2023-01-03 22:21: Train Epoch 1: 367/634 Loss: 0.204950
