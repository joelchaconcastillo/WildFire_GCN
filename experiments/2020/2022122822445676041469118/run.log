2022-12-28 22:44: log dir: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122822445676041469118
2022-12-28 22:44: Experiment log path in: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122822445676041469118
2022-12-28 22:44: Argument: Namespace(batch_size=256, clc='vec', cuda=True, dataset='2020', dataset_root='/home/joel.chacon/tmp/datasets_grl/', debug=False, default_graph=True, device='cpu', dynamic_features=['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh'], early_stop=True, early_stop_patience=8, embed_dim=128, epochs=30, grad_norm=False, horizon=1, input_dim=25, lag=10, link_len=2, log_dir='/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122822445676041469118', log_step=1, loss_func='nllloss', lr_decay=True, lr_decay_rate=0.1, lr_decay_step='15, 20', lr_init=0.0005, max_grad_norm=5, minbatch_size=256, mode='train', model='fire_GCN', nan_fill=0.5, num_layers=1, num_nodes=625, num_workers=20, output_dim=2, patch_height=25, patch_width=25, persistent_workers=True, pin_memory=True, plot=False, positive_weight=0.5, prefetch_factor=2, real_value=True, rnn_units=16, seed=10000, static_features=['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density'], teacher_forcing=False, weight_decay=0.0, window_len=10)
2022-12-28 22:44: Argument batch_size: 256
2022-12-28 22:44: Argument clc: 'vec'
2022-12-28 22:44: Argument cuda: True
2022-12-28 22:44: Argument dataset: '2020'
2022-12-28 22:44: Argument dataset_root: '/home/joel.chacon/tmp/datasets_grl/'
2022-12-28 22:44: Argument debug: False
2022-12-28 22:44: Argument default_graph: True
2022-12-28 22:44: Argument device: 'cpu'
2022-12-28 22:44: Argument dynamic_features: ['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh']
2022-12-28 22:44: Argument early_stop: True
2022-12-28 22:44: Argument early_stop_patience: 8
2022-12-28 22:44: Argument embed_dim: 128
2022-12-28 22:44: Argument epochs: 30
2022-12-28 22:44: Argument grad_norm: False
2022-12-28 22:44: Argument horizon: 1
2022-12-28 22:44: Argument input_dim: 25
2022-12-28 22:44: Argument lag: 10
2022-12-28 22:44: Argument link_len: 2
2022-12-28 22:44: Argument log_dir: '/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122822445676041469118'
2022-12-28 22:44: Argument log_step: 1
2022-12-28 22:44: Argument loss_func: 'nllloss'
2022-12-28 22:44: Argument lr_decay: True
2022-12-28 22:44: Argument lr_decay_rate: 0.1
2022-12-28 22:44: Argument lr_decay_step: '15, 20'
2022-12-28 22:44: Argument lr_init: 0.0005
2022-12-28 22:44: Argument max_grad_norm: 5
2022-12-28 22:44: Argument minbatch_size: 256
2022-12-28 22:44: Argument mode: 'train'
2022-12-28 22:44: Argument model: 'fire_GCN'
2022-12-28 22:44: Argument nan_fill: 0.5
2022-12-28 22:44: Argument num_layers: 1
2022-12-28 22:44: Argument num_nodes: 625
2022-12-28 22:44: Argument num_workers: 20
2022-12-28 22:44: Argument output_dim: 2
2022-12-28 22:44: Argument patch_height: 25
2022-12-28 22:44: Argument patch_width: 25
2022-12-28 22:44: Argument persistent_workers: True
2022-12-28 22:44: Argument pin_memory: True
2022-12-28 22:44: Argument plot: False
2022-12-28 22:44: Argument positive_weight: 0.5
2022-12-28 22:44: Argument prefetch_factor: 2
2022-12-28 22:44: Argument real_value: True
2022-12-28 22:44: Argument rnn_units: 16
2022-12-28 22:44: Argument seed: 10000
2022-12-28 22:44: Argument static_features: ['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density']
2022-12-28 22:44: Argument teacher_forcing: False
2022-12-28 22:44: Argument weight_decay: 0.0
2022-12-28 22:44: Argument window_len: 10
2022-12-28 22:45: Train Epoch 1: 0/61 Loss: 2.431704
2022-12-28 22:45: Train Epoch 1: 1/61 Loss: 1.424968
2022-12-28 22:45: Train Epoch 1: 2/61 Loss: 1.055997
2022-12-28 22:45: Train Epoch 1: 3/61 Loss: 1.340065
2022-12-28 22:45: Train Epoch 1: 4/61 Loss: 1.203026
2022-12-28 22:45: Train Epoch 1: 5/61 Loss: 0.908692
2022-12-28 22:45: Train Epoch 1: 6/61 Loss: 0.649870
2022-12-28 22:45: Train Epoch 1: 7/61 Loss: 0.755478
2022-12-28 22:45: Train Epoch 1: 8/61 Loss: 0.919882
2022-12-28 22:45: Train Epoch 1: 9/61 Loss: 0.808776
2022-12-28 22:46: Train Epoch 1: 10/61 Loss: 0.660949
2022-12-28 22:46: Train Epoch 1: 11/61 Loss: 0.637967
2022-12-28 22:46: Train Epoch 1: 12/61 Loss: 0.757106
2022-12-28 22:46: Train Epoch 1: 13/61 Loss: 0.798311
2022-12-28 22:46: Train Epoch 1: 14/61 Loss: 0.754617
2022-12-28 22:46: Train Epoch 1: 15/61 Loss: 0.618850
2022-12-28 22:46: Train Epoch 1: 16/61 Loss: 0.690893
2022-12-28 22:46: Train Epoch 1: 17/61 Loss: 0.661784
2022-12-28 22:46: Train Epoch 1: 18/61 Loss: 0.676161
2022-12-28 22:46: Train Epoch 1: 19/61 Loss: 0.645081
2022-12-28 22:47: Train Epoch 1: 20/61 Loss: 0.663597
2022-12-28 22:47: Train Epoch 1: 21/61 Loss: 0.680435
2022-12-28 22:47: Train Epoch 1: 22/61 Loss: 0.687212
2022-12-28 22:47: Train Epoch 1: 23/61 Loss: 0.684892
2022-12-28 22:47: Train Epoch 1: 24/61 Loss: 0.676773
2022-12-28 22:47: Train Epoch 1: 25/61 Loss: 0.664274
2022-12-28 22:47: Train Epoch 1: 26/61 Loss: 0.648607
2022-12-28 22:47: Train Epoch 1: 27/61 Loss: 0.649501
2022-12-28 22:47: Train Epoch 1: 28/61 Loss: 0.614530
2022-12-28 22:47: Train Epoch 1: 29/61 Loss: 0.666204
2022-12-28 22:48: Train Epoch 1: 30/61 Loss: 0.623618
2022-12-28 22:48: Train Epoch 1: 31/61 Loss: 0.616110
2022-12-28 22:48: Train Epoch 1: 32/61 Loss: 0.617887
2022-12-28 22:48: Train Epoch 1: 33/61 Loss: 0.676129
2022-12-28 22:48: Train Epoch 1: 34/61 Loss: 0.589423
2022-12-28 22:48: Train Epoch 1: 35/61 Loss: 0.629971
2022-12-28 22:48: Train Epoch 1: 36/61 Loss: 0.644078
2022-12-28 22:48: Train Epoch 1: 37/61 Loss: 0.587194
2022-12-28 22:48: Train Epoch 1: 38/61 Loss: 0.618647
2022-12-28 22:48: Train Epoch 1: 39/61 Loss: 0.639624
2022-12-28 22:48: Train Epoch 1: 40/61 Loss: 0.658735
2022-12-28 22:49: Train Epoch 1: 41/61 Loss: 0.611820
2022-12-28 22:49: Train Epoch 1: 42/61 Loss: 0.632146
2022-12-28 22:49: Train Epoch 1: 43/61 Loss: 0.620947
2022-12-28 22:49: Train Epoch 1: 44/61 Loss: 0.621848
2022-12-28 22:49: Train Epoch 1: 45/61 Loss: 0.631577
2022-12-28 22:49: Train Epoch 1: 46/61 Loss: 0.656283
2022-12-28 22:49: Train Epoch 1: 47/61 Loss: 0.628182
2022-12-28 22:49: Train Epoch 1: 48/61 Loss: 0.631494
2022-12-28 22:49: Train Epoch 1: 49/61 Loss: 0.632077
2022-12-28 22:49: Train Epoch 1: 50/61 Loss: 0.613693
2022-12-28 22:49: Train Epoch 1: 51/61 Loss: 0.612159
2022-12-28 22:49: Train Epoch 1: 52/61 Loss: 0.629109
2022-12-28 22:49: Train Epoch 1: 53/61 Loss: 0.613590
2022-12-28 22:50: Train Epoch 1: 54/61 Loss: 0.586247
2022-12-28 22:50: Train Epoch 1: 55/61 Loss: 0.662495
2022-12-28 22:50: Train Epoch 1: 56/61 Loss: 0.639930
2022-12-28 22:50: Train Epoch 1: 57/61 Loss: 0.660418
2022-12-28 22:50: Train Epoch 1: 58/61 Loss: 0.599741
2022-12-28 22:50: Train Epoch 1: 59/61 Loss: 0.613111
2022-12-28 22:50: Train Epoch 1: 60/61 Loss: 0.622219
2022-12-28 22:50: **********Train Epoch 1: averaged Loss: 0.728798 
2022-12-28 22:50: 
Epoch time elapsed: 335.8453619480133

2022-12-28 22:51: 
 metrics validation: {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1300, 'AUC': 0.7440171597633136, 'AUCPR': 0.5764855143345011, 'TP': 0, 'FP': 0, 'TN': 2600, 'FN': 1300} 

2022-12-28 22:51: **********Val Epoch 1: average Loss: 0.628759
2022-12-28 22:51: *********************************Current best model saved!
2022-12-28 22:51: 
 Testing metrics {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1228, 'AUC': 0.8350983830067162, 'AUCPR': 0.7093018664403516, 'TP': 0, 'FP': 0, 'TN': 2456, 'FN': 1228} 

2022-12-28 22:51: Train Epoch 2: 0/61 Loss: 0.645805
2022-12-28 22:51: Train Epoch 2: 1/61 Loss: 0.585234
2022-12-28 22:52: Train Epoch 2: 2/61 Loss: 0.650949
2022-12-28 22:52: Train Epoch 2: 3/61 Loss: 0.617062
2022-12-28 22:52: Train Epoch 2: 4/61 Loss: 0.636664
2022-12-28 22:52: Train Epoch 2: 5/61 Loss: 0.600657
2022-12-28 22:52: Train Epoch 2: 6/61 Loss: 0.593830
2022-12-28 22:52: Train Epoch 2: 7/61 Loss: 0.662195
2022-12-28 22:52: Train Epoch 2: 8/61 Loss: 0.609065
2022-12-28 22:52: Train Epoch 2: 9/61 Loss: 0.609042
2022-12-28 22:52: Train Epoch 2: 10/61 Loss: 0.617596
2022-12-28 22:53: Train Epoch 2: 11/61 Loss: 0.599041
2022-12-28 22:53: Train Epoch 2: 12/61 Loss: 0.649633
2022-12-28 22:53: Train Epoch 2: 13/61 Loss: 0.588153
2022-12-28 22:53: Train Epoch 2: 14/61 Loss: 0.598439
2022-12-28 22:53: Train Epoch 2: 15/61 Loss: 0.639151
2022-12-28 22:53: Train Epoch 2: 16/61 Loss: 0.638681
2022-12-28 22:53: Train Epoch 2: 17/61 Loss: 0.637011
2022-12-28 22:53: Train Epoch 2: 18/61 Loss: 0.618038
2022-12-28 22:53: Train Epoch 2: 19/61 Loss: 0.639290
2022-12-28 22:54: Train Epoch 2: 20/61 Loss: 0.602204
2022-12-28 22:54: Train Epoch 2: 21/61 Loss: 0.624768
2022-12-28 22:54: Train Epoch 2: 22/61 Loss: 0.621998
2022-12-28 22:54: Train Epoch 2: 23/61 Loss: 0.628150
2022-12-28 22:54: Train Epoch 2: 24/61 Loss: 0.621137
2022-12-28 22:54: Train Epoch 2: 25/61 Loss: 0.610812
2022-12-28 22:54: Train Epoch 2: 26/61 Loss: 0.622728
2022-12-28 22:54: Train Epoch 2: 27/61 Loss: 0.601087
2022-12-28 22:54: Train Epoch 2: 28/61 Loss: 0.603118
2022-12-28 22:54: Train Epoch 2: 29/61 Loss: 0.632773
2022-12-28 22:54: Train Epoch 2: 30/61 Loss: 0.610434
2022-12-28 22:54: Train Epoch 2: 31/61 Loss: 0.588965
2022-12-28 22:55: Train Epoch 2: 32/61 Loss: 0.662573
2022-12-28 22:55: Train Epoch 2: 33/61 Loss: 0.607983
2022-12-28 22:55: Train Epoch 2: 34/61 Loss: 0.617030
2022-12-28 22:55: Train Epoch 2: 35/61 Loss: 0.580768
2022-12-28 22:55: Train Epoch 2: 36/61 Loss: 0.573900
2022-12-28 22:55: Train Epoch 2: 37/61 Loss: 0.590707
2022-12-28 22:55: Train Epoch 2: 38/61 Loss: 0.594065
2022-12-28 22:55: Train Epoch 2: 39/61 Loss: 0.605436
2022-12-28 22:55: Train Epoch 2: 40/61 Loss: 0.607430
2022-12-28 22:55: Train Epoch 2: 41/61 Loss: 0.596708
2022-12-28 22:55: Train Epoch 2: 42/61 Loss: 0.578862
2022-12-28 22:56: Train Epoch 2: 43/61 Loss: 0.622955
2022-12-28 22:56: Train Epoch 2: 44/61 Loss: 0.626250
2022-12-28 22:56: Train Epoch 2: 45/61 Loss: 0.609057
2022-12-28 22:56: Train Epoch 2: 46/61 Loss: 0.582627
2022-12-28 22:56: Train Epoch 2: 47/61 Loss: 0.594362
2022-12-28 22:56: Train Epoch 2: 48/61 Loss: 0.587076
2022-12-28 22:56: Train Epoch 2: 49/61 Loss: 0.601792
2022-12-28 22:56: Train Epoch 2: 50/61 Loss: 0.615862
2022-12-28 22:56: Train Epoch 2: 51/61 Loss: 0.575666
2022-12-28 22:56: Train Epoch 2: 52/61 Loss: 0.587426
2022-12-28 22:57: Train Epoch 2: 53/61 Loss: 0.601391
2022-12-28 22:57: Train Epoch 2: 54/61 Loss: 0.590792
2022-12-28 22:57: Train Epoch 2: 55/61 Loss: 0.569121
2022-12-28 22:57: Train Epoch 2: 56/61 Loss: 0.614418
2022-12-28 22:57: Train Epoch 2: 57/61 Loss: 0.604836
2022-12-28 22:57: Train Epoch 2: 58/61 Loss: 0.612537
2022-12-28 22:57: Train Epoch 2: 59/61 Loss: 0.603170
2022-12-28 22:57: Train Epoch 2: 60/61 Loss: 0.636268
2022-12-28 22:57: **********Train Epoch 2: averaged Loss: 0.610734 
2022-12-28 22:57: 
Epoch time elapsed: 358.33699560165405

