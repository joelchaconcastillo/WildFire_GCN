/home/joel.chacon/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2022-12-28 00:10: log dir: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122800103490462288058
2022-12-28 00:10: Experiment log path in: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122800103490462288058
2022-12-28 00:10: Argument: Namespace(batch_size=256, clc='vec', cuda=True, dataset='2020', dataset_root='/home/joel.chacon/tmp/datasets_grl/', debug=False, default_graph=True, device='cpu', dynamic_features=['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh'], early_stop=True, early_stop_patience=8, embed_dim=64, epochs=30, grad_norm=False, horizon=1, input_dim=25, lag=10, link_len=2, log_dir='/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122800103490462288058', log_step=1, loss_func='nllloss', lr_decay=True, lr_decay_rate=0.1, lr_decay_step='15, 20', lr_init=0.0005, max_grad_norm=5, mode='train', model='fire_GCN', nan_fill=0.5, num_layers=1, num_nodes=625, num_workers=20, output_dim=2, patch_height=25, patch_width=25, persistent_workers=True, pin_memory=True, plot=False, positive_weight=0.5, prefetch_factor=2, real_value=True, rnn_units=16, seed=10000, static_features=['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density'], teacher_forcing=False, weight_decay=0.0, window_len=10)
2022-12-28 00:10: Argument batch_size: 256
2022-12-28 00:10: Argument clc: 'vec'
2022-12-28 00:10: Argument cuda: True
2022-12-28 00:10: Argument dataset: '2020'
2022-12-28 00:10: Argument dataset_root: '/home/joel.chacon/tmp/datasets_grl/'
2022-12-28 00:10: Argument debug: False
2022-12-28 00:10: Argument default_graph: True
2022-12-28 00:10: Argument device: 'cpu'
2022-12-28 00:10: Argument dynamic_features: ['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh']
2022-12-28 00:10: Argument early_stop: True
2022-12-28 00:10: Argument early_stop_patience: 8
2022-12-28 00:10: Argument embed_dim: 64
2022-12-28 00:10: Argument epochs: 30
2022-12-28 00:10: Argument grad_norm: False
2022-12-28 00:10: Argument horizon: 1
2022-12-28 00:10: Argument input_dim: 25
2022-12-28 00:10: Argument lag: 10
2022-12-28 00:10: Argument link_len: 2
2022-12-28 00:10: Argument log_dir: '/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122800103490462288058'
2022-12-28 00:10: Argument log_step: 1
2022-12-28 00:10: Argument loss_func: 'nllloss'
2022-12-28 00:10: Argument lr_decay: True
2022-12-28 00:10: Argument lr_decay_rate: 0.1
2022-12-28 00:10: Argument lr_decay_step: '15, 20'
2022-12-28 00:10: Argument lr_init: 0.0005
2022-12-28 00:10: Argument max_grad_norm: 5
2022-12-28 00:10: Argument mode: 'train'
2022-12-28 00:10: Argument model: 'fire_GCN'
2022-12-28 00:10: Argument nan_fill: 0.5
2022-12-28 00:10: Argument num_layers: 1
2022-12-28 00:10: Argument num_nodes: 625
2022-12-28 00:10: Argument num_workers: 20
2022-12-28 00:10: Argument output_dim: 2
2022-12-28 00:10: Argument patch_height: 25
2022-12-28 00:10: Argument patch_width: 25
2022-12-28 00:10: Argument persistent_workers: True
2022-12-28 00:10: Argument pin_memory: True
2022-12-28 00:10: Argument plot: False
2022-12-28 00:10: Argument positive_weight: 0.5
2022-12-28 00:10: Argument prefetch_factor: 2
2022-12-28 00:10: Argument real_value: True
2022-12-28 00:10: Argument rnn_units: 16
2022-12-28 00:10: Argument seed: 10000
2022-12-28 00:10: Argument static_features: ['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density']
2022-12-28 00:10: Argument teacher_forcing: False
2022-12-28 00:10: Argument weight_decay: 0.0
2022-12-28 00:10: Argument window_len: 10
++++++++++++++
2020_fire_GCN.conf
++++++++++++++
*****************Model Parameter*****************
node_embeddings torch.Size([625, 64]) True
ln1.weight torch.Size([25]) True
ln1.bias torch.Size([25]) True
encoder.cell_list.0.gate.weights_pool torch.Size([64, 2, 41, 16]) True
encoder.cell_list.0.gate.weights_window torch.Size([64, 1, 16]) True
encoder.cell_list.0.gate.bias_pool torch.Size([64, 32]) True
encoder.cell_list.0.gate.T torch.Size([10]) True
encoder.cell_list.0.update.weights_pool torch.Size([64, 2, 41, 8]) True
encoder.cell_list.0.update.weights_window torch.Size([64, 1, 8]) True
encoder.cell_list.0.update.bias_pool torch.Size([64, 16]) True
encoder.cell_list.0.update.T torch.Size([10]) True
end_conv.weight torch.Size([2, 1, 625, 16]) True
end_conv.bias torch.Size([2]) True
Total params num: 190632
*****************Finish Parameter****************
Positives: 13518 / Negatives: 27036
Dataset length 40554
Positives: 1300 / Negatives: 2600
Dataset length 3900
Positives: 1228 / Negatives: 2456
Dataset length 3684
Applying learning rate decay.
Creat Log File in:  /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122800103490462288058/run.log
2022-12-28 00:11: Train Epoch 1: 0/159 Loss: 0.661263
2022-12-28 00:12: Train Epoch 1: 1/159 Loss: 1.651548
2022-12-28 00:13: Train Epoch 1: 2/159 Loss: 0.780679
2022-12-28 00:14: Train Epoch 1: 3/159 Loss: 1.165932
2022-12-28 00:15: Train Epoch 1: 4/159 Loss: 1.013679
2022-12-28 00:16: Train Epoch 1: 5/159 Loss: 0.655059
2022-12-28 00:17: Train Epoch 1: 6/159 Loss: 0.740150
2022-12-28 00:17: Train Epoch 1: 7/159 Loss: 0.894173
2022-12-28 00:18: Train Epoch 1: 8/159 Loss: 0.912000
2022-12-28 00:19: Train Epoch 1: 9/159 Loss: 0.689404
2022-12-28 00:20: Train Epoch 1: 10/159 Loss: 0.633931
2022-12-28 00:21: Train Epoch 1: 11/159 Loss: 0.752281
2022-12-28 00:22: Train Epoch 1: 12/159 Loss: 0.833167
2022-12-28 00:23: Train Epoch 1: 13/159 Loss: 0.668408
2022-12-28 00:24: Train Epoch 1: 14/159 Loss: 0.624613
2022-12-28 00:25: Train Epoch 1: 15/159 Loss: 0.665532
2022-12-28 00:25: Train Epoch 1: 16/159 Loss: 0.685049
2022-12-28 00:26: Train Epoch 1: 17/159 Loss: 0.746794
2022-12-28 00:27: Train Epoch 1: 18/159 Loss: 0.593587
2022-12-28 00:28: Train Epoch 1: 19/159 Loss: 0.619676
2022-12-28 00:29: Train Epoch 1: 20/159 Loss: 0.649812
2022-12-28 00:30: Train Epoch 1: 21/159 Loss: 0.706452
2022-12-28 00:31: Train Epoch 1: 22/159 Loss: 0.647471
2022-12-28 00:32: Train Epoch 1: 23/159 Loss: 0.606995
2022-12-28 00:32: Train Epoch 1: 24/159 Loss: 0.607985
2022-12-28 00:33: Train Epoch 1: 25/159 Loss: 0.624846
2022-12-28 00:34: Train Epoch 1: 26/159 Loss: 0.550145
2022-12-28 00:35: Train Epoch 1: 27/159 Loss: 0.585348
2022-12-28 00:36: Train Epoch 1: 28/159 Loss: 0.527198
2022-12-28 00:37: Train Epoch 1: 29/159 Loss: 0.568252
2022-12-28 00:38: Train Epoch 1: 30/159 Loss: 0.558177
2022-12-28 00:39: Train Epoch 1: 31/159 Loss: 0.558002
2022-12-28 00:39: Train Epoch 1: 32/159 Loss: 0.568917
2022-12-28 00:40: Train Epoch 1: 33/159 Loss: 0.523656
2022-12-28 00:41: Train Epoch 1: 34/159 Loss: 0.510577
2022-12-28 00:42: Train Epoch 1: 35/159 Loss: 0.549368
2022-12-28 00:43: Train Epoch 1: 36/159 Loss: 0.488583
2022-12-28 00:44: Train Epoch 1: 37/159 Loss: 0.498526
2022-12-28 00:45: Train Epoch 1: 38/159 Loss: 0.478228
2022-12-28 00:46: Train Epoch 1: 39/159 Loss: 0.464827
2022-12-28 00:46: Train Epoch 1: 40/159 Loss: 0.446156
2022-12-28 00:47: Train Epoch 1: 41/159 Loss: 0.493168
2022-12-28 00:48: Train Epoch 1: 42/159 Loss: 0.457505
2022-12-28 00:49: Train Epoch 1: 43/159 Loss: 0.472639
2022-12-28 00:50: Train Epoch 1: 44/159 Loss: 0.447432
2022-12-28 00:51: Train Epoch 1: 45/159 Loss: 0.416629
2022-12-28 00:52: Train Epoch 1: 46/159 Loss: 0.397023
2022-12-28 00:53: Train Epoch 1: 47/159 Loss: 0.447735
2022-12-28 00:53: Train Epoch 1: 48/159 Loss: 0.466508
2022-12-28 00:54: Train Epoch 1: 49/159 Loss: 0.403388
2022-12-28 00:55: Train Epoch 1: 50/159 Loss: 0.398178
2022-12-28 00:56: Train Epoch 1: 51/159 Loss: 0.386884
2022-12-28 00:57: Train Epoch 1: 52/159 Loss: 0.376970
2022-12-28 00:58: Train Epoch 1: 53/159 Loss: 0.389826
2022-12-28 00:59: Train Epoch 1: 54/159 Loss: 0.359373
2022-12-28 01:00: Train Epoch 1: 55/159 Loss: 0.355481
2022-12-28 01:01: Train Epoch 1: 56/159 Loss: 0.350438
2022-12-28 01:01: Train Epoch 1: 57/159 Loss: 0.357661
2022-12-28 01:02: Train Epoch 1: 58/159 Loss: 0.306093
2022-12-28 01:03: Train Epoch 1: 59/159 Loss: 0.365149
2022-12-28 01:04: Train Epoch 1: 60/159 Loss: 0.342808
2022-12-28 01:05: Train Epoch 1: 61/159 Loss: 0.341357
2022-12-28 01:06: Train Epoch 1: 62/159 Loss: 0.331970
2022-12-28 01:07: Train Epoch 1: 63/159 Loss: 0.350568
2022-12-28 01:08: Train Epoch 1: 64/159 Loss: 0.327988
2022-12-28 01:08: Train Epoch 1: 65/159 Loss: 0.334869
2022-12-28 01:09: Train Epoch 1: 66/159 Loss: 0.296758
2022-12-28 01:10: Train Epoch 1: 67/159 Loss: 0.326240
2022-12-28 01:11: Train Epoch 1: 68/159 Loss: 0.293736
2022-12-28 01:12: Train Epoch 1: 69/159 Loss: 0.393154
2022-12-28 01:13: Train Epoch 1: 70/159 Loss: 0.304357
2022-12-28 01:14: Train Epoch 1: 71/159 Loss: 0.273625
2022-12-28 01:15: Train Epoch 1: 72/159 Loss: 0.296962
2022-12-28 01:15: Train Epoch 1: 73/159 Loss: 0.288377
2022-12-28 01:16: Train Epoch 1: 74/159 Loss: 0.286078
2022-12-28 01:17: Train Epoch 1: 75/159 Loss: 0.253535
2022-12-28 01:18: Train Epoch 1: 76/159 Loss: 0.290159
2022-12-28 01:19: Train Epoch 1: 77/159 Loss: 0.283045
2022-12-28 01:20: Train Epoch 1: 78/159 Loss: 0.256007
2022-12-28 01:21: Train Epoch 1: 79/159 Loss: 0.348533
2022-12-28 01:22: Train Epoch 1: 80/159 Loss: 0.326547
2022-12-28 01:22: Train Epoch 1: 81/159 Loss: 0.288436
2022-12-28 01:23: Train Epoch 1: 82/159 Loss: 0.320982
2022-12-28 01:24: Train Epoch 1: 83/159 Loss: 0.321368
2022-12-28 01:25: Train Epoch 1: 84/159 Loss: 0.261126
2022-12-28 01:26: Train Epoch 1: 85/159 Loss: 0.295010
2022-12-28 01:27: Train Epoch 1: 86/159 Loss: 0.309167
2022-12-28 01:28: Train Epoch 1: 87/159 Loss: 0.266723
2022-12-28 01:29: Train Epoch 1: 88/159 Loss: 0.388978
2022-12-28 01:29: Train Epoch 1: 89/159 Loss: 0.302471
2022-12-28 01:30: Train Epoch 1: 90/159 Loss: 0.308425
2022-12-28 01:31: Train Epoch 1: 91/159 Loss: 0.301255
2022-12-28 01:32: Train Epoch 1: 92/159 Loss: 0.287699
2022-12-28 01:33: Train Epoch 1: 93/159 Loss: 0.352855
2022-12-28 01:34: Train Epoch 1: 94/159 Loss: 0.331557
2022-12-28 01:35: Train Epoch 1: 95/159 Loss: 0.329976
2022-12-28 01:36: Train Epoch 1: 96/159 Loss: 0.264938
2022-12-28 01:36: Train Epoch 1: 97/159 Loss: 0.261327
2022-12-28 01:37: Train Epoch 1: 98/159 Loss: 0.284453
2022-12-28 01:38: Train Epoch 1: 99/159 Loss: 0.289765
2022-12-28 01:39: Train Epoch 1: 100/159 Loss: 0.305135
2022-12-28 01:40: Train Epoch 1: 101/159 Loss: 0.275995
2022-12-28 01:41: Train Epoch 1: 102/159 Loss: 0.270812
2022-12-28 01:42: Train Epoch 1: 103/159 Loss: 0.243574
2022-12-28 01:43: Train Epoch 1: 104/159 Loss: 0.319146
2022-12-28 01:43: Train Epoch 1: 105/159 Loss: 0.246508
2022-12-28 01:44: Train Epoch 1: 106/159 Loss: 0.316399
2022-12-28 01:45: Train Epoch 1: 107/159 Loss: 0.282893
2022-12-28 01:46: Train Epoch 1: 108/159 Loss: 0.325798
2022-12-28 01:47: Train Epoch 1: 109/159 Loss: 0.328646
2022-12-28 01:48: Train Epoch 1: 110/159 Loss: 0.403126
2022-12-28 01:49: Train Epoch 1: 111/159 Loss: 0.284336
2022-12-28 01:49: Train Epoch 1: 112/159 Loss: 0.389018
2022-12-28 01:50: Train Epoch 1: 113/159 Loss: 0.273120
2022-12-28 01:51: Train Epoch 1: 114/159 Loss: 0.286276
2022-12-28 01:52: Train Epoch 1: 115/159 Loss: 0.238026
2022-12-28 01:53: Train Epoch 1: 116/159 Loss: 0.260549
2022-12-28 01:54: Train Epoch 1: 117/159 Loss: 0.307548
2022-12-28 01:55: Train Epoch 1: 118/159 Loss: 0.282600
2022-12-28 01:56: Train Epoch 1: 119/159 Loss: 0.283091
2022-12-28 01:57: Train Epoch 1: 120/159 Loss: 0.321506
2022-12-28 01:57: Train Epoch 1: 121/159 Loss: 0.242944
2022-12-28 01:58: Train Epoch 1: 122/159 Loss: 0.300888
2022-12-28 01:59: Train Epoch 1: 123/159 Loss: 0.253877
2022-12-28 02:00: Train Epoch 1: 124/159 Loss: 0.320562
2022-12-28 02:01: Train Epoch 1: 125/159 Loss: 0.367660
2022-12-28 02:02: Train Epoch 1: 126/159 Loss: 0.196260
2022-12-28 02:03: Train Epoch 1: 127/159 Loss: 0.267672
2022-12-28 02:04: Train Epoch 1: 128/159 Loss: 0.289617
2022-12-28 02:05: Train Epoch 1: 129/159 Loss: 0.316315
2022-12-28 02:05: Train Epoch 1: 130/159 Loss: 0.237679
2022-12-28 02:06: Train Epoch 1: 131/159 Loss: 0.273007
2022-12-28 02:07: Train Epoch 1: 132/159 Loss: 0.269497
2022-12-28 02:08: Train Epoch 1: 133/159 Loss: 0.286594
2022-12-28 02:09: Train Epoch 1: 134/159 Loss: 0.291841
2022-12-28 02:10: Train Epoch 1: 135/159 Loss: 0.283013
2022-12-28 02:11: Train Epoch 1: 136/159 Loss: 0.286351
2022-12-28 02:11: Train Epoch 1: 137/159 Loss: 0.284473
2022-12-28 02:12: Train Epoch 1: 138/159 Loss: 0.248381
2022-12-28 02:13: Train Epoch 1: 139/159 Loss: 0.260888
2022-12-28 02:14: Train Epoch 1: 140/159 Loss: 0.351292
2022-12-28 02:15: Train Epoch 1: 141/159 Loss: 0.279485
2022-12-28 02:16: Train Epoch 1: 142/159 Loss: 0.254523
2022-12-28 02:17: Train Epoch 1: 143/159 Loss: 0.300236
2022-12-28 02:17: Train Epoch 1: 144/159 Loss: 0.252418
2022-12-28 02:18: Train Epoch 1: 145/159 Loss: 0.254828
2022-12-28 02:19: Train Epoch 1: 146/159 Loss: 0.307731
2022-12-28 02:20: Train Epoch 1: 147/159 Loss: 0.333731
2022-12-28 02:21: Train Epoch 1: 148/159 Loss: 0.261746
2022-12-28 02:22: Train Epoch 1: 149/159 Loss: 0.364456
2022-12-28 02:23: Train Epoch 1: 150/159 Loss: 0.269771
2022-12-28 02:23: Train Epoch 1: 151/159 Loss: 0.240462
2022-12-28 02:24: Train Epoch 1: 152/159 Loss: 0.279593
2022-12-28 02:25: Train Epoch 1: 153/159 Loss: 0.246403
2022-12-28 02:26: Train Epoch 1: 154/159 Loss: 0.380212
2022-12-28 02:27: Train Epoch 1: 155/159 Loss: 0.305917
2022-12-28 02:28: Train Epoch 1: 156/159 Loss: 0.299210
2022-12-28 02:28: Train Epoch 1: 157/159 Loss: 0.262617
2022-12-28 02:29: Train Epoch 1: 158/159 Loss: 0.362260
2022-12-28 02:29: **********Train Epoch 1: averaged Loss: 0.407523 
2022-12-28 02:29: 
Epoch time elapsed: 8320.88488817215

2022-12-28 02:33: 
 metrics validation: {'precision': 0.7086092715231788, 'recall': 0.7407692307692307, 'f1-score': 0.724332455810455, 'support': 1300, 'AUC': 0.8499721893491125, 'AUCPR': 0.7640607261363919, 'TP': 963, 'FP': 396, 'TN': 2204, 'FN': 337} 

2022-12-28 02:33: **********Val Epoch 1: average Loss: 0.508160
2022-12-28 02:33: *********************************Current best model saved!
/home/joel.chacon/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2022-12-28 02:37: 
 Testing metrics {'precision': 0.7452596867271228, 'recall': 0.7361563517915309, 'f1-score': 0.7406800491601802, 'support': 1228, 'AUC': 0.8712907431378583, 'AUCPR': 0.7919405663011803, 'TP': 904, 'FP': 309, 'TN': 2147, 'FN': 324} 

2022-12-28 02:38: Train Epoch 2: 0/159 Loss: 0.258086
2022-12-28 02:39: Train Epoch 2: 1/159 Loss: 0.302578
2022-12-28 02:40: Train Epoch 2: 2/159 Loss: 0.394537
2022-12-28 02:41: Train Epoch 2: 3/159 Loss: 0.331474
2022-12-28 02:42: Train Epoch 2: 4/159 Loss: 0.297215
2022-12-28 02:43: Train Epoch 2: 5/159 Loss: 0.239547
2022-12-28 02:44: Train Epoch 2: 6/159 Loss: 0.258736
2022-12-28 02:45: Train Epoch 2: 7/159 Loss: 0.321145
2022-12-28 02:46: Train Epoch 2: 8/159 Loss: 0.340130
2022-12-28 02:46: Train Epoch 2: 9/159 Loss: 0.288984
2022-12-28 02:47: Train Epoch 2: 10/159 Loss: 0.355761
2022-12-28 02:48: Train Epoch 2: 11/159 Loss: 0.328171
2022-12-28 02:49: Train Epoch 2: 12/159 Loss: 0.265072
2022-12-28 02:50: Train Epoch 2: 13/159 Loss: 0.313588
2022-12-28 02:51: Train Epoch 2: 14/159 Loss: 0.284859
2022-12-28 02:52: Train Epoch 2: 15/159 Loss: 0.336896
2022-12-28 02:53: Train Epoch 2: 16/159 Loss: 0.279449
2022-12-28 02:53: Train Epoch 2: 17/159 Loss: 0.361539
2022-12-28 02:54: Train Epoch 2: 18/159 Loss: 0.213254
2022-12-28 02:55: Train Epoch 2: 19/159 Loss: 0.284567
2022-12-28 02:56: Train Epoch 2: 20/159 Loss: 0.262688
2022-12-28 02:57: Train Epoch 2: 21/159 Loss: 0.257947
2022-12-28 02:58: Train Epoch 2: 22/159 Loss: 0.258586
2022-12-28 02:59: Train Epoch 2: 23/159 Loss: 0.293440
2022-12-28 03:00: Train Epoch 2: 24/159 Loss: 0.218136
2022-12-28 03:01: Train Epoch 2: 25/159 Loss: 0.270309
2022-12-28 03:01: Train Epoch 2: 26/159 Loss: 0.292804
2022-12-28 03:02: Train Epoch 2: 27/159 Loss: 0.330606
2022-12-28 03:03: Train Epoch 2: 28/159 Loss: 0.254697
2022-12-28 03:04: Train Epoch 2: 29/159 Loss: 0.254930
2022-12-28 03:05: Train Epoch 2: 30/159 Loss: 0.289526
2022-12-28 03:06: Train Epoch 2: 31/159 Loss: 0.226282
2022-12-28 03:07: Train Epoch 2: 32/159 Loss: 0.304841
2022-12-28 03:08: Train Epoch 2: 33/159 Loss: 0.239585
2022-12-28 03:08: Train Epoch 2: 34/159 Loss: 0.294741
2022-12-28 03:09: Train Epoch 2: 35/159 Loss: 0.322475
2022-12-28 03:10: Train Epoch 2: 36/159 Loss: 0.196649
2022-12-28 03:11: Train Epoch 2: 37/159 Loss: 0.283302
2022-12-28 03:12: Train Epoch 2: 38/159 Loss: 0.322429
2022-12-28 03:13: Train Epoch 2: 39/159 Loss: 0.280120
2022-12-28 03:14: Train Epoch 2: 40/159 Loss: 0.311967
2022-12-28 03:14: Train Epoch 2: 41/159 Loss: 0.278578
2022-12-28 03:15: Train Epoch 2: 42/159 Loss: 0.301542
2022-12-28 03:16: Train Epoch 2: 43/159 Loss: 0.256398
2022-12-28 03:17: Train Epoch 2: 44/159 Loss: 0.243821
2022-12-28 03:18: Train Epoch 2: 45/159 Loss: 0.307629
2022-12-28 03:19: Train Epoch 2: 46/159 Loss: 0.273936
2022-12-28 03:20: Train Epoch 2: 47/159 Loss: 0.278930
2022-12-28 03:21: Train Epoch 2: 48/159 Loss: 0.296990
2022-12-28 03:22: Train Epoch 2: 49/159 Loss: 0.240850
2022-12-28 03:22: Train Epoch 2: 50/159 Loss: 0.258585
2022-12-28 03:23: Train Epoch 2: 51/159 Loss: 0.251959
2022-12-28 03:24: Train Epoch 2: 52/159 Loss: 0.349847
2022-12-28 03:25: Train Epoch 2: 53/159 Loss: 0.290855
2022-12-28 03:26: Train Epoch 2: 54/159 Loss: 0.297930
2022-12-28 03:27: Train Epoch 2: 55/159 Loss: 0.293684
2022-12-28 03:28: Train Epoch 2: 56/159 Loss: 0.249668
2022-12-28 03:29: Train Epoch 2: 57/159 Loss: 0.285549
2022-12-28 03:29: Train Epoch 2: 58/159 Loss: 0.248907
2022-12-28 03:30: Train Epoch 2: 59/159 Loss: 0.323595
2022-12-28 03:31: Train Epoch 2: 60/159 Loss: 0.274807
2022-12-28 03:32: Train Epoch 2: 61/159 Loss: 0.271525
2022-12-28 03:33: Train Epoch 2: 62/159 Loss: 0.283742
2022-12-28 03:34: Train Epoch 2: 63/159 Loss: 0.305586
2022-12-28 03:35: Train Epoch 2: 64/159 Loss: 0.259306
2022-12-28 03:36: Train Epoch 2: 65/159 Loss: 0.305806
2022-12-28 03:36: Train Epoch 2: 66/159 Loss: 0.229977
2022-12-28 03:37: Train Epoch 2: 67/159 Loss: 0.228992
2022-12-28 03:38: Train Epoch 2: 68/159 Loss: 0.300846
2022-12-28 03:39: Train Epoch 2: 69/159 Loss: 0.348866
2022-12-28 03:40: Train Epoch 2: 70/159 Loss: 0.234401
2022-12-28 03:41: Train Epoch 2: 71/159 Loss: 0.180788
2022-12-28 03:42: Train Epoch 2: 72/159 Loss: 0.273670
2022-12-28 03:43: Train Epoch 2: 73/159 Loss: 0.247859
2022-12-28 03:44: Train Epoch 2: 74/159 Loss: 0.295895
2022-12-28 03:44: Train Epoch 2: 75/159 Loss: 0.259448
2022-12-28 03:45: Train Epoch 2: 76/159 Loss: 0.199961
2022-12-28 03:46: Train Epoch 2: 77/159 Loss: 0.204447
2022-12-28 03:47: Train Epoch 2: 78/159 Loss: 0.253233
2022-12-28 03:48: Train Epoch 2: 79/159 Loss: 0.305600
2022-12-28 03:49: Train Epoch 2: 80/159 Loss: 0.301540
2022-12-28 03:50: Train Epoch 2: 81/159 Loss: 0.316960
2022-12-28 03:51: Train Epoch 2: 82/159 Loss: 0.236281
2022-12-28 03:52: Train Epoch 2: 83/159 Loss: 0.350973
2022-12-28 03:52: Train Epoch 2: 84/159 Loss: 0.270607
2022-12-28 03:53: Train Epoch 2: 85/159 Loss: 0.290276
2022-12-28 03:54: Train Epoch 2: 86/159 Loss: 0.220317
2022-12-28 03:55: Train Epoch 2: 87/159 Loss: 0.281724
2022-12-28 03:56: Train Epoch 2: 88/159 Loss: 0.252907
2022-12-28 03:57: Train Epoch 2: 89/159 Loss: 0.301679
2022-12-28 03:58: Train Epoch 2: 90/159 Loss: 0.270087
2022-12-28 03:59: Train Epoch 2: 91/159 Loss: 0.306273
2022-12-28 03:59: Train Epoch 2: 92/159 Loss: 0.302927
2022-12-28 04:00: Train Epoch 2: 93/159 Loss: 0.264425
2022-12-28 04:01: Train Epoch 2: 94/159 Loss: 0.260900
2022-12-28 04:02: Train Epoch 2: 95/159 Loss: 0.230654
2022-12-28 04:03: Train Epoch 2: 96/159 Loss: 0.268302
2022-12-28 04:04: Train Epoch 2: 97/159 Loss: 0.324943
2022-12-28 04:05: Train Epoch 2: 98/159 Loss: 0.234227
2022-12-28 04:06: Train Epoch 2: 99/159 Loss: 0.310101
2022-12-28 04:06: Train Epoch 2: 100/159 Loss: 0.371164
2022-12-28 04:07: Train Epoch 2: 101/159 Loss: 0.282688
2022-12-28 04:08: Train Epoch 2: 102/159 Loss: 0.242728
2022-12-28 04:09: Train Epoch 2: 103/159 Loss: 0.213232
2022-12-28 04:10: Train Epoch 2: 104/159 Loss: 0.272090
2022-12-28 04:11: Train Epoch 2: 105/159 Loss: 0.262780
2022-12-28 04:12: Train Epoch 2: 106/159 Loss: 0.329678
2022-12-28 04:13: Train Epoch 2: 107/159 Loss: 0.289134
2022-12-28 04:14: Train Epoch 2: 108/159 Loss: 0.344889
2022-12-28 04:14: Train Epoch 2: 109/159 Loss: 0.299921
2022-12-28 04:15: Train Epoch 2: 110/159 Loss: 0.291683
2022-12-28 04:16: Train Epoch 2: 111/159 Loss: 0.293031
2022-12-28 04:17: Train Epoch 2: 112/159 Loss: 0.345309
2022-12-28 04:18: Train Epoch 2: 113/159 Loss: 0.317902
2022-12-28 04:19: Train Epoch 2: 114/159 Loss: 0.224661
2022-12-28 04:20: Train Epoch 2: 115/159 Loss: 0.255747
2022-12-28 04:21: Train Epoch 2: 116/159 Loss: 0.271428
2022-12-28 04:21: Train Epoch 2: 117/159 Loss: 0.273619
2022-12-28 04:22: Train Epoch 2: 118/159 Loss: 0.252080
2022-12-28 04:23: Train Epoch 2: 119/159 Loss: 0.230472
2022-12-28 04:24: Train Epoch 2: 120/159 Loss: 0.254075
2022-12-28 04:25: Train Epoch 2: 121/159 Loss: 0.322805
2022-12-28 04:26: Train Epoch 2: 122/159 Loss: 0.276898
2022-12-28 04:27: Train Epoch 2: 123/159 Loss: 0.295266
2022-12-28 04:28: Train Epoch 2: 124/159 Loss: 0.351143
2022-12-28 04:29: Train Epoch 2: 125/159 Loss: 0.339895
2022-12-28 04:29: Train Epoch 2: 126/159 Loss: 0.303270
2022-12-28 04:30: Train Epoch 2: 127/159 Loss: 0.281499
2022-12-28 04:31: Train Epoch 2: 128/159 Loss: 0.340485
2022-12-28 04:32: Train Epoch 2: 129/159 Loss: 0.277581
2022-12-28 04:33: Train Epoch 2: 130/159 Loss: 0.207543
2022-12-28 04:34: Train Epoch 2: 131/159 Loss: 0.294056
2022-12-28 04:35: Train Epoch 2: 132/159 Loss: 0.255688
2022-12-28 04:36: Train Epoch 2: 133/159 Loss: 0.311445
2022-12-28 04:37: Train Epoch 2: 134/159 Loss: 0.211133
2022-12-28 04:38: Train Epoch 2: 135/159 Loss: 0.223607
2022-12-28 04:38: Train Epoch 2: 136/159 Loss: 0.211834
2022-12-28 04:39: Train Epoch 2: 137/159 Loss: 0.277176
2022-12-28 04:40: Train Epoch 2: 138/159 Loss: 0.240531
2022-12-28 04:41: Train Epoch 2: 139/159 Loss: 0.333730
2022-12-28 04:42: Train Epoch 2: 140/159 Loss: 0.222223
2022-12-28 04:43: Train Epoch 2: 141/159 Loss: 0.303034
2022-12-28 04:44: Train Epoch 2: 142/159 Loss: 0.244924
2022-12-28 04:45: Train Epoch 2: 143/159 Loss: 0.351570
2022-12-28 04:46: Train Epoch 2: 144/159 Loss: 0.324763
2022-12-28 04:46: Train Epoch 2: 145/159 Loss: 0.253909
2022-12-28 04:47: Train Epoch 2: 146/159 Loss: 0.289170
2022-12-28 04:48: Train Epoch 2: 147/159 Loss: 0.244402
2022-12-28 04:49: Train Epoch 2: 148/159 Loss: 0.326974
2022-12-28 04:50: Train Epoch 2: 149/159 Loss: 0.302576
2022-12-28 04:51: Train Epoch 2: 150/159 Loss: 0.313817
2022-12-28 04:52: Train Epoch 2: 151/159 Loss: 0.222816
2022-12-28 04:53: Train Epoch 2: 152/159 Loss: 0.217122
2022-12-28 04:54: Train Epoch 2: 153/159 Loss: 0.287143
2022-12-28 04:54: Train Epoch 2: 154/159 Loss: 0.294941
2022-12-28 04:55: Train Epoch 2: 155/159 Loss: 0.279518
2022-12-28 04:56: Train Epoch 2: 156/159 Loss: 0.206998
2022-12-28 04:57: Train Epoch 2: 157/159 Loss: 0.295726
2022-12-28 04:57: Train Epoch 2: 158/159 Loss: 0.235321
2022-12-28 04:57: **********Train Epoch 2: averaged Loss: 0.279891 
2022-12-28 04:57: 
Epoch time elapsed: 8421.749950885773

2022-12-28 05:01: 
 metrics validation: {'precision': 0.7096307023895728, 'recall': 0.7538461538461538, 'f1-score': 0.7310704960835508, 'support': 1300, 'AUC': 0.8629285502958579, 'AUCPR': 0.7905549241118554, 'TP': 980, 'FP': 401, 'TN': 2199, 'FN': 320} 

2022-12-28 05:01: **********Val Epoch 2: average Loss: 0.489136
2022-12-28 05:01: *********************************Current best model saved!
2022-12-28 05:05: 
 Testing metrics {'precision': 0.7557781201848999, 'recall': 0.7988599348534202, 'f1-score': 0.7767220902612827, 'support': 1228, 'AUC': 0.880746413755053, 'AUCPR': 0.8153603788852737, 'TP': 981, 'FP': 317, 'TN': 2139, 'FN': 247} 

2022-12-28 05:06: Train Epoch 3: 0/159 Loss: 0.284692
2022-12-28 05:07: Train Epoch 3: 1/159 Loss: 0.274657
2022-12-28 05:08: Train Epoch 3: 2/159 Loss: 0.258388
2022-12-28 05:09: Train Epoch 3: 3/159 Loss: 0.290796
2022-12-28 05:10: Train Epoch 3: 4/159 Loss: 0.241620
2022-12-28 05:11: Train Epoch 3: 5/159 Loss: 0.304190
2022-12-28 05:11: Train Epoch 3: 6/159 Loss: 0.277321
2022-12-28 05:12: Train Epoch 3: 7/159 Loss: 0.238244
2022-12-28 05:13: Train Epoch 3: 8/159 Loss: 0.239845
2022-12-28 05:14: Train Epoch 3: 9/159 Loss: 0.302820
2022-12-28 05:15: Train Epoch 3: 10/159 Loss: 0.255098
2022-12-28 05:16: Train Epoch 3: 11/159 Loss: 0.293938
2022-12-28 05:17: Train Epoch 3: 12/159 Loss: 0.256920
2022-12-28 05:18: Train Epoch 3: 13/159 Loss: 0.255667
2022-12-28 05:19: Train Epoch 3: 14/159 Loss: 0.298280
2022-12-28 05:19: Train Epoch 3: 15/159 Loss: 0.281963
2022-12-28 05:20: Train Epoch 3: 16/159 Loss: 0.279121
2022-12-28 05:21: Train Epoch 3: 17/159 Loss: 0.271616
2022-12-28 05:22: Train Epoch 3: 18/159 Loss: 0.226561
2022-12-28 05:23: Train Epoch 3: 19/159 Loss: 0.286893
2022-12-28 05:24: Train Epoch 3: 20/159 Loss: 0.281387
2022-12-28 05:25: Train Epoch 3: 21/159 Loss: 0.335709
2022-12-28 05:26: Train Epoch 3: 22/159 Loss: 0.244066
2022-12-28 05:26: Train Epoch 3: 23/159 Loss: 0.272522
2022-12-28 05:27: Train Epoch 3: 24/159 Loss: 0.319459
2022-12-28 05:28: Train Epoch 3: 25/159 Loss: 0.266041
2022-12-28 05:29: Train Epoch 3: 26/159 Loss: 0.234880
2022-12-28 05:30: Train Epoch 3: 27/159 Loss: 0.285774
2022-12-28 05:31: Train Epoch 3: 28/159 Loss: 0.309636
2022-12-28 05:32: Train Epoch 3: 29/159 Loss: 0.222623
2022-12-28 05:33: Train Epoch 3: 30/159 Loss: 0.280434
2022-12-28 05:34: Train Epoch 3: 31/159 Loss: 0.290991
2022-12-28 05:34: Train Epoch 3: 32/159 Loss: 0.262767
2022-12-28 05:35: Train Epoch 3: 33/159 Loss: 0.287972
2022-12-28 05:36: Train Epoch 3: 34/159 Loss: 0.215531
2022-12-28 05:37: Train Epoch 3: 35/159 Loss: 0.241761
2022-12-28 05:38: Train Epoch 3: 36/159 Loss: 0.292465
2022-12-28 05:39: Train Epoch 3: 37/159 Loss: 0.194737
2022-12-28 05:40: Train Epoch 3: 38/159 Loss: 0.261382
2022-12-28 05:41: Train Epoch 3: 39/159 Loss: 0.348101
2022-12-28 05:41: Train Epoch 3: 40/159 Loss: 0.284474
2022-12-28 05:42: Train Epoch 3: 41/159 Loss: 0.243547
2022-12-28 05:43: Train Epoch 3: 42/159 Loss: 0.269027
2022-12-28 05:44: Train Epoch 3: 43/159 Loss: 0.248093
2022-12-28 05:45: Train Epoch 3: 44/159 Loss: 0.262294
2022-12-28 05:46: Train Epoch 3: 45/159 Loss: 0.294608
2022-12-28 05:47: Train Epoch 3: 46/159 Loss: 0.300730
2022-12-28 05:48: Train Epoch 3: 47/159 Loss: 0.231818
2022-12-28 05:49: Train Epoch 3: 48/159 Loss: 0.272256
2022-12-28 05:49: Train Epoch 3: 49/159 Loss: 0.304897
2022-12-28 05:50: Train Epoch 3: 50/159 Loss: 0.280981
2022-12-28 05:51: Train Epoch 3: 51/159 Loss: 0.232362
2022-12-28 05:52: Train Epoch 3: 52/159 Loss: 0.208838
2022-12-28 05:53: Train Epoch 3: 53/159 Loss: 0.279218
2022-12-28 05:54: Train Epoch 3: 54/159 Loss: 0.305920
2022-12-28 05:55: Train Epoch 3: 55/159 Loss: 0.264556
2022-12-28 05:56: Train Epoch 3: 56/159 Loss: 0.290221
2022-12-28 05:57: Train Epoch 3: 57/159 Loss: 0.264422
2022-12-28 05:57: Train Epoch 3: 58/159 Loss: 0.247876
2022-12-28 05:58: Train Epoch 3: 59/159 Loss: 0.258663
2022-12-28 05:59: Train Epoch 3: 60/159 Loss: 0.234101
2022-12-28 06:00: Train Epoch 3: 61/159 Loss: 0.289915
2022-12-28 06:01: Train Epoch 3: 62/159 Loss: 0.240079
2022-12-28 06:02: Train Epoch 3: 63/159 Loss: 0.330965
2022-12-28 06:03: Train Epoch 3: 64/159 Loss: 0.250097
2022-12-28 06:04: Train Epoch 3: 65/159 Loss: 0.353088
2022-12-28 06:04: Train Epoch 3: 66/159 Loss: 0.313233
2022-12-28 06:05: Train Epoch 3: 67/159 Loss: 0.269182
2022-12-28 06:06: Train Epoch 3: 68/159 Loss: 0.276849
2022-12-28 06:07: Train Epoch 3: 69/159 Loss: 0.250655
2022-12-28 06:08: Train Epoch 3: 70/159 Loss: 0.222279
2022-12-28 06:09: Train Epoch 3: 71/159 Loss: 0.296347
2022-12-28 06:10: Train Epoch 3: 72/159 Loss: 0.268190
2022-12-28 06:11: Train Epoch 3: 73/159 Loss: 0.278742
2022-12-28 06:11: Train Epoch 3: 74/159 Loss: 0.323730
2022-12-28 06:12: Train Epoch 3: 75/159 Loss: 0.226880
2022-12-28 06:13: Train Epoch 3: 76/159 Loss: 0.209216
2022-12-28 06:14: Train Epoch 3: 77/159 Loss: 0.235748
2022-12-28 06:15: Train Epoch 3: 78/159 Loss: 0.245738
2022-12-28 06:16: Train Epoch 3: 79/159 Loss: 0.254269
2022-12-28 06:17: Train Epoch 3: 80/159 Loss: 0.333621
2022-12-28 06:18: Train Epoch 3: 81/159 Loss: 0.227691
2022-12-28 06:19: Train Epoch 3: 82/159 Loss: 0.203852
2022-12-28 06:19: Train Epoch 3: 83/159 Loss: 0.263567
2022-12-28 06:20: Train Epoch 3: 84/159 Loss: 0.262203
2022-12-28 06:21: Train Epoch 3: 85/159 Loss: 0.320506
2022-12-28 06:22: Train Epoch 3: 86/159 Loss: 0.311401
2022-12-28 06:23: Train Epoch 3: 87/159 Loss: 0.269002
2022-12-28 06:24: Train Epoch 3: 88/159 Loss: 0.253203
2022-12-28 06:25: Train Epoch 3: 89/159 Loss: 0.250970
2022-12-28 06:26: Train Epoch 3: 90/159 Loss: 0.286753
2022-12-28 06:27: Train Epoch 3: 91/159 Loss: 0.266836
2022-12-28 06:27: Train Epoch 3: 92/159 Loss: 0.294891
2022-12-28 06:28: Train Epoch 3: 93/159 Loss: 0.286585
2022-12-28 06:29: Train Epoch 3: 94/159 Loss: 0.275138
2022-12-28 06:30: Train Epoch 3: 95/159 Loss: 0.262412
2022-12-28 06:31: Train Epoch 3: 96/159 Loss: 0.263310
2022-12-28 06:32: Train Epoch 3: 97/159 Loss: 0.315190
2022-12-28 06:33: Train Epoch 3: 98/159 Loss: 0.219462
2022-12-28 06:34: Train Epoch 3: 99/159 Loss: 0.246537
2022-12-28 06:35: Train Epoch 3: 100/159 Loss: 0.297263
2022-12-28 06:35: Train Epoch 3: 101/159 Loss: 0.258893
2022-12-28 06:36: Train Epoch 3: 102/159 Loss: 0.320027
2022-12-28 06:37: Train Epoch 3: 103/159 Loss: 0.249953
2022-12-28 06:38: Train Epoch 3: 104/159 Loss: 0.185459
2022-12-28 06:39: Train Epoch 3: 105/159 Loss: 0.260646
2022-12-28 06:40: Train Epoch 3: 106/159 Loss: 0.282110
2022-12-28 06:41: Train Epoch 3: 107/159 Loss: 0.253129
2022-12-28 06:42: Train Epoch 3: 108/159 Loss: 0.309030
2022-12-28 06:43: Train Epoch 3: 109/159 Loss: 0.278886
2022-12-28 06:43: Train Epoch 3: 110/159 Loss: 0.324315
2022-12-28 06:44: Train Epoch 3: 111/159 Loss: 0.258632
2022-12-28 06:45: Train Epoch 3: 112/159 Loss: 0.237626
2022-12-28 06:46: Train Epoch 3: 113/159 Loss: 0.284048
2022-12-28 06:47: Train Epoch 3: 114/159 Loss: 0.246187
2022-12-28 06:48: Train Epoch 3: 115/159 Loss: 0.250066
2022-12-28 06:49: Train Epoch 3: 116/159 Loss: 0.252489
2022-12-28 06:50: Train Epoch 3: 117/159 Loss: 0.240879
2022-12-28 06:51: Train Epoch 3: 118/159 Loss: 0.304064
2022-12-28 06:51: Train Epoch 3: 119/159 Loss: 0.289158
2022-12-28 06:52: Train Epoch 3: 120/159 Loss: 0.284158
2022-12-28 06:53: Train Epoch 3: 121/159 Loss: 0.229449
2022-12-28 06:54: Train Epoch 3: 122/159 Loss: 0.273890
2022-12-28 06:55: Train Epoch 3: 123/159 Loss: 0.224755
2022-12-28 06:56: Train Epoch 3: 124/159 Loss: 0.316269
2022-12-28 06:57: Train Epoch 3: 125/159 Loss: 0.235176
2022-12-28 06:58: Train Epoch 3: 126/159 Loss: 0.295409
2022-12-28 06:58: Train Epoch 3: 127/159 Loss: 0.241449
2022-12-28 06:59: Train Epoch 3: 128/159 Loss: 0.246982
2022-12-28 07:00: Train Epoch 3: 129/159 Loss: 0.246904
2022-12-28 07:01: Train Epoch 3: 130/159 Loss: 0.275663
2022-12-28 07:02: Train Epoch 3: 131/159 Loss: 0.267996
2022-12-28 07:03: Train Epoch 3: 132/159 Loss: 0.255081
2022-12-28 07:04: Train Epoch 3: 133/159 Loss: 0.280849
2022-12-28 07:05: Train Epoch 3: 134/159 Loss: 0.280916
2022-12-28 07:05: Train Epoch 3: 135/159 Loss: 0.303402
2022-12-28 07:06: Train Epoch 3: 136/159 Loss: 0.231118
2022-12-28 07:07: Train Epoch 3: 137/159 Loss: 0.283388
2022-12-28 07:08: Train Epoch 3: 138/159 Loss: 0.297983
2022-12-28 07:09: Train Epoch 3: 139/159 Loss: 0.291821
2022-12-28 07:10: Train Epoch 3: 140/159 Loss: 0.310334
2022-12-28 07:11: Train Epoch 3: 141/159 Loss: 0.241518
2022-12-28 07:12: Train Epoch 3: 142/159 Loss: 0.209053
2022-12-28 07:12: Train Epoch 3: 143/159 Loss: 0.346235
2022-12-28 07:13: Train Epoch 3: 144/159 Loss: 0.246669
2022-12-28 07:14: Train Epoch 3: 145/159 Loss: 0.242037
2022-12-28 07:15: Train Epoch 3: 146/159 Loss: 0.202138
2022-12-28 07:16: Train Epoch 3: 147/159 Loss: 0.242949
2022-12-28 07:17: Train Epoch 3: 148/159 Loss: 0.278224
2022-12-28 07:18: Train Epoch 3: 149/159 Loss: 0.302792
2022-12-28 07:19: Train Epoch 3: 150/159 Loss: 0.259028
2022-12-28 07:19: Train Epoch 3: 151/159 Loss: 0.263023
2022-12-28 07:20: Train Epoch 3: 152/159 Loss: 0.307319
2022-12-28 07:21: Train Epoch 3: 153/159 Loss: 0.290518
2022-12-28 07:22: Train Epoch 3: 154/159 Loss: 0.245643
2022-12-28 07:23: Train Epoch 3: 155/159 Loss: 0.283870
2022-12-28 07:24: Train Epoch 3: 156/159 Loss: 0.325575
2022-12-28 07:25: Train Epoch 3: 157/159 Loss: 0.222768
2022-12-28 07:25: Train Epoch 3: 158/159 Loss: 0.203357
2022-12-28 07:25: **********Train Epoch 3: averaged Loss: 0.268948 
2022-12-28 07:25: 
Epoch time elapsed: 8388.668563127518

2022-12-28 07:29: 
 metrics validation: {'precision': 0.7783641160949868, 'recall': 0.6807692307692308, 'f1-score': 0.7263028313500205, 'support': 1300, 'AUC': 0.8689044378698225, 'AUCPR': 0.7979695237518617, 'TP': 885, 'FP': 252, 'TN': 2348, 'FN': 415} 

2022-12-28 07:29: **********Val Epoch 3: average Loss: 0.491302
2022-12-28 07:33: 
 Testing metrics {'precision': 0.7557781201848999, 'recall': 0.7988599348534202, 'f1-score': 0.7767220902612827, 'support': 1228, 'AUC': 0.880746413755053, 'AUCPR': 0.8153603788852737, 'TP': 981, 'FP': 317, 'TN': 2139, 'FN': 247} 

2022-12-28 07:34: Train Epoch 4: 0/159 Loss: 0.297038
2022-12-28 07:35: Train Epoch 4: 1/159 Loss: 0.288179
2022-12-28 07:36: Train Epoch 4: 2/159 Loss: 0.274264
2022-12-28 07:37: Train Epoch 4: 3/159 Loss: 0.252990
2022-12-28 07:38: Train Epoch 4: 4/159 Loss: 0.363356
2022-12-28 07:38: Train Epoch 4: 5/159 Loss: 0.229852
2022-12-28 07:39: Train Epoch 4: 6/159 Loss: 0.320873
2022-12-28 07:40: Train Epoch 4: 7/159 Loss: 0.257180
2022-12-28 07:41: Train Epoch 4: 8/159 Loss: 0.244428
2022-12-28 07:42: Train Epoch 4: 9/159 Loss: 0.251324
2022-12-28 07:43: Train Epoch 4: 10/159 Loss: 0.236586
2022-12-28 07:44: Train Epoch 4: 11/159 Loss: 0.277465
2022-12-28 07:45: Train Epoch 4: 12/159 Loss: 0.275231
2022-12-28 07:46: Train Epoch 4: 13/159 Loss: 0.273872
2022-12-28 07:46: Train Epoch 4: 14/159 Loss: 0.323449
2022-12-28 07:47: Train Epoch 4: 15/159 Loss: 0.278313
2022-12-28 07:48: Train Epoch 4: 16/159 Loss: 0.247536
2022-12-28 07:49: Train Epoch 4: 17/159 Loss: 0.229359
2022-12-28 07:50: Train Epoch 4: 18/159 Loss: 0.245476
2022-12-28 07:51: Train Epoch 4: 19/159 Loss: 0.230450
2022-12-28 07:52: Train Epoch 4: 20/159 Loss: 0.379276
2022-12-28 07:53: Train Epoch 4: 21/159 Loss: 0.240141
2022-12-28 07:54: Train Epoch 4: 22/159 Loss: 0.227809
2022-12-28 07:54: Train Epoch 4: 23/159 Loss: 0.218834
2022-12-28 07:55: Train Epoch 4: 24/159 Loss: 0.200169
2022-12-28 07:56: Train Epoch 4: 25/159 Loss: 0.248249
2022-12-28 07:57: Train Epoch 4: 26/159 Loss: 0.306260
2022-12-28 07:58: Train Epoch 4: 27/159 Loss: 0.257236
2022-12-28 07:59: Train Epoch 4: 28/159 Loss: 0.276664
2022-12-28 08:00: Train Epoch 4: 29/159 Loss: 0.247370
2022-12-28 08:01: Train Epoch 4: 30/159 Loss: 0.252626
2022-12-28 08:02: Train Epoch 4: 31/159 Loss: 0.363722
2022-12-28 08:02: Train Epoch 4: 32/159 Loss: 0.301224
2022-12-28 08:03: Train Epoch 4: 33/159 Loss: 0.264737
2022-12-28 08:04: Train Epoch 4: 34/159 Loss: 0.315535
2022-12-28 08:05: Train Epoch 4: 35/159 Loss: 0.261017
2022-12-28 08:06: Train Epoch 4: 36/159 Loss: 0.230528
2022-12-28 08:07: Train Epoch 4: 37/159 Loss: 0.278527
2022-12-28 08:08: Train Epoch 4: 38/159 Loss: 0.347389
2022-12-28 08:09: Train Epoch 4: 39/159 Loss: 0.244249
2022-12-28 08:09: Train Epoch 4: 40/159 Loss: 0.327760
2022-12-28 08:10: Train Epoch 4: 41/159 Loss: 0.380896
2022-12-28 08:11: Train Epoch 4: 42/159 Loss: 0.292428
2022-12-28 08:12: Train Epoch 4: 43/159 Loss: 0.337828
2022-12-28 08:13: Train Epoch 4: 44/159 Loss: 0.315681
2022-12-28 08:14: Train Epoch 4: 45/159 Loss: 0.309809
2022-12-28 08:15: Train Epoch 4: 46/159 Loss: 0.285688
2022-12-28 08:16: Train Epoch 4: 47/159 Loss: 0.229329
2022-12-28 08:17: Train Epoch 4: 48/159 Loss: 0.252904
2022-12-28 08:17: Train Epoch 4: 49/159 Loss: 0.233826
2022-12-28 08:18: Train Epoch 4: 50/159 Loss: 0.288079
2022-12-28 08:19: Train Epoch 4: 51/159 Loss: 0.246336
2022-12-28 08:20: Train Epoch 4: 52/159 Loss: 0.268375
2022-12-28 08:21: Train Epoch 4: 53/159 Loss: 0.292956
2022-12-28 08:22: Train Epoch 4: 54/159 Loss: 0.288743
2022-12-28 08:23: Train Epoch 4: 55/159 Loss: 0.232445
2022-12-28 08:24: Train Epoch 4: 56/159 Loss: 0.280699
2022-12-28 08:24: Train Epoch 4: 57/159 Loss: 0.212115
2022-12-28 08:25: Train Epoch 4: 58/159 Loss: 0.228029
2022-12-28 08:26: Train Epoch 4: 59/159 Loss: 0.278543
2022-12-28 08:27: Train Epoch 4: 60/159 Loss: 0.269340
2022-12-28 08:28: Train Epoch 4: 61/159 Loss: 0.250294
2022-12-28 08:29: Train Epoch 4: 62/159 Loss: 0.235776
2022-12-28 08:30: Train Epoch 4: 63/159 Loss: 0.260666
2022-12-28 08:31: Train Epoch 4: 64/159 Loss: 0.255881
2022-12-28 08:31: Train Epoch 4: 65/159 Loss: 0.266132
2022-12-28 08:32: Train Epoch 4: 66/159 Loss: 0.253345
2022-12-28 08:33: Train Epoch 4: 67/159 Loss: 0.319933
2022-12-28 08:34: Train Epoch 4: 68/159 Loss: 0.223266
2022-12-28 08:35: Train Epoch 4: 69/159 Loss: 0.269645
2022-12-28 08:36: Train Epoch 4: 70/159 Loss: 0.260850
2022-12-28 08:37: Train Epoch 4: 71/159 Loss: 0.267726
2022-12-28 08:38: Train Epoch 4: 72/159 Loss: 0.300790
2022-12-28 08:38: Train Epoch 4: 73/159 Loss: 0.277637
2022-12-28 08:39: Train Epoch 4: 74/159 Loss: 0.226613
2022-12-28 08:40: Train Epoch 4: 75/159 Loss: 0.272409
2022-12-28 08:41: Train Epoch 4: 76/159 Loss: 0.258167
2022-12-28 08:42: Train Epoch 4: 77/159 Loss: 0.350802
2022-12-28 08:43: Train Epoch 4: 78/159 Loss: 0.255567
2022-12-28 08:44: Train Epoch 4: 79/159 Loss: 0.270968
2022-12-28 08:45: Train Epoch 4: 80/159 Loss: 0.311765
2022-12-28 08:45: Train Epoch 4: 81/159 Loss: 0.237093
2022-12-28 08:46: Train Epoch 4: 82/159 Loss: 0.256282
2022-12-28 08:47: Train Epoch 4: 83/159 Loss: 0.287506
2022-12-28 08:48: Train Epoch 4: 84/159 Loss: 0.215762
2022-12-28 08:49: Train Epoch 4: 85/159 Loss: 0.245004
2022-12-28 08:50: Train Epoch 4: 86/159 Loss: 0.242362
2022-12-28 08:51: Train Epoch 4: 87/159 Loss: 0.277091
2022-12-28 08:52: Train Epoch 4: 88/159 Loss: 0.303715
2022-12-28 08:52: Train Epoch 4: 89/159 Loss: 0.293545
2022-12-28 08:53: Train Epoch 4: 90/159 Loss: 0.323319
2022-12-28 08:54: Train Epoch 4: 91/159 Loss: 0.232860
2022-12-28 08:55: Train Epoch 4: 92/159 Loss: 0.346951
2022-12-28 08:56: Train Epoch 4: 93/159 Loss: 0.270601
2022-12-28 08:57: Train Epoch 4: 94/159 Loss: 0.262959
2022-12-28 08:58: Train Epoch 4: 95/159 Loss: 0.286665
2022-12-28 08:59: Train Epoch 4: 96/159 Loss: 0.224091
2022-12-28 09:00: Train Epoch 4: 97/159 Loss: 0.282046
2022-12-28 09:00: Train Epoch 4: 98/159 Loss: 0.222087
2022-12-28 09:01: Train Epoch 4: 99/159 Loss: 0.235586
2022-12-28 09:02: Train Epoch 4: 100/159 Loss: 0.309378
2022-12-28 09:03: Train Epoch 4: 101/159 Loss: 0.285716
2022-12-28 09:04: Train Epoch 4: 102/159 Loss: 0.282501
2022-12-28 09:05: Train Epoch 4: 103/159 Loss: 0.283117
2022-12-28 09:06: Train Epoch 4: 104/159 Loss: 0.232056
2022-12-28 09:07: Train Epoch 4: 105/159 Loss: 0.270548
2022-12-28 09:07: Train Epoch 4: 106/159 Loss: 0.250677
2022-12-28 09:08: Train Epoch 4: 107/159 Loss: 0.318465
2022-12-28 09:09: Train Epoch 4: 108/159 Loss: 0.283965
2022-12-28 09:10: Train Epoch 4: 109/159 Loss: 0.307319
2022-12-28 09:11: Train Epoch 4: 110/159 Loss: 0.276965
2022-12-28 09:12: Train Epoch 4: 111/159 Loss: 0.269756
2022-12-28 09:13: Train Epoch 4: 112/159 Loss: 0.265478
2022-12-28 09:14: Train Epoch 4: 113/159 Loss: 0.271987
2022-12-28 09:15: Train Epoch 4: 114/159 Loss: 0.264373
2022-12-28 09:15: Train Epoch 4: 115/159 Loss: 0.292867
2022-12-28 09:16: Train Epoch 4: 116/159 Loss: 0.230912
2022-12-28 09:17: Train Epoch 4: 117/159 Loss: 0.202401
2022-12-28 09:18: Train Epoch 4: 118/159 Loss: 0.290306
2022-12-28 09:19: Train Epoch 4: 119/159 Loss: 0.265418
2022-12-28 09:20: Train Epoch 4: 120/159 Loss: 0.218293
2022-12-28 09:21: Train Epoch 4: 121/159 Loss: 0.286457
2022-12-28 09:22: Train Epoch 4: 122/159 Loss: 0.218842
2022-12-28 09:23: Train Epoch 4: 123/159 Loss: 0.286765
2022-12-28 09:23: Train Epoch 4: 124/159 Loss: 0.251359
2022-12-28 09:24: Train Epoch 4: 125/159 Loss: 0.249963
2022-12-28 09:25: Train Epoch 4: 126/159 Loss: 0.264782
2022-12-28 09:26: Train Epoch 4: 127/159 Loss: 0.312591
2022-12-28 09:27: Train Epoch 4: 128/159 Loss: 0.289756
2022-12-28 09:28: Train Epoch 4: 129/159 Loss: 0.257632
2022-12-28 09:29: Train Epoch 4: 130/159 Loss: 0.247585
2022-12-28 09:30: Train Epoch 4: 131/159 Loss: 0.217875
2022-12-28 09:31: Train Epoch 4: 132/159 Loss: 0.267894
2022-12-28 09:31: Train Epoch 4: 133/159 Loss: 0.243732
2022-12-28 09:32: Train Epoch 4: 134/159 Loss: 0.236154
2022-12-28 09:33: Train Epoch 4: 135/159 Loss: 0.297224
2022-12-28 09:34: Train Epoch 4: 136/159 Loss: 0.355558
2022-12-28 09:35: Train Epoch 4: 137/159 Loss: 0.246183
2022-12-28 09:36: Train Epoch 4: 138/159 Loss: 0.199526
2022-12-28 09:37: Train Epoch 4: 139/159 Loss: 0.296877
2022-12-28 09:38: Train Epoch 4: 140/159 Loss: 0.281115
2022-12-28 09:39: Train Epoch 4: 141/159 Loss: 0.281147
2022-12-28 09:39: Train Epoch 4: 142/159 Loss: 0.302862
2022-12-28 09:40: Train Epoch 4: 143/159 Loss: 0.332685
2022-12-28 09:41: Train Epoch 4: 144/159 Loss: 0.281949
2022-12-28 09:42: Train Epoch 4: 145/159 Loss: 0.308931
2022-12-28 09:43: Train Epoch 4: 146/159 Loss: 0.247758
2022-12-28 09:44: Train Epoch 4: 147/159 Loss: 0.286772
2022-12-28 09:45: Train Epoch 4: 148/159 Loss: 0.253544
2022-12-28 09:46: Train Epoch 4: 149/159 Loss: 0.188470
2022-12-28 09:47: Train Epoch 4: 150/159 Loss: 0.243555
2022-12-28 09:47: Train Epoch 4: 151/159 Loss: 0.227929
2022-12-28 09:48: Train Epoch 4: 152/159 Loss: 0.320145
2022-12-28 09:49: Train Epoch 4: 153/159 Loss: 0.229547
2022-12-28 09:50: Train Epoch 4: 154/159 Loss: 0.258416
2022-12-28 09:51: Train Epoch 4: 155/159 Loss: 0.288106
2022-12-28 09:52: Train Epoch 4: 156/159 Loss: 0.319272
2022-12-28 09:53: Train Epoch 4: 157/159 Loss: 0.283485
2022-12-28 09:53: Train Epoch 4: 158/159 Loss: 0.236802
2022-12-28 09:53: **********Train Epoch 4: averaged Loss: 0.270541 
2022-12-28 09:53: 
Epoch time elapsed: 8423.867841005325

2022-12-28 09:57: 
 metrics validation: {'precision': 0.6757457846952011, 'recall': 0.8015384615384615, 'f1-score': 0.7332864180154821, 'support': 1300, 'AUC': 0.8670403846153847, 'AUCPR': 0.7949937099487363, 'TP': 1042, 'FP': 500, 'TN': 2100, 'FN': 258} 

2022-12-28 09:57: **********Val Epoch 4: average Loss: 0.496093
2022-12-28 10:01: 
 Testing metrics {'precision': 0.7557781201848999, 'recall': 0.7988599348534202, 'f1-score': 0.7767220902612827, 'support': 1228, 'AUC': 0.880746413755053, 'AUCPR': 0.8153603788852737, 'TP': 981, 'FP': 317, 'TN': 2139, 'FN': 247} 

2022-12-28 10:02: Train Epoch 5: 0/159 Loss: 0.343074
2022-12-28 10:03: Train Epoch 5: 1/159 Loss: 0.235378
2022-12-28 10:04: Train Epoch 5: 2/159 Loss: 0.315459
2022-12-28 10:05: Train Epoch 5: 3/159 Loss: 0.300496
2022-12-28 10:06: Train Epoch 5: 4/159 Loss: 0.247369
2022-12-28 10:07: Train Epoch 5: 5/159 Loss: 0.286140
2022-12-28 10:08: Train Epoch 5: 6/159 Loss: 0.357661
2022-12-28 10:09: Train Epoch 5: 7/159 Loss: 0.278738
2022-12-28 10:10: Train Epoch 5: 8/159 Loss: 0.284689
2022-12-28 10:10: Train Epoch 5: 9/159 Loss: 0.352389
2022-12-28 10:11: Train Epoch 5: 10/159 Loss: 0.367697
2022-12-28 10:12: Train Epoch 5: 11/159 Loss: 0.242333
2022-12-28 10:13: Train Epoch 5: 12/159 Loss: 0.298221
2022-12-28 10:14: Train Epoch 5: 13/159 Loss: 0.354166
2022-12-28 10:15: Train Epoch 5: 14/159 Loss: 0.270153
2022-12-28 10:16: Train Epoch 5: 15/159 Loss: 0.283875
2022-12-28 10:17: Train Epoch 5: 16/159 Loss: 0.319394
2022-12-28 10:18: Train Epoch 5: 17/159 Loss: 0.258161
2022-12-28 10:18: Train Epoch 5: 18/159 Loss: 0.295022
2022-12-28 10:19: Train Epoch 5: 19/159 Loss: 0.273668
2022-12-28 10:20: Train Epoch 5: 20/159 Loss: 0.275934
2022-12-28 10:21: Train Epoch 5: 21/159 Loss: 0.333500
2022-12-28 10:22: Train Epoch 5: 22/159 Loss: 0.251779
2022-12-28 10:23: Train Epoch 5: 23/159 Loss: 0.319452
2022-12-28 10:24: Train Epoch 5: 24/159 Loss: 0.253819
2022-12-28 10:25: Train Epoch 5: 25/159 Loss: 0.218225
2022-12-28 10:26: Train Epoch 5: 26/159 Loss: 0.290393
2022-12-28 10:26: Train Epoch 5: 27/159 Loss: 0.223482
2022-12-28 10:27: Train Epoch 5: 28/159 Loss: 0.228653
2022-12-28 10:28: Train Epoch 5: 29/159 Loss: 0.289509
2022-12-28 10:29: Train Epoch 5: 30/159 Loss: 0.305245
2022-12-28 10:30: Train Epoch 5: 31/159 Loss: 0.261620
2022-12-28 10:31: Train Epoch 5: 32/159 Loss: 0.325963
2022-12-28 10:32: Train Epoch 5: 33/159 Loss: 0.224502
2022-12-28 10:33: Train Epoch 5: 34/159 Loss: 0.243905
2022-12-28 10:33: Train Epoch 5: 35/159 Loss: 0.247240
2022-12-28 10:34: Train Epoch 5: 36/159 Loss: 0.226805
2022-12-28 10:35: Train Epoch 5: 37/159 Loss: 0.318451
2022-12-28 10:36: Train Epoch 5: 38/159 Loss: 0.218544
2022-12-28 10:37: Train Epoch 5: 39/159 Loss: 0.295291
2022-12-28 10:38: Train Epoch 5: 40/159 Loss: 0.232318
2022-12-28 10:39: Train Epoch 5: 41/159 Loss: 0.276652
2022-12-28 10:40: Train Epoch 5: 42/159 Loss: 0.243069
2022-12-28 10:40: Train Epoch 5: 43/159 Loss: 0.303836
2022-12-28 10:41: Train Epoch 5: 44/159 Loss: 0.220543
2022-12-28 10:42: Train Epoch 5: 45/159 Loss: 0.308829
2022-12-28 10:43: Train Epoch 5: 46/159 Loss: 0.402167
2022-12-28 10:44: Train Epoch 5: 47/159 Loss: 0.304104
2022-12-28 10:45: Train Epoch 5: 48/159 Loss: 0.282286
2022-12-28 10:46: Train Epoch 5: 49/159 Loss: 0.235900
2022-12-28 10:47: Train Epoch 5: 50/159 Loss: 0.255214
2022-12-28 10:48: Train Epoch 5: 51/159 Loss: 0.264766
2022-12-28 10:48: Train Epoch 5: 52/159 Loss: 0.243808
2022-12-28 10:49: Train Epoch 5: 53/159 Loss: 0.301812
2022-12-28 10:50: Train Epoch 5: 54/159 Loss: 0.235923
2022-12-28 10:51: Train Epoch 5: 55/159 Loss: 0.272192
2022-12-28 10:52: Train Epoch 5: 56/159 Loss: 0.221186
2022-12-28 10:53: Train Epoch 5: 57/159 Loss: 0.309186
2022-12-28 10:54: Train Epoch 5: 58/159 Loss: 0.250268
2022-12-28 10:55: Train Epoch 5: 59/159 Loss: 0.277045
2022-12-28 10:55: Train Epoch 5: 60/159 Loss: 0.237690
2022-12-28 10:56: Train Epoch 5: 61/159 Loss: 0.317538
2022-12-28 10:57: Train Epoch 5: 62/159 Loss: 0.234188
2022-12-28 10:58: Train Epoch 5: 63/159 Loss: 0.316767
2022-12-28 10:59: Train Epoch 5: 64/159 Loss: 0.266452
2022-12-28 11:00: Train Epoch 5: 65/159 Loss: 0.270652
2022-12-28 11:01: Train Epoch 5: 66/159 Loss: 0.346682
2022-12-28 11:02: Train Epoch 5: 67/159 Loss: 0.263551
2022-12-28 11:02: Train Epoch 5: 68/159 Loss: 0.338250
2022-12-28 11:03: Train Epoch 5: 69/159 Loss: 0.298426
2022-12-28 11:04: Train Epoch 5: 70/159 Loss: 0.243929
2022-12-28 11:05: Train Epoch 5: 71/159 Loss: 0.226817
2022-12-28 11:06: Train Epoch 5: 72/159 Loss: 0.290607
2022-12-28 11:07: Train Epoch 5: 73/159 Loss: 0.265540
2022-12-28 11:08: Train Epoch 5: 74/159 Loss: 0.281579
2022-12-28 11:09: Train Epoch 5: 75/159 Loss: 0.319087
2022-12-28 11:09: Train Epoch 5: 76/159 Loss: 0.235034
2022-12-28 11:10: Train Epoch 5: 77/159 Loss: 0.237558
2022-12-28 11:11: Train Epoch 5: 78/159 Loss: 0.299712
2022-12-28 11:12: Train Epoch 5: 79/159 Loss: 0.253275
2022-12-28 11:13: Train Epoch 5: 80/159 Loss: 0.234728
2022-12-28 11:14: Train Epoch 5: 81/159 Loss: 0.278517
2022-12-28 11:15: Train Epoch 5: 82/159 Loss: 0.316299
2022-12-28 11:15: Train Epoch 5: 83/159 Loss: 0.345587
2022-12-28 11:16: Train Epoch 5: 84/159 Loss: 0.255123
2022-12-28 11:17: Train Epoch 5: 85/159 Loss: 0.203332
2022-12-28 11:18: Train Epoch 5: 86/159 Loss: 0.258553
2022-12-28 11:19: Train Epoch 5: 87/159 Loss: 0.302679
2022-12-28 11:20: Train Epoch 5: 88/159 Loss: 0.269168
2022-12-28 11:21: Train Epoch 5: 89/159 Loss: 0.254993
2022-12-28 11:22: Train Epoch 5: 90/159 Loss: 0.293932
2022-12-28 11:23: Train Epoch 5: 91/159 Loss: 0.366836
2022-12-28 11:23: Train Epoch 5: 92/159 Loss: 0.290738
2022-12-28 11:24: Train Epoch 5: 93/159 Loss: 0.370879
2022-12-28 11:25: Train Epoch 5: 94/159 Loss: 0.270159
2022-12-28 11:26: Train Epoch 5: 95/159 Loss: 0.209916
2022-12-28 11:27: Train Epoch 5: 96/159 Loss: 0.348208
2022-12-28 11:28: Train Epoch 5: 97/159 Loss: 0.263952
2022-12-28 11:29: Train Epoch 5: 98/159 Loss: 0.262280
2022-12-28 11:30: Train Epoch 5: 99/159 Loss: 0.337317
2022-12-28 11:30: Train Epoch 5: 100/159 Loss: 0.324209
2022-12-28 11:31: Train Epoch 5: 101/159 Loss: 0.267610
2022-12-28 11:32: Train Epoch 5: 102/159 Loss: 0.317164
2022-12-28 11:33: Train Epoch 5: 103/159 Loss: 0.371307
2022-12-28 11:34: Train Epoch 5: 104/159 Loss: 0.297593
2022-12-28 11:35: Train Epoch 5: 105/159 Loss: 0.270869
2022-12-28 11:36: Train Epoch 5: 106/159 Loss: 0.261124
2022-12-28 11:37: Train Epoch 5: 107/159 Loss: 0.277853
2022-12-28 11:38: Train Epoch 5: 108/159 Loss: 0.238829
2022-12-28 11:38: Train Epoch 5: 109/159 Loss: 0.280361
2022-12-28 11:39: Train Epoch 5: 110/159 Loss: 0.230812
2022-12-28 11:40: Train Epoch 5: 111/159 Loss: 0.277775
2022-12-28 11:41: Train Epoch 5: 112/159 Loss: 0.355509
2022-12-28 11:42: Train Epoch 5: 113/159 Loss: 0.253423
2022-12-28 11:43: Train Epoch 5: 114/159 Loss: 0.312329
2022-12-28 11:44: Train Epoch 5: 115/159 Loss: 0.234023
2022-12-28 11:45: Train Epoch 5: 116/159 Loss: 0.306530
2022-12-28 11:46: Train Epoch 5: 117/159 Loss: 0.348832
2022-12-28 11:46: Train Epoch 5: 118/159 Loss: 0.206601
2022-12-28 11:47: Train Epoch 5: 119/159 Loss: 0.312126
2022-12-28 11:48: Train Epoch 5: 120/159 Loss: 0.400957
2022-12-28 11:49: Train Epoch 5: 121/159 Loss: 0.310381
2022-12-28 11:50: Train Epoch 5: 122/159 Loss: 0.236842
2022-12-28 11:51: Train Epoch 5: 123/159 Loss: 0.272695
2022-12-28 11:52: Train Epoch 5: 124/159 Loss: 0.260925
2022-12-28 11:53: Train Epoch 5: 125/159 Loss: 0.248391
2022-12-28 11:53: Train Epoch 5: 126/159 Loss: 0.240275
2022-12-28 11:54: Train Epoch 5: 127/159 Loss: 0.248946
2022-12-28 11:55: Train Epoch 5: 128/159 Loss: 0.256627
2022-12-28 11:56: Train Epoch 5: 129/159 Loss: 0.257663
2022-12-28 11:57: Train Epoch 5: 130/159 Loss: 0.250579
2022-12-28 11:58: Train Epoch 5: 131/159 Loss: 0.292047
2022-12-28 11:59: Train Epoch 5: 132/159 Loss: 0.285177
2022-12-28 12:00: Train Epoch 5: 133/159 Loss: 0.236048
2022-12-28 12:01: Train Epoch 5: 134/159 Loss: 0.284050
2022-12-28 12:01: Train Epoch 5: 135/159 Loss: 0.282667
2022-12-28 12:02: Train Epoch 5: 136/159 Loss: 0.287286
2022-12-28 12:03: Train Epoch 5: 137/159 Loss: 0.298667
2022-12-28 12:04: Train Epoch 5: 138/159 Loss: 0.280843
2022-12-28 12:05: Train Epoch 5: 139/159 Loss: 0.252687
2022-12-28 12:06: Train Epoch 5: 140/159 Loss: 0.260650
2022-12-28 12:07: Train Epoch 5: 141/159 Loss: 0.263324
2022-12-28 12:08: Train Epoch 5: 142/159 Loss: 0.254212
2022-12-28 12:08: Train Epoch 5: 143/159 Loss: 0.183981
2022-12-28 12:09: Train Epoch 5: 144/159 Loss: 0.257190
2022-12-28 12:10: Train Epoch 5: 145/159 Loss: 0.267697
2022-12-28 12:11: Train Epoch 5: 146/159 Loss: 0.235034
2022-12-28 12:12: Train Epoch 5: 147/159 Loss: 0.259301
2022-12-28 12:13: Train Epoch 5: 148/159 Loss: 0.232967
2022-12-28 12:14: Train Epoch 5: 149/159 Loss: 0.259760
2022-12-28 12:15: Train Epoch 5: 150/159 Loss: 0.276815
2022-12-28 12:16: Train Epoch 5: 151/159 Loss: 0.281938
2022-12-28 12:16: Train Epoch 5: 152/159 Loss: 0.298379
2022-12-28 12:17: Train Epoch 5: 153/159 Loss: 0.230669
2022-12-28 12:18: Train Epoch 5: 154/159 Loss: 0.244070
2022-12-28 12:19: Train Epoch 5: 155/159 Loss: 0.292567
2022-12-28 12:20: Train Epoch 5: 156/159 Loss: 0.257401
2022-12-28 12:21: Train Epoch 5: 157/159 Loss: 0.288241
2022-12-28 12:21: Train Epoch 5: 158/159 Loss: 0.241728
2022-12-28 12:21: **********Train Epoch 5: averaged Loss: 0.277664 
2022-12-28 12:21: 
Epoch time elapsed: 8412.444112300873

2022-12-28 12:25: 
 metrics validation: {'precision': 0.8308400460299195, 'recall': 0.5553846153846154, 'f1-score': 0.6657445827570309, 'support': 1300, 'AUC': 0.8672544378698225, 'AUCPR': 0.7967693793152417, 'TP': 722, 'FP': 147, 'TN': 2453, 'FN': 578} 

2022-12-28 12:25: **********Val Epoch 5: average Loss: 0.565541
2022-12-28 12:29: 
 Testing metrics {'precision': 0.7557781201848999, 'recall': 0.7988599348534202, 'f1-score': 0.7767220902612827, 'support': 1228, 'AUC': 0.880746413755053, 'AUCPR': 0.8153603788852737, 'TP': 981, 'FP': 317, 'TN': 2139, 'FN': 247} 

