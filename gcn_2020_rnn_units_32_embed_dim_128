/home/joel.chacon/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)
  return torch._C._cuda_getDeviceCount() > 0
2022-12-20 12:13: log dir: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/20221220121325
2022-12-20 12:13: Experiment log path in: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/20221220121325
2022-12-20 12:13: Argument: Namespace(batch_size=256, clc='vec', cuda=True, dataset='2020', dataset_root='/home/joel.chacon/tmp/datasets_grl/', debug=False, default_graph=True, device='cpu', dynamic_features=['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh'], early_stop=True, early_stop_patience=8, embed_dim=128, epochs=30, gamma=1.0, grad_norm=False, horizon=1, input_dim=25, lag=10, link_len=2, log_dir='/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/20221220121325', log_step=1, loss_func='nllloss', lr_decay=True, lr_decay_rate=0.1, lr_decay_step='10, 15, 20, 25', lr_init=0.0001, mae_thresh=None, mape_thresh=0.0, max_grad_norm=5, mode='train', model='fire_GCN', nan_fill=0.5, num_layers=1, num_nodes=625, num_workers=12, output_dim=2, patch_height=25, patch_width=25, persistent_workers=True, pin_memory=True, plot=False, positive_weight=0.5, prefetch_factor=2, real_value=True, rnn_units=32, seed=1992, static_features=['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density'], teacher_forcing=False, test_ratio=0.2, val_ratio=0.2, weight_decay=0.01, window_len=10)
2022-12-20 12:13: Argument batch_size: 256
2022-12-20 12:13: Argument clc: 'vec'
2022-12-20 12:13: Argument cuda: True
2022-12-20 12:13: Argument dataset: '2020'
2022-12-20 12:13: Argument dataset_root: '/home/joel.chacon/tmp/datasets_grl/'
2022-12-20 12:13: Argument debug: False
2022-12-20 12:13: Argument default_graph: True
2022-12-20 12:13: Argument device: 'cpu'
2022-12-20 12:13: Argument dynamic_features: ['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh']
2022-12-20 12:13: Argument early_stop: True
2022-12-20 12:13: Argument early_stop_patience: 8
2022-12-20 12:13: Argument embed_dim: 128
2022-12-20 12:13: Argument epochs: 30
2022-12-20 12:13: Argument gamma: 1.0
2022-12-20 12:13: Argument grad_norm: False
2022-12-20 12:13: Argument horizon: 1
2022-12-20 12:13: Argument input_dim: 25
2022-12-20 12:13: Argument lag: 10
2022-12-20 12:13: Argument link_len: 2
2022-12-20 12:13: Argument log_dir: '/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/20221220121325'
2022-12-20 12:13: Argument log_step: 1
2022-12-20 12:13: Argument loss_func: 'nllloss'
2022-12-20 12:13: Argument lr_decay: True
2022-12-20 12:13: Argument lr_decay_rate: 0.1
2022-12-20 12:13: Argument lr_decay_step: '10, 15, 20, 25'
2022-12-20 12:13: Argument lr_init: 0.0001
2022-12-20 12:13: Argument mae_thresh: None
2022-12-20 12:13: Argument mape_thresh: 0.0
2022-12-20 12:13: Argument max_grad_norm: 5
2022-12-20 12:13: Argument mode: 'train'
2022-12-20 12:13: Argument model: 'fire_GCN'
2022-12-20 12:13: Argument nan_fill: 0.5
2022-12-20 12:13: Argument num_layers: 1
2022-12-20 12:13: Argument num_nodes: 625
2022-12-20 12:13: Argument num_workers: 12
2022-12-20 12:13: Argument output_dim: 2
2022-12-20 12:13: Argument patch_height: 25
2022-12-20 12:13: Argument patch_width: 25
2022-12-20 12:13: Argument persistent_workers: True
2022-12-20 12:13: Argument pin_memory: True
2022-12-20 12:13: Argument plot: False
2022-12-20 12:13: Argument positive_weight: 0.5
2022-12-20 12:13: Argument prefetch_factor: 2
2022-12-20 12:13: Argument real_value: True
2022-12-20 12:13: Argument rnn_units: 32
2022-12-20 12:13: Argument seed: 1992
2022-12-20 12:13: Argument static_features: ['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density']
2022-12-20 12:13: Argument teacher_forcing: False
2022-12-20 12:13: Argument test_ratio: 0.2
2022-12-20 12:13: Argument val_ratio: 0.2
2022-12-20 12:13: Argument weight_decay: 0.01
2022-12-20 12:13: Argument window_len: 10
++++++++++++++
2020_fire_GCN.conf
++++++++++++++
*****************Model Parameter*****************
node_embeddings torch.Size([625, 128]) True
ln1.weight torch.Size([25]) True
ln1.bias torch.Size([25]) True
encoder.cell_list.0.gate.weights_pool torch.Size([128, 2, 57, 32]) True
encoder.cell_list.0.gate.weights_window torch.Size([128, 1, 32]) True
encoder.cell_list.0.gate.bias_pool torch.Size([128, 64]) True
encoder.cell_list.0.gate.T torch.Size([10]) True
encoder.cell_list.0.gate.cnn.features.0.weight torch.Size([16, 1, 3, 3]) True
encoder.cell_list.0.gate.cnn.features.0.bias torch.Size([16]) True
encoder.cell_list.0.gate.cnn.features.3.weight torch.Size([32, 16, 3, 3]) True
encoder.cell_list.0.gate.cnn.features.3.bias torch.Size([32]) True
encoder.cell_list.0.update.weights_pool torch.Size([128, 2, 57, 16]) True
encoder.cell_list.0.update.weights_window torch.Size([128, 1, 16]) True
encoder.cell_list.0.update.bias_pool torch.Size([128, 32]) True
encoder.cell_list.0.update.T torch.Size([10]) True
encoder.cell_list.0.update.cnn.features.0.weight torch.Size([8, 1, 3, 3]) True
encoder.cell_list.0.update.cnn.features.0.bias torch.Size([8]) True
encoder.cell_list.0.update.cnn.features.3.weight torch.Size([16, 8, 3, 3]) True
encoder.cell_list.0.update.cnn.features.3.bias torch.Size([16]) True
end_conv.weight torch.Size([2, 1, 625, 32]) True
end_conv.bias torch.Size([2]) True
Total params num: 844968
*****************Finish Parameter****************
Positives: 13518 / Negatives: 27036
Dataset length 40554
Positives: 1300 / Negatives: 2600
Dataset length 3900
Positives: 1228 / Negatives: 2456
Dataset length 3684
Applying learning rate decay.
Creat Log File in:  /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/20221220121325/run.log
2022-12-20 12:14: Train Epoch 1: 0/159 Loss: 0.923057
2022-12-20 12:15: Train Epoch 1: 1/159 Loss: 0.646915
2022-12-20 12:16: Train Epoch 1: 2/159 Loss: 0.642511
2022-12-20 12:17: Train Epoch 1: 3/159 Loss: 0.782708
2022-12-20 12:18: Train Epoch 1: 4/159 Loss: 0.784173
2022-12-20 12:19: Train Epoch 1: 5/159 Loss: 0.721922
2022-12-20 12:20: Train Epoch 1: 6/159 Loss: 0.660153
2022-12-20 12:21: Train Epoch 1: 7/159 Loss: 0.636243
2022-12-20 12:22: Train Epoch 1: 8/159 Loss: 0.671089
2022-12-20 12:23: Train Epoch 1: 9/159 Loss: 0.703709
2022-12-20 12:24: Train Epoch 1: 10/159 Loss: 0.705120
2022-12-20 12:25: Train Epoch 1: 11/159 Loss: 0.667338
2022-12-20 12:26: Train Epoch 1: 12/159 Loss: 0.639883
2022-12-20 12:27: Train Epoch 1: 13/159 Loss: 0.638150
2022-12-20 12:28: Train Epoch 1: 14/159 Loss: 0.613344
2022-12-20 12:29: Train Epoch 1: 15/159 Loss: 0.629607
2022-12-20 12:30: Train Epoch 1: 16/159 Loss: 0.667636
2022-12-20 12:31: Train Epoch 1: 17/159 Loss: 0.627518
2022-12-20 12:32: Train Epoch 1: 18/159 Loss: 0.626010
2022-12-20 12:33: Train Epoch 1: 19/159 Loss: 0.606608
2022-12-20 12:34: Train Epoch 1: 20/159 Loss: 0.629082
2022-12-20 12:35: Train Epoch 1: 21/159 Loss: 0.616328
2022-12-20 12:36: Train Epoch 1: 22/159 Loss: 0.634030
2022-12-20 12:37: Train Epoch 1: 23/159 Loss: 0.630344
2022-12-20 12:38: Train Epoch 1: 24/159 Loss: 0.630302
2022-12-20 12:39: Train Epoch 1: 25/159 Loss: 0.618929
2022-12-20 12:40: Train Epoch 1: 26/159 Loss: 0.603916
2022-12-20 12:41: Train Epoch 1: 27/159 Loss: 0.638285
2022-12-20 12:42: Train Epoch 1: 28/159 Loss: 0.616569
2022-12-20 12:43: Train Epoch 1: 29/159 Loss: 0.591790
2022-12-20 12:44: Train Epoch 1: 30/159 Loss: 0.669066
2022-12-20 12:45: Train Epoch 1: 31/159 Loss: 0.641004
2022-12-20 12:46: Train Epoch 1: 32/159 Loss: 0.596826
2022-12-20 12:47: Train Epoch 1: 33/159 Loss: 0.594619
2022-12-20 12:48: Train Epoch 1: 34/159 Loss: 0.587512
2022-12-20 12:49: Train Epoch 1: 35/159 Loss: 0.627484
2022-12-20 12:50: Train Epoch 1: 36/159 Loss: 0.618693
2022-12-20 12:51: Train Epoch 1: 37/159 Loss: 0.578735
2022-12-20 12:52: Train Epoch 1: 38/159 Loss: 0.604068
2022-12-20 12:53: Train Epoch 1: 39/159 Loss: 0.599766
2022-12-20 12:54: Train Epoch 1: 40/159 Loss: 0.611954
2022-12-20 12:55: Train Epoch 1: 41/159 Loss: 0.611505
2022-12-20 12:56: Train Epoch 1: 42/159 Loss: 0.583794
2022-12-20 12:57: Train Epoch 1: 43/159 Loss: 0.588148
2022-12-20 12:58: Train Epoch 1: 44/159 Loss: 0.572923
2022-12-20 12:59: Train Epoch 1: 45/159 Loss: 0.607817
2022-12-20 13:00: Train Epoch 1: 46/159 Loss: 0.579302
2022-12-20 13:01: Train Epoch 1: 47/159 Loss: 0.593514
2022-12-20 13:02: Train Epoch 1: 48/159 Loss: 0.553269
2022-12-20 13:03: Train Epoch 1: 49/159 Loss: 0.565780
2022-12-20 13:04: Train Epoch 1: 50/159 Loss: 0.611560
2022-12-20 13:05: Train Epoch 1: 51/159 Loss: 0.594375
2022-12-20 13:06: Train Epoch 1: 52/159 Loss: 0.585528
2022-12-20 13:07: Train Epoch 1: 53/159 Loss: 0.582730
2022-12-20 13:08: Train Epoch 1: 54/159 Loss: 0.552520
2022-12-20 13:09: Train Epoch 1: 55/159 Loss: 0.557188
2022-12-20 13:10: Train Epoch 1: 56/159 Loss: 0.580422
2022-12-20 13:11: Train Epoch 1: 57/159 Loss: 0.565396
2022-12-20 13:12: Train Epoch 1: 58/159 Loss: 0.595842
2022-12-20 13:13: Train Epoch 1: 59/159 Loss: 0.553507
2022-12-20 13:14: Train Epoch 1: 60/159 Loss: 0.573763
2022-12-20 13:15: Train Epoch 1: 61/159 Loss: 0.542560
2022-12-20 13:16: Train Epoch 1: 62/159 Loss: 0.585926
2022-12-20 13:17: Train Epoch 1: 63/159 Loss: 0.534601
2022-12-20 13:18: Train Epoch 1: 64/159 Loss: 0.547870
2022-12-20 13:19: Train Epoch 1: 65/159 Loss: 0.554614
2022-12-20 13:20: Train Epoch 1: 66/159 Loss: 0.547196
2022-12-20 13:21: Train Epoch 1: 67/159 Loss: 0.554039
2022-12-20 13:22: Train Epoch 1: 68/159 Loss: 0.519405
2022-12-20 13:23: Train Epoch 1: 69/159 Loss: 0.529854
2022-12-20 13:24: Train Epoch 1: 70/159 Loss: 0.510821
2022-12-20 13:25: Train Epoch 1: 71/159 Loss: 0.498166
2022-12-20 13:27: Train Epoch 1: 72/159 Loss: 0.502738
2022-12-20 13:27: Train Epoch 1: 73/159 Loss: 0.531768
2022-12-20 13:29: Train Epoch 1: 74/159 Loss: 0.535595
2022-12-20 13:30: Train Epoch 1: 75/159 Loss: 0.496929
2022-12-20 13:31: Train Epoch 1: 76/159 Loss: 0.516928
2022-12-20 13:32: Train Epoch 1: 77/159 Loss: 0.517242
2022-12-20 13:33: Train Epoch 1: 78/159 Loss: 0.502904
2022-12-20 13:34: Train Epoch 1: 79/159 Loss: 0.510328
2022-12-20 13:35: Train Epoch 1: 80/159 Loss: 0.507395
2022-12-20 13:36: Train Epoch 1: 81/159 Loss: 0.507346
2022-12-20 13:37: Train Epoch 1: 82/159 Loss: 0.468180
2022-12-20 13:38: Train Epoch 1: 83/159 Loss: 0.494885
2022-12-20 13:39: Train Epoch 1: 84/159 Loss: 0.484294
2022-12-20 13:40: Train Epoch 1: 85/159 Loss: 0.448850
2022-12-20 13:41: Train Epoch 1: 86/159 Loss: 0.506549
2022-12-20 13:42: Train Epoch 1: 87/159 Loss: 0.465904
2022-12-20 13:43: Train Epoch 1: 88/159 Loss: 0.465174
2022-12-20 13:44: Train Epoch 1: 89/159 Loss: 0.436155
2022-12-20 13:45: Train Epoch 1: 90/159 Loss: 0.479641
2022-12-20 13:46: Train Epoch 1: 91/159 Loss: 0.469611
2022-12-20 13:47: Train Epoch 1: 92/159 Loss: 0.437433
2022-12-20 13:48: Train Epoch 1: 93/159 Loss: 0.455867
2022-12-20 13:49: Train Epoch 1: 94/159 Loss: 0.433741
2022-12-20 13:50: Train Epoch 1: 95/159 Loss: 0.446700
2022-12-20 13:52: Train Epoch 1: 96/159 Loss: 0.432228
2022-12-20 13:53: Train Epoch 1: 97/159 Loss: 0.425632
2022-12-20 13:54: Train Epoch 1: 98/159 Loss: 0.440401
2022-12-20 13:55: Train Epoch 1: 99/159 Loss: 0.424913
2022-12-20 13:56: Train Epoch 1: 100/159 Loss: 0.415631
2022-12-20 13:57: Train Epoch 1: 101/159 Loss: 0.393862
2022-12-20 13:58: Train Epoch 1: 102/159 Loss: 0.415520
2022-12-20 13:59: Train Epoch 1: 103/159 Loss: 0.410179
2022-12-20 14:00: Train Epoch 1: 104/159 Loss: 0.427466
2022-12-20 14:01: Train Epoch 1: 105/159 Loss: 0.419220
2022-12-20 14:02: Train Epoch 1: 106/159 Loss: 0.351786
2022-12-20 14:03: Train Epoch 1: 107/159 Loss: 0.424662
2022-12-20 14:05: Train Epoch 1: 108/159 Loss: 0.359292
2022-12-20 14:06: Train Epoch 1: 109/159 Loss: 0.381229
2022-12-20 14:07: Train Epoch 1: 110/159 Loss: 0.425834
2022-12-20 14:08: Train Epoch 1: 111/159 Loss: 0.410768
2022-12-20 14:09: Train Epoch 1: 112/159 Loss: 0.381991
2022-12-20 14:10: Train Epoch 1: 113/159 Loss: 0.367134
2022-12-20 14:11: Train Epoch 1: 114/159 Loss: 0.371121
2022-12-20 14:11: Train Epoch 1: 115/159 Loss: 0.407221
2022-12-20 14:12: Train Epoch 1: 116/159 Loss: 0.428521
2022-12-20 14:13: Train Epoch 1: 117/159 Loss: 0.350095
2022-12-20 14:14: Train Epoch 1: 118/159 Loss: 0.359555
2022-12-20 14:16: Train Epoch 1: 119/159 Loss: 0.382526
2022-12-20 14:16: Train Epoch 1: 120/159 Loss: 0.380696
2022-12-20 14:17: Train Epoch 1: 121/159 Loss: 0.360794
2022-12-20 14:18: Train Epoch 1: 122/159 Loss: 0.365508
2022-12-20 14:19: Train Epoch 1: 123/159 Loss: 0.368401
2022-12-20 14:20: Train Epoch 1: 124/159 Loss: 0.403818
2022-12-20 14:21: Train Epoch 1: 125/159 Loss: 0.374829
2022-12-20 14:22: Train Epoch 1: 126/159 Loss: 0.314555
2022-12-20 14:23: Train Epoch 1: 127/159 Loss: 0.359791
2022-12-20 14:24: Train Epoch 1: 128/159 Loss: 0.384716
2022-12-20 14:25: Train Epoch 1: 129/159 Loss: 0.344710
2022-12-20 14:26: Train Epoch 1: 130/159 Loss: 0.322994
2022-12-20 14:27: Train Epoch 1: 131/159 Loss: 0.378895
2022-12-20 14:28: Train Epoch 1: 132/159 Loss: 0.349781
2022-12-20 14:29: Train Epoch 1: 133/159 Loss: 0.296970
2022-12-20 14:31: Train Epoch 1: 134/159 Loss: 0.318658
2022-12-20 14:32: Train Epoch 1: 135/159 Loss: 0.308298
2022-12-20 14:33: Train Epoch 1: 136/159 Loss: 0.318526
2022-12-20 14:33: Train Epoch 1: 137/159 Loss: 0.347702
2022-12-20 14:34: Train Epoch 1: 138/159 Loss: 0.297803
2022-12-20 14:35: Train Epoch 1: 139/159 Loss: 0.292383
2022-12-20 14:36: Train Epoch 1: 140/159 Loss: 0.336710
2022-12-20 14:37: Train Epoch 1: 141/159 Loss: 0.326623
2022-12-20 14:38: Train Epoch 1: 142/159 Loss: 0.292208
2022-12-20 14:39: Train Epoch 1: 143/159 Loss: 0.323095
2022-12-20 14:40: Train Epoch 1: 144/159 Loss: 0.322171
2022-12-20 14:41: Train Epoch 1: 145/159 Loss: 0.330805
2022-12-20 14:42: Train Epoch 1: 146/159 Loss: 0.350098
2022-12-20 14:43: Train Epoch 1: 147/159 Loss: 0.299469
2022-12-20 14:44: Train Epoch 1: 148/159 Loss: 0.290140
2022-12-20 14:46: Train Epoch 1: 149/159 Loss: 0.323925
2022-12-20 14:47: Train Epoch 1: 150/159 Loss: 0.333357
2022-12-20 14:48: Train Epoch 1: 151/159 Loss: 0.285730
2022-12-20 14:49: Train Epoch 1: 152/159 Loss: 0.333103
2022-12-20 14:49: Train Epoch 1: 153/159 Loss: 0.380818
2022-12-20 14:51: Train Epoch 1: 154/159 Loss: 0.323307
2022-12-20 14:52: Train Epoch 1: 155/159 Loss: 0.320710
2022-12-20 14:53: Train Epoch 1: 156/159 Loss: 0.305686
2022-12-20 14:54: Train Epoch 1: 157/159 Loss: 0.328390
2022-12-20 14:54: Train Epoch 1: 158/159 Loss: 0.313882
2022-12-20 14:54: **********Train Epoch 1: averaged Loss: 0.492490 
2022-12-20 14:54: 
Epoch time elapsed: 9662.573905229568

2022-12-20 15:00: 
 metrics validation: {'precision': 0.7681818181818182, 'recall': 0.52, 'f1-score': 0.6201834862385321, 'support': 1300, 'AUC': 0.8100674556213018, 'AUCPR': 0.6894211460984108, 'TP': 676, 'FP': 204, 'TN': 2396, 'FN': 624} 

2022-12-20 15:00: **********Val Epoch 1: average Loss: 0.556761
2022-12-20 15:00: *********************************Current best model saved!
2022-12-20 15:05: 
 Testing metrics {'precision': 0.8089385474860336, 'recall': 0.5895765472312704, 'f1-score': 0.6820536975977392, 'support': 1228, 'AUC': 0.8528379611454763, 'AUCPR': 0.7538701421875775, 'TP': 724, 'FP': 171, 'TN': 2285, 'FN': 504} 

2022-12-20 15:06: Train Epoch 2: 0/159 Loss: 0.285514
2022-12-20 15:07: Train Epoch 2: 1/159 Loss: 0.345968
2022-12-20 15:08: Train Epoch 2: 2/159 Loss: 0.346031
2022-12-20 15:09: Train Epoch 2: 3/159 Loss: 0.409979
2022-12-20 15:10: Train Epoch 2: 4/159 Loss: 0.371902
2022-12-20 15:11: Train Epoch 2: 5/159 Loss: 0.297357
2022-12-20 15:13: Train Epoch 2: 6/159 Loss: 0.331628
2022-12-20 15:14: Train Epoch 2: 7/159 Loss: 0.305994
2022-12-20 15:15: Train Epoch 2: 8/159 Loss: 0.295838
2022-12-20 15:16: Train Epoch 2: 9/159 Loss: 0.313745
2022-12-20 15:17: Train Epoch 2: 10/159 Loss: 0.258077
2022-12-20 15:18: Train Epoch 2: 11/159 Loss: 0.281809
2022-12-20 15:19: Train Epoch 2: 12/159 Loss: 0.313338
2022-12-20 15:20: Train Epoch 2: 13/159 Loss: 0.326445
2022-12-20 15:21: Train Epoch 2: 14/159 Loss: 0.367245
2022-12-20 15:22: Train Epoch 2: 15/159 Loss: 0.311154
2022-12-20 15:23: Train Epoch 2: 16/159 Loss: 0.326186
2022-12-20 15:24: Train Epoch 2: 17/159 Loss: 0.360115
2022-12-20 15:25: Train Epoch 2: 18/159 Loss: 0.243394
2022-12-20 15:26: Train Epoch 2: 19/159 Loss: 0.301443
2022-12-20 15:27: Train Epoch 2: 20/159 Loss: 0.339342
2022-12-20 15:28: Train Epoch 2: 21/159 Loss: 0.331557
2022-12-20 15:29: Train Epoch 2: 22/159 Loss: 0.280235
2022-12-20 15:30: Train Epoch 2: 23/159 Loss: 0.348739
2022-12-20 15:31: Train Epoch 2: 24/159 Loss: 0.311179
2022-12-20 15:32: Train Epoch 2: 25/159 Loss: 0.307269
2022-12-20 15:33: Train Epoch 2: 26/159 Loss: 0.316958
2022-12-20 15:34: Train Epoch 2: 27/159 Loss: 0.340680
2022-12-20 15:35: Train Epoch 2: 28/159 Loss: 0.368615
2022-12-20 15:36: Train Epoch 2: 29/159 Loss: 0.270694
2022-12-20 15:37: Train Epoch 2: 30/159 Loss: 0.332257
2022-12-20 15:39: Train Epoch 2: 31/159 Loss: 0.346183
2022-12-20 15:40: Train Epoch 2: 32/159 Loss: 0.349357
2022-12-20 15:41: Train Epoch 2: 33/159 Loss: 0.295568
2022-12-20 15:42: Train Epoch 2: 34/159 Loss: 0.336980
2022-12-20 15:43: Train Epoch 2: 35/159 Loss: 0.358077
2022-12-20 15:44: Train Epoch 2: 36/159 Loss: 0.294141
2022-12-20 15:45: Train Epoch 2: 37/159 Loss: 0.295511
2022-12-20 15:46: Train Epoch 2: 38/159 Loss: 0.263639
2022-12-20 15:47: Train Epoch 2: 39/159 Loss: 0.342106
2022-12-20 15:48: Train Epoch 2: 40/159 Loss: 0.349009
2022-12-20 15:49: Train Epoch 2: 41/159 Loss: 0.336910
2022-12-20 15:50: Train Epoch 2: 42/159 Loss: 0.314886
2022-12-20 15:51: Train Epoch 2: 43/159 Loss: 0.267002
2022-12-20 15:52: Train Epoch 2: 44/159 Loss: 0.322664
2022-12-20 15:53: Train Epoch 2: 45/159 Loss: 0.298278
2022-12-20 15:54: Train Epoch 2: 46/159 Loss: 0.325633
2022-12-20 15:55: Train Epoch 2: 47/159 Loss: 0.297384
2022-12-20 15:56: Train Epoch 2: 48/159 Loss: 0.310691
2022-12-20 15:57: Train Epoch 2: 49/159 Loss: 0.306880
2022-12-20 15:58: Train Epoch 2: 50/159 Loss: 0.316710
2022-12-20 15:59: Train Epoch 2: 51/159 Loss: 0.339545
2022-12-20 16:00: Train Epoch 2: 52/159 Loss: 0.266517
2022-12-20 16:01: Train Epoch 2: 53/159 Loss: 0.279831
2022-12-20 16:02: Train Epoch 2: 54/159 Loss: 0.287788
2022-12-20 16:03: Train Epoch 2: 55/159 Loss: 0.303019
2022-12-20 16:04: Train Epoch 2: 56/159 Loss: 0.368977
2022-12-20 16:05: Train Epoch 2: 57/159 Loss: 0.247162
2022-12-20 16:06: Train Epoch 2: 58/159 Loss: 0.333689
2022-12-20 16:07: Train Epoch 2: 59/159 Loss: 0.282226
2022-12-20 16:08: Train Epoch 2: 60/159 Loss: 0.286546
2022-12-20 16:09: Train Epoch 2: 61/159 Loss: 0.315117
2022-12-20 16:10: Train Epoch 2: 62/159 Loss: 0.273534
2022-12-20 16:11: Train Epoch 2: 63/159 Loss: 0.432746
2022-12-20 16:12: Train Epoch 2: 64/159 Loss: 0.305103
2022-12-20 16:14: Train Epoch 2: 65/159 Loss: 0.324806
2022-12-20 16:15: Train Epoch 2: 66/159 Loss: 0.284503
2022-12-20 16:16: Train Epoch 2: 67/159 Loss: 0.275828
2022-12-20 16:17: Train Epoch 2: 68/159 Loss: 0.329815
2022-12-20 16:18: Train Epoch 2: 69/159 Loss: 0.320968
2022-12-20 16:19: Train Epoch 2: 70/159 Loss: 0.274691
2022-12-20 16:20: Train Epoch 2: 71/159 Loss: 0.260893
2022-12-20 16:21: Train Epoch 2: 72/159 Loss: 0.262731
2022-12-20 16:22: Train Epoch 2: 73/159 Loss: 0.258699
2022-12-20 16:23: Train Epoch 2: 74/159 Loss: 0.275950
2022-12-20 16:24: Train Epoch 2: 75/159 Loss: 0.321672
2022-12-20 16:25: Train Epoch 2: 76/159 Loss: 0.329111
2022-12-20 16:26: Train Epoch 2: 77/159 Loss: 0.287046
2022-12-20 16:27: Train Epoch 2: 78/159 Loss: 0.273827
2022-12-20 16:28: Train Epoch 2: 79/159 Loss: 0.291376
2022-12-20 16:29: Train Epoch 2: 80/159 Loss: 0.293534
2022-12-20 16:30: Train Epoch 2: 81/159 Loss: 0.286981
2022-12-20 16:31: Train Epoch 2: 82/159 Loss: 0.315523
2022-12-20 16:32: Train Epoch 2: 83/159 Loss: 0.235958
2022-12-20 16:33: Train Epoch 2: 84/159 Loss: 0.309170
2022-12-20 16:34: Train Epoch 2: 85/159 Loss: 0.267891
2022-12-20 16:35: Train Epoch 2: 86/159 Loss: 0.384423
2022-12-20 16:36: Train Epoch 2: 87/159 Loss: 0.279383
2022-12-20 16:37: Train Epoch 2: 88/159 Loss: 0.280438
2022-12-20 16:38: Train Epoch 2: 89/159 Loss: 0.290414
2022-12-20 16:40: Train Epoch 2: 90/159 Loss: 0.329102
2022-12-20 16:41: Train Epoch 2: 91/159 Loss: 0.295693
2022-12-20 16:42: Train Epoch 2: 92/159 Loss: 0.318084
2022-12-20 16:43: Train Epoch 2: 93/159 Loss: 0.339662
2022-12-20 16:44: Train Epoch 2: 94/159 Loss: 0.278535
2022-12-20 16:45: Train Epoch 2: 95/159 Loss: 0.330768
2022-12-20 16:46: Train Epoch 2: 96/159 Loss: 0.272251
2022-12-20 16:47: Train Epoch 2: 97/159 Loss: 0.288648
2022-12-20 16:48: Train Epoch 2: 98/159 Loss: 0.375296
2022-12-20 16:49: Train Epoch 2: 99/159 Loss: 0.245269
2022-12-20 16:50: Train Epoch 2: 100/159 Loss: 0.262209
2022-12-20 16:51: Train Epoch 2: 101/159 Loss: 0.330534
2022-12-20 16:52: Train Epoch 2: 102/159 Loss: 0.277034
2022-12-20 16:53: Train Epoch 2: 103/159 Loss: 0.234826
2022-12-20 16:54: Train Epoch 2: 104/159 Loss: 0.293880
2022-12-20 16:55: Train Epoch 2: 105/159 Loss: 0.279145
2022-12-20 16:56: Train Epoch 2: 106/159 Loss: 0.354965
2022-12-20 16:57: Train Epoch 2: 107/159 Loss: 0.273932
2022-12-20 16:58: Train Epoch 2: 108/159 Loss: 0.330784
2022-12-20 16:59: Train Epoch 2: 109/159 Loss: 0.355622
2022-12-20 17:01: Train Epoch 2: 110/159 Loss: 0.316681
2022-12-20 17:02: Train Epoch 2: 111/159 Loss: 0.227901
2022-12-20 17:03: Train Epoch 2: 112/159 Loss: 0.242165
2022-12-20 17:04: Train Epoch 2: 113/159 Loss: 0.270843
2022-12-20 17:05: Train Epoch 2: 114/159 Loss: 0.347037
2022-12-20 17:06: Train Epoch 2: 115/159 Loss: 0.319005
2022-12-20 17:07: Train Epoch 2: 116/159 Loss: 0.315052
2022-12-20 17:08: Train Epoch 2: 117/159 Loss: 0.241982
2022-12-20 17:09: Train Epoch 2: 118/159 Loss: 0.291285
2022-12-20 17:10: Train Epoch 2: 119/159 Loss: 0.345342
2022-12-20 17:11: Train Epoch 2: 120/159 Loss: 0.232690
2022-12-20 17:12: Train Epoch 2: 121/159 Loss: 0.308695
2022-12-20 17:13: Train Epoch 2: 122/159 Loss: 0.313553
2022-12-20 17:14: Train Epoch 2: 123/159 Loss: 0.350313
2022-12-20 17:15: Train Epoch 2: 124/159 Loss: 0.301504
2022-12-20 17:16: Train Epoch 2: 125/159 Loss: 0.300949
2022-12-20 17:17: Train Epoch 2: 126/159 Loss: 0.291711
2022-12-20 17:18: Train Epoch 2: 127/159 Loss: 0.366131
2022-12-20 17:19: Train Epoch 2: 128/159 Loss: 0.342580
2022-12-20 17:20: Train Epoch 2: 129/159 Loss: 0.317020
2022-12-20 17:21: Train Epoch 2: 130/159 Loss: 0.263540
2022-12-20 17:22: Train Epoch 2: 131/159 Loss: 0.292549
2022-12-20 17:23: Train Epoch 2: 132/159 Loss: 0.294942
2022-12-20 17:24: Train Epoch 2: 133/159 Loss: 0.289258
2022-12-20 17:25: Train Epoch 2: 134/159 Loss: 0.296590
2022-12-20 17:26: Train Epoch 2: 135/159 Loss: 0.303463
2022-12-20 17:27: Train Epoch 2: 136/159 Loss: 0.308904
2022-12-20 17:28: Train Epoch 2: 137/159 Loss: 0.249774
2022-12-20 17:29: Train Epoch 2: 138/159 Loss: 0.304688
2022-12-20 17:30: Train Epoch 2: 139/159 Loss: 0.295211
2022-12-20 17:31: Train Epoch 2: 140/159 Loss: 0.320377
2022-12-20 17:33: Train Epoch 2: 141/159 Loss: 0.272757
2022-12-20 17:33: Train Epoch 2: 142/159 Loss: 0.273040
2022-12-20 17:35: Train Epoch 2: 143/159 Loss: 0.268053
2022-12-20 17:36: Train Epoch 2: 144/159 Loss: 0.229350
2022-12-20 17:37: Train Epoch 2: 145/159 Loss: 0.306797
2022-12-20 17:38: Train Epoch 2: 146/159 Loss: 0.294138
2022-12-20 17:39: Train Epoch 2: 147/159 Loss: 0.302029
2022-12-20 17:40: Train Epoch 2: 148/159 Loss: 0.275547
2022-12-20 17:41: Train Epoch 2: 149/159 Loss: 0.346986
2022-12-20 17:42: Train Epoch 2: 150/159 Loss: 0.283173
2022-12-20 17:43: Train Epoch 2: 151/159 Loss: 0.303471
2022-12-20 17:44: Train Epoch 2: 152/159 Loss: 0.305442
2022-12-20 17:45: Train Epoch 2: 153/159 Loss: 0.349871
2022-12-20 17:46: Train Epoch 2: 154/159 Loss: 0.240075
2022-12-20 17:47: Train Epoch 2: 155/159 Loss: 0.315931
2022-12-20 17:48: Train Epoch 2: 156/159 Loss: 0.287511
2022-12-20 17:49: Train Epoch 2: 157/159 Loss: 0.279435
2022-12-20 17:49: Train Epoch 2: 158/159 Loss: 0.240357
2022-12-20 17:49: **********Train Epoch 2: averaged Loss: 0.304451 
2022-12-20 17:49: 
Epoch time elapsed: 9841.540870666504

2022-12-20 17:55: 
 metrics validation: {'precision': 0.712, 'recall': 0.6846153846153846, 'f1-score': 0.6980392156862745, 'support': 1300, 'AUC': 0.825105325443787, 'AUCPR': 0.7262040032745458, 'TP': 890, 'FP': 360, 'TN': 2240, 'FN': 410} 

2022-12-20 17:55: **********Val Epoch 2: average Loss: 0.539176
2022-12-20 17:55: *********************************Current best model saved!
2022-12-20 18:01: 
 Testing metrics {'precision': 0.7459227467811159, 'recall': 0.7076547231270358, 'f1-score': 0.7262849979105725, 'support': 1228, 'AUC': 0.8583234304873261, 'AUCPR': 0.769817935136508, 'TP': 869, 'FP': 296, 'TN': 2160, 'FN': 359} 

2022-12-20 18:02: Train Epoch 3: 0/159 Loss: 0.283489
2022-12-20 18:03: Train Epoch 3: 1/159 Loss: 0.285576
2022-12-20 18:04: Train Epoch 3: 2/159 Loss: 0.355767
2022-12-20 18:05: Train Epoch 3: 3/159 Loss: 0.322976
2022-12-20 18:06: Train Epoch 3: 4/159 Loss: 0.227223
2022-12-20 18:07: Train Epoch 3: 5/159 Loss: 0.278311
2022-12-20 18:08: Train Epoch 3: 6/159 Loss: 0.344876
2022-12-20 18:09: Train Epoch 3: 7/159 Loss: 0.263622
2022-12-20 18:11: Train Epoch 3: 8/159 Loss: 0.297782
2022-12-20 18:12: Train Epoch 3: 9/159 Loss: 0.268455
2022-12-20 18:13: Train Epoch 3: 10/159 Loss: 0.318090
2022-12-20 18:14: Train Epoch 3: 11/159 Loss: 0.317572
2022-12-20 18:15: Train Epoch 3: 12/159 Loss: 0.290854
2022-12-20 18:16: Train Epoch 3: 13/159 Loss: 0.322510
2022-12-20 18:17: Train Epoch 3: 14/159 Loss: 0.392467
2022-12-20 18:18: Train Epoch 3: 15/159 Loss: 0.315578
2022-12-20 18:19: Train Epoch 3: 16/159 Loss: 0.299469
2022-12-20 18:20: Train Epoch 3: 17/159 Loss: 0.259750
2022-12-20 18:21: Train Epoch 3: 18/159 Loss: 0.327337
2022-12-20 18:22: Train Epoch 3: 19/159 Loss: 0.300049
2022-12-20 18:23: Train Epoch 3: 20/159 Loss: 0.308236
2022-12-20 18:24: Train Epoch 3: 21/159 Loss: 0.293488
2022-12-20 18:25: Train Epoch 3: 22/159 Loss: 0.317495
2022-12-20 18:26: Train Epoch 3: 23/159 Loss: 0.308733
2022-12-20 18:27: Train Epoch 3: 24/159 Loss: 0.358810
2022-12-20 18:28: Train Epoch 3: 25/159 Loss: 0.297484
2022-12-20 18:29: Train Epoch 3: 26/159 Loss: 0.327671
2022-12-20 18:30: Train Epoch 3: 27/159 Loss: 0.410514
2022-12-20 18:31: Train Epoch 3: 28/159 Loss: 0.329645
2022-12-20 18:32: Train Epoch 3: 29/159 Loss: 0.352720
2022-12-20 18:33: Train Epoch 3: 30/159 Loss: 0.259671
2022-12-20 18:34: Train Epoch 3: 31/159 Loss: 0.358309
2022-12-20 18:35: Train Epoch 3: 32/159 Loss: 0.275896
2022-12-20 18:37: Train Epoch 3: 33/159 Loss: 0.286298
2022-12-20 18:38: Train Epoch 3: 34/159 Loss: 0.308309
2022-12-20 18:39: Train Epoch 3: 35/159 Loss: 0.320164
2022-12-20 18:40: Train Epoch 3: 36/159 Loss: 0.238072
2022-12-20 18:41: Train Epoch 3: 37/159 Loss: 0.266689
2022-12-20 18:42: Train Epoch 3: 38/159 Loss: 0.306318
2022-12-20 18:43: Train Epoch 3: 39/159 Loss: 0.304487
2022-12-20 18:44: Train Epoch 3: 40/159 Loss: 0.303313
2022-12-20 18:45: Train Epoch 3: 41/159 Loss: 0.286698
2022-12-20 18:46: Train Epoch 3: 42/159 Loss: 0.314476
2022-12-20 18:47: Train Epoch 3: 43/159 Loss: 0.268096
2022-12-20 18:48: Train Epoch 3: 44/159 Loss: 0.291870
2022-12-20 18:49: Train Epoch 3: 45/159 Loss: 0.295161
2022-12-20 18:50: Train Epoch 3: 46/159 Loss: 0.261212
2022-12-20 18:51: Train Epoch 3: 47/159 Loss: 0.301617
2022-12-20 18:52: Train Epoch 3: 48/159 Loss: 0.296801
2022-12-20 18:53: Train Epoch 3: 49/159 Loss: 0.320663
2022-12-20 18:54: Train Epoch 3: 50/159 Loss: 0.338276
2022-12-20 18:55: Train Epoch 3: 51/159 Loss: 0.295644
