2022-12-29 18:19: log dir: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122918194927476669118
2022-12-29 18:19: Experiment log path in: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122918194927476669118
2022-12-29 18:19: Argument: Namespace(batch_size=256, clc='vec', cuda=True, dataset='2020', dataset_root='/home/joel.chacon/tmp/datasets_grl/', debug=False, default_graph=True, device='cpu', dynamic_features=['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh'], early_stop=True, early_stop_patience=8, embed_dim=256, epochs=30, grad_norm=False, horizon=1, input_dim=25, lag=10, link_len=2, log_dir='/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122918194927476669118', log_step=1, loss_func='nllloss', lr_decay=True, lr_decay_rate=0.1, lr_decay_step='15, 20', lr_init=0.0005, max_grad_norm=5, minbatch_size=64, mode='train', model='fire_GCN', nan_fill=0.5, num_layers=2, num_nodes=625, num_workers=20, output_dim=2, patch_height=25, patch_width=25, persistent_workers=True, pin_memory=True, plot=False, positive_weight=0.5, prefetch_factor=2, real_value=True, rnn_units=16, seed=10000, static_features=['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density'], teacher_forcing=False, weight_decay=0.0, window_len=10)
2022-12-29 18:19: Argument batch_size: 256
2022-12-29 18:19: Argument clc: 'vec'
2022-12-29 18:19: Argument cuda: True
2022-12-29 18:19: Argument dataset: '2020'
2022-12-29 18:19: Argument dataset_root: '/home/joel.chacon/tmp/datasets_grl/'
2022-12-29 18:19: Argument debug: False
2022-12-29 18:19: Argument default_graph: True
2022-12-29 18:19: Argument device: 'cpu'
2022-12-29 18:19: Argument dynamic_features: ['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh']
2022-12-29 18:19: Argument early_stop: True
2022-12-29 18:19: Argument early_stop_patience: 8
2022-12-29 18:19: Argument embed_dim: 256
2022-12-29 18:19: Argument epochs: 30
2022-12-29 18:19: Argument grad_norm: False
2022-12-29 18:19: Argument horizon: 1
2022-12-29 18:19: Argument input_dim: 25
2022-12-29 18:19: Argument lag: 10
2022-12-29 18:19: Argument link_len: 2
2022-12-29 18:19: Argument log_dir: '/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122918194927476669118'
2022-12-29 18:19: Argument log_step: 1
2022-12-29 18:19: Argument loss_func: 'nllloss'
2022-12-29 18:19: Argument lr_decay: True
2022-12-29 18:19: Argument lr_decay_rate: 0.1
2022-12-29 18:19: Argument lr_decay_step: '15, 20'
2022-12-29 18:19: Argument lr_init: 0.0005
2022-12-29 18:19: Argument max_grad_norm: 5
2022-12-29 18:19: Argument minbatch_size: 64
2022-12-29 18:19: Argument mode: 'train'
2022-12-29 18:19: Argument model: 'fire_GCN'
2022-12-29 18:19: Argument nan_fill: 0.5
2022-12-29 18:19: Argument num_layers: 2
2022-12-29 18:19: Argument num_nodes: 625
2022-12-29 18:19: Argument num_workers: 20
2022-12-29 18:19: Argument output_dim: 2
2022-12-29 18:19: Argument patch_height: 25
2022-12-29 18:19: Argument patch_width: 25
2022-12-29 18:19: Argument persistent_workers: True
2022-12-29 18:19: Argument pin_memory: True
2022-12-29 18:19: Argument plot: False
2022-12-29 18:19: Argument positive_weight: 0.5
2022-12-29 18:19: Argument prefetch_factor: 2
2022-12-29 18:19: Argument real_value: True
2022-12-29 18:19: Argument rnn_units: 16
2022-12-29 18:19: Argument seed: 10000
2022-12-29 18:19: Argument static_features: ['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density']
2022-12-29 18:19: Argument teacher_forcing: False
2022-12-29 18:19: Argument weight_decay: 0.0
2022-12-29 18:19: Argument window_len: 10
++++++++++++++
2020_fire_GCN.conf
++++++++++++++
*****************Model Parameter*****************
node_embeddings torch.Size([625, 256]) True
ln1.weight torch.Size([25]) True
ln1.bias torch.Size([25]) True
encoder.cell_list.0.gate.weights_pool torch.Size([256, 2, 41, 16]) True
encoder.cell_list.0.gate.weights_window torch.Size([256, 1, 16]) True
encoder.cell_list.0.gate.bias_pool torch.Size([256, 32]) True
encoder.cell_list.0.gate.T torch.Size([10]) True
encoder.cell_list.0.update.weights_pool torch.Size([256, 2, 41, 8]) True
encoder.cell_list.0.update.weights_window torch.Size([256, 1, 8]) True
encoder.cell_list.0.update.bias_pool torch.Size([256, 16]) True
encoder.cell_list.0.update.T torch.Size([10]) True
encoder.cell_list.1.gate.weights_pool torch.Size([256, 2, 32, 16]) True
encoder.cell_list.1.gate.weights_window torch.Size([256, 16, 16]) True
encoder.cell_list.1.gate.bias_pool torch.Size([256, 32]) True
encoder.cell_list.1.gate.T torch.Size([10]) True
encoder.cell_list.1.update.weights_pool torch.Size([256, 2, 32, 8]) True
encoder.cell_list.1.update.weights_window torch.Size([256, 16, 8]) True
encoder.cell_list.1.update.bias_pool torch.Size([256, 16]) True
encoder.cell_list.1.update.T torch.Size([10]) True
fc1.weight torch.Size([2, 10000]) True
fc1.bias torch.Size([2]) True
Total params num: 1206140
*****************Finish Parameter****************
Positives: 5201 / Negatives: 10402
Dataset length 15603
Positives: 1300 / Negatives: 2600
Dataset length 3900
Positives: 1228 / Negatives: 2456
Dataset length 3684
Applying learning rate decay.
Creat Log File in:  /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122918194927476669118/run.log
2022-12-29 18:20: Train Epoch 1: 3/244 Loss: 0.363572
2022-12-29 18:20: Train Epoch 1: 7/244 Loss: 7.748627
2022-12-29 18:20: Train Epoch 1: 11/244 Loss: 2.671924
2022-12-29 18:20: Train Epoch 1: 15/244 Loss: 2.540966
2022-12-29 18:20: Train Epoch 1: 19/244 Loss: 1.495956
2022-12-29 18:21: Train Epoch 1: 23/244 Loss: 1.754649
2022-12-29 18:21: Train Epoch 1: 27/244 Loss: 1.773631
2022-12-29 18:21: Train Epoch 1: 31/244 Loss: 0.815890
2022-12-29 18:21: Train Epoch 1: 35/244 Loss: 0.772707
2022-12-29 18:21: Train Epoch 1: 39/244 Loss: 0.943279
2022-12-29 18:22: Train Epoch 1: 43/244 Loss: 1.152984
2022-12-29 18:22: Train Epoch 1: 47/244 Loss: 0.851967
2022-12-29 18:22: Train Epoch 1: 51/244 Loss: 0.698106
2022-12-29 18:22: Train Epoch 1: 55/244 Loss: 0.469749
2022-12-29 18:22: Train Epoch 1: 59/244 Loss: 0.293399
2022-12-29 18:22: Train Epoch 1: 63/244 Loss: 0.561156
2022-12-29 18:23: Train Epoch 1: 67/244 Loss: 0.719449
2022-12-29 18:23: Train Epoch 1: 71/244 Loss: 0.518907
2022-12-29 18:23: Train Epoch 1: 75/244 Loss: 0.403854
2022-12-29 18:23: Train Epoch 1: 79/244 Loss: 0.296105
2022-12-29 18:23: Train Epoch 1: 83/244 Loss: 0.235775
2022-12-29 18:24: Train Epoch 1: 87/244 Loss: 0.427794
2022-12-29 18:24: Train Epoch 1: 91/244 Loss: 0.473212
2022-12-29 18:24: Train Epoch 1: 95/244 Loss: 0.479295
2022-12-29 18:24: Train Epoch 1: 99/244 Loss: 0.434250
2022-12-29 18:24: Train Epoch 1: 103/244 Loss: 0.324930
2022-12-29 18:24: Train Epoch 1: 107/244 Loss: 0.235990
2022-12-29 18:25: Train Epoch 1: 111/244 Loss: 0.390174
2022-12-29 18:25: Train Epoch 1: 115/244 Loss: 0.419797
2022-12-29 18:25: Train Epoch 1: 119/244 Loss: 0.327451
2022-12-29 18:25: Train Epoch 1: 123/244 Loss: 0.257138
2022-12-29 18:25: Train Epoch 1: 127/244 Loss: 0.280601
2022-12-29 18:26: Train Epoch 1: 131/244 Loss: 0.241118
2022-12-29 18:26: Train Epoch 1: 135/244 Loss: 0.327672
2022-12-29 18:26: Train Epoch 1: 139/244 Loss: 0.360972
2022-12-29 18:26: Train Epoch 1: 143/244 Loss: 0.272694
2022-12-29 18:26: Train Epoch 1: 147/244 Loss: 0.263225
2022-12-29 18:26: Train Epoch 1: 151/244 Loss: 0.274528
2022-12-29 18:27: Train Epoch 1: 155/244 Loss: 0.225259
2022-12-29 18:27: Train Epoch 1: 159/244 Loss: 0.217568
2022-12-29 18:27: Train Epoch 1: 163/244 Loss: 0.242700
2022-12-29 18:27: Train Epoch 1: 167/244 Loss: 0.220168
2022-12-29 18:27: Train Epoch 1: 171/244 Loss: 0.221141
2022-12-29 18:28: Train Epoch 1: 175/244 Loss: 0.257913
2022-12-29 18:28: Train Epoch 1: 179/244 Loss: 0.256452
2022-12-29 18:28: Train Epoch 1: 183/244 Loss: 0.203575
2022-12-29 18:28: Train Epoch 1: 187/244 Loss: 0.281025
2022-12-29 18:28: Train Epoch 1: 191/244 Loss: 0.224468
2022-12-29 18:28: Train Epoch 1: 195/244 Loss: 0.224000
2022-12-29 18:29: Train Epoch 1: 199/244 Loss: 0.184379
2022-12-29 18:29: Train Epoch 1: 203/244 Loss: 0.189554
2022-12-29 18:29: Train Epoch 1: 207/244 Loss: 0.244723
2022-12-29 18:29: Train Epoch 1: 211/244 Loss: 0.198255
2022-12-29 18:29: Train Epoch 1: 215/244 Loss: 0.216429
2022-12-29 18:30: Train Epoch 1: 219/244 Loss: 0.229264
2022-12-29 18:30: Train Epoch 1: 223/244 Loss: 0.243660
2022-12-29 18:30: Train Epoch 1: 227/244 Loss: 0.199771
2022-12-29 18:30: Train Epoch 1: 231/244 Loss: 0.206558
2022-12-29 18:30: Train Epoch 1: 235/244 Loss: 0.239765
2022-12-29 18:31: Train Epoch 1: 239/244 Loss: 0.200758
2022-12-29 18:31: Train Epoch 1: 243/244 Loss: 0.206800
2022-12-29 18:31: **********Train Epoch 1: averaged Loss: 0.623077 
2022-12-29 18:31: 
Epoch time elapsed: 683.6035118103027

2022-12-29 18:32: 
 metrics validation: {'precision': 0.7512116316639742, 'recall': 0.3576923076923077, 'f1-score': 0.48462741010943206, 'support': 1300, 'AUC': 0.7606612426035503, 'AUCPR': 0.6611488649388673, 'TP': 465, 'FP': 154, 'TN': 2446, 'FN': 835} 

2022-12-29 18:32: **********Val Epoch 1: average Loss: 0.394523
2022-12-29 18:32: *********************************Current best model saved!
2022-12-29 18:33: 
 Testing metrics {'precision': 0.8145896656534954, 'recall': 0.4364820846905538, 'f1-score': 0.5683987274655355, 'support': 1228, 'AUC': 0.8416024307950216, 'AUCPR': 0.7552920513442553, 'TP': 536, 'FP': 122, 'TN': 2334, 'FN': 692} 

2022-12-29 18:33: Train Epoch 2: 3/244 Loss: 0.245806
2022-12-29 18:33: Train Epoch 2: 7/244 Loss: 0.246806
2022-12-29 18:33: Train Epoch 2: 11/244 Loss: 0.278331
2022-12-29 18:33: Train Epoch 2: 15/244 Loss: 0.189255
2022-12-29 18:34: Train Epoch 2: 19/244 Loss: 0.272596
2022-12-29 18:34: Train Epoch 2: 23/244 Loss: 0.228453
2022-12-29 18:34: Train Epoch 2: 27/244 Loss: 0.188612
2022-12-29 18:34: Train Epoch 2: 31/244 Loss: 0.232177
2022-12-29 18:34: Train Epoch 2: 35/244 Loss: 0.210859
2022-12-29 18:35: Train Epoch 2: 39/244 Loss: 0.255270
2022-12-29 18:35: Train Epoch 2: 43/244 Loss: 0.205237
2022-12-29 18:35: Train Epoch 2: 47/244 Loss: 0.215308
2022-12-29 18:35: Train Epoch 2: 51/244 Loss: 0.265985
2022-12-29 18:35: Train Epoch 2: 55/244 Loss: 0.194876
2022-12-29 18:35: Train Epoch 2: 59/244 Loss: 0.219335
2022-12-29 18:36: Train Epoch 2: 63/244 Loss: 0.192453
2022-12-29 18:36: Train Epoch 2: 67/244 Loss: 0.197739
2022-12-29 18:36: Train Epoch 2: 71/244 Loss: 0.200761
2022-12-29 18:36: Train Epoch 2: 75/244 Loss: 0.204982
2022-12-29 18:36: Train Epoch 2: 79/244 Loss: 0.265080
2022-12-29 18:37: Train Epoch 2: 83/244 Loss: 0.204857
2022-12-29 18:37: Train Epoch 2: 87/244 Loss: 0.259773
2022-12-29 18:37: Train Epoch 2: 91/244 Loss: 0.204995
2022-12-29 18:37: Train Epoch 2: 95/244 Loss: 0.223786
2022-12-29 18:37: Train Epoch 2: 99/244 Loss: 0.219887
2022-12-29 18:38: Train Epoch 2: 103/244 Loss: 0.193808
2022-12-29 18:38: Train Epoch 2: 107/244 Loss: 0.233394
2022-12-29 18:38: Train Epoch 2: 111/244 Loss: 0.187581
2022-12-29 18:38: Train Epoch 2: 115/244 Loss: 0.246200
2022-12-29 18:38: Train Epoch 2: 119/244 Loss: 0.185384
2022-12-29 18:39: Train Epoch 2: 123/244 Loss: 0.196633
2022-12-29 18:39: Train Epoch 2: 127/244 Loss: 0.195860
2022-12-29 18:39: Train Epoch 2: 131/244 Loss: 0.185183
2022-12-29 18:39: Train Epoch 2: 135/244 Loss: 0.186398
2022-12-29 18:39: Train Epoch 2: 139/244 Loss: 0.229371
2022-12-29 18:39: Train Epoch 2: 143/244 Loss: 0.176986
2022-12-29 18:40: Train Epoch 2: 147/244 Loss: 0.178346
2022-12-29 18:40: Train Epoch 2: 151/244 Loss: 0.175380
2022-12-29 18:40: Train Epoch 2: 155/244 Loss: 0.224914
2022-12-29 18:40: Train Epoch 2: 159/244 Loss: 0.171316
2022-12-29 18:40: Train Epoch 2: 163/244 Loss: 0.196689
2022-12-29 18:41: Train Epoch 2: 167/244 Loss: 0.196819
2022-12-29 18:41: Train Epoch 2: 171/244 Loss: 0.215275
2022-12-29 18:41: Train Epoch 2: 175/244 Loss: 0.231918
2022-12-29 18:41: Train Epoch 2: 179/244 Loss: 0.187914
2022-12-29 18:41: Train Epoch 2: 183/244 Loss: 0.195391
2022-12-29 18:42: Train Epoch 2: 187/244 Loss: 0.247501
2022-12-29 18:42: Train Epoch 2: 191/244 Loss: 0.164766
2022-12-29 18:42: Train Epoch 2: 195/244 Loss: 0.231329
2022-12-29 18:42: Train Epoch 2: 199/244 Loss: 0.164755
2022-12-29 18:42: Train Epoch 2: 203/244 Loss: 0.207958
2022-12-29 18:43: Train Epoch 2: 207/244 Loss: 0.188493
2022-12-29 18:43: Train Epoch 2: 211/244 Loss: 0.181091
2022-12-29 18:43: Train Epoch 2: 215/244 Loss: 0.182375
2022-12-29 18:43: Train Epoch 2: 219/244 Loss: 0.195889
2022-12-29 18:43: Train Epoch 2: 223/244 Loss: 0.177817
2022-12-29 18:43: Train Epoch 2: 227/244 Loss: 0.172942
2022-12-29 18:44: Train Epoch 2: 231/244 Loss: 0.193527
2022-12-29 18:44: Train Epoch 2: 235/244 Loss: 0.217074
2022-12-29 18:44: Train Epoch 2: 239/244 Loss: 0.226633
2022-12-29 18:44: Train Epoch 2: 243/244 Loss: 0.170637
2022-12-29 18:44: **********Train Epoch 2: averaged Loss: 0.208799 
2022-12-29 18:44: 
Epoch time elapsed: 683.9878950119019

2022-12-29 18:45: 
 metrics validation: {'precision': 0.6154929577464788, 'recall': 0.6723076923076923, 'f1-score': 0.6426470588235292, 'support': 1300, 'AUC': 0.7768357988165682, 'AUCPR': 0.7043864963493052, 'TP': 874, 'FP': 546, 'TN': 2054, 'FN': 426} 

2022-12-29 18:45: **********Val Epoch 2: average Loss: 0.319369
2022-12-29 18:45: *********************************Current best model saved!
2022-12-29 18:46: 
 Testing metrics {'precision': 0.7035139092240117, 'recall': 0.7825732899022801, 'f1-score': 0.7409406322282189, 'support': 1228, 'AUC': 0.8484436837526128, 'AUCPR': 0.787819057535618, 'TP': 961, 'FP': 405, 'TN': 2051, 'FN': 267} 

2022-12-29 18:46: Train Epoch 3: 3/244 Loss: 0.201771
2022-12-29 18:46: Train Epoch 3: 7/244 Loss: 0.153307
2022-12-29 18:46: Train Epoch 3: 11/244 Loss: 0.214138
2022-12-29 18:47: Train Epoch 3: 15/244 Loss: 0.180165
2022-12-29 18:47: Train Epoch 3: 19/244 Loss: 0.231886
2022-12-29 18:47: Train Epoch 3: 23/244 Loss: 0.207716
2022-12-29 18:47: Train Epoch 3: 27/244 Loss: 0.197345
2022-12-29 18:47: Train Epoch 3: 31/244 Loss: 0.203012
2022-12-29 18:48: Train Epoch 3: 35/244 Loss: 0.221411
2022-12-29 18:48: Train Epoch 3: 39/244 Loss: 0.184524
2022-12-29 18:48: Train Epoch 3: 43/244 Loss: 0.185188
2022-12-29 18:48: Train Epoch 3: 47/244 Loss: 0.210171
2022-12-29 18:48: Train Epoch 3: 51/244 Loss: 0.213335
2022-12-29 18:48: Train Epoch 3: 55/244 Loss: 0.183807
2022-12-29 18:49: Train Epoch 3: 59/244 Loss: 0.225914
2022-12-29 18:49: Train Epoch 3: 63/244 Loss: 0.202580
2022-12-29 18:49: Train Epoch 3: 67/244 Loss: 0.172710
2022-12-29 18:49: Train Epoch 3: 71/244 Loss: 0.202590
2022-12-29 18:49: Train Epoch 3: 75/244 Loss: 0.168963
2022-12-29 18:50: Train Epoch 3: 79/244 Loss: 0.211718
2022-12-29 18:50: Train Epoch 3: 83/244 Loss: 0.221003
2022-12-29 18:50: Train Epoch 3: 87/244 Loss: 0.198169
2022-12-29 18:50: Train Epoch 3: 91/244 Loss: 0.196085
2022-12-29 18:50: Train Epoch 3: 95/244 Loss: 0.148414
2022-12-29 18:51: Train Epoch 3: 99/244 Loss: 0.140348
2022-12-29 18:51: Train Epoch 3: 103/244 Loss: 0.223564
2022-12-29 18:51: Train Epoch 3: 107/244 Loss: 0.188199
2022-12-29 18:51: Train Epoch 3: 111/244 Loss: 0.194114
2022-12-29 18:51: Train Epoch 3: 115/244 Loss: 0.201086
2022-12-29 18:52: Train Epoch 3: 119/244 Loss: 0.204094
2022-12-29 18:52: Train Epoch 3: 123/244 Loss: 0.176581
2022-12-29 18:52: Train Epoch 3: 127/244 Loss: 0.262939
2022-12-29 18:52: Train Epoch 3: 131/244 Loss: 0.171232
2022-12-29 18:52: Train Epoch 3: 135/244 Loss: 0.201311
2022-12-29 18:52: Train Epoch 3: 139/244 Loss: 0.214488
2022-12-29 18:53: Train Epoch 3: 143/244 Loss: 0.205054
2022-12-29 18:53: Train Epoch 3: 147/244 Loss: 0.187363
2022-12-29 18:53: Train Epoch 3: 151/244 Loss: 0.179082
2022-12-29 18:53: Train Epoch 3: 155/244 Loss: 0.198235
2022-12-29 18:53: Train Epoch 3: 159/244 Loss: 0.214125
2022-12-29 18:54: Train Epoch 3: 163/244 Loss: 0.193962
2022-12-29 18:54: Train Epoch 3: 167/244 Loss: 0.237897
2022-12-29 18:54: Train Epoch 3: 171/244 Loss: 0.176088
2022-12-29 18:54: Train Epoch 3: 175/244 Loss: 0.203816
2022-12-29 18:54: Train Epoch 3: 179/244 Loss: 0.209315
2022-12-29 18:54: Train Epoch 3: 183/244 Loss: 0.216236
2022-12-29 18:55: Train Epoch 3: 187/244 Loss: 0.201842
2022-12-29 18:55: Train Epoch 3: 191/244 Loss: 0.232099
2022-12-29 18:55: Train Epoch 3: 195/244 Loss: 0.208818
2022-12-29 18:55: Train Epoch 3: 199/244 Loss: 0.207205
2022-12-29 18:55: Train Epoch 3: 203/244 Loss: 0.189922
2022-12-29 18:56: Train Epoch 3: 207/244 Loss: 0.196465
2022-12-29 18:56: Train Epoch 3: 211/244 Loss: 0.262081
2022-12-29 18:56: Train Epoch 3: 215/244 Loss: 0.224001
2022-12-29 18:56: Train Epoch 3: 219/244 Loss: 0.191213
2022-12-29 18:56: Train Epoch 3: 223/244 Loss: 0.200148
2022-12-29 18:56: Train Epoch 3: 227/244 Loss: 0.173923
2022-12-29 18:57: Train Epoch 3: 231/244 Loss: 0.248175
2022-12-29 18:57: Train Epoch 3: 235/244 Loss: 0.179904
2022-12-29 18:57: Train Epoch 3: 239/244 Loss: 0.175610
2022-12-29 18:57: Train Epoch 3: 243/244 Loss: 0.158990
2022-12-29 18:57: **********Train Epoch 3: averaged Loss: 0.199761 
2022-12-29 18:57: 
Epoch time elapsed: 681.6162493228912

2022-12-29 18:58: 
 metrics validation: {'precision': 0.710172744721689, 'recall': 0.5692307692307692, 'f1-score': 0.6319385140905209, 'support': 1300, 'AUC': 0.7868857988165681, 'AUCPR': 0.7193164653673562, 'TP': 740, 'FP': 302, 'TN': 2298, 'FN': 560} 

2022-12-29 18:58: **********Val Epoch 3: average Loss: 0.307436
2022-12-29 18:58: *********************************Current best model saved!
2022-12-29 18:59: 
 Testing metrics {'precision': 0.7950897072710104, 'recall': 0.6856677524429967, 'f1-score': 0.7363358111062528, 'support': 1228, 'AUC': 0.8497842152171375, 'AUCPR': 0.7970942869967189, 'TP': 842, 'FP': 217, 'TN': 2239, 'FN': 386} 

2022-12-29 18:59: Train Epoch 4: 3/244 Loss: 0.172906
2022-12-29 18:59: Train Epoch 4: 7/244 Loss: 0.214962
2022-12-29 18:59: Train Epoch 4: 11/244 Loss: 0.204829
2022-12-29 19:00: Train Epoch 4: 15/244 Loss: 0.190457
2022-12-29 19:00: Train Epoch 4: 19/244 Loss: 0.192854
2022-12-29 19:00: Train Epoch 4: 23/244 Loss: 0.189808
2022-12-29 19:00: Train Epoch 4: 27/244 Loss: 0.258746
2022-12-29 19:01: Train Epoch 4: 31/244 Loss: 0.205418
2022-12-29 19:01: Train Epoch 4: 35/244 Loss: 0.262756
2022-12-29 19:01: Train Epoch 4: 39/244 Loss: 0.181562
2022-12-29 19:01: Train Epoch 4: 43/244 Loss: 0.219908
2022-12-29 19:01: Train Epoch 4: 47/244 Loss: 0.198763
2022-12-29 19:01: Train Epoch 4: 51/244 Loss: 0.230296
2022-12-29 19:02: Train Epoch 4: 55/244 Loss: 0.178942
2022-12-29 19:02: Train Epoch 4: 59/244 Loss: 0.151843
2022-12-29 19:02: Train Epoch 4: 63/244 Loss: 0.228076
2022-12-29 19:02: Train Epoch 4: 67/244 Loss: 0.188879
2022-12-29 19:02: Train Epoch 4: 71/244 Loss: 0.297159
2022-12-29 19:03: Train Epoch 4: 75/244 Loss: 0.214259
2022-12-29 19:03: Train Epoch 4: 79/244 Loss: 0.266386
2022-12-29 19:03: Train Epoch 4: 83/244 Loss: 0.235008
2022-12-29 19:03: Train Epoch 4: 87/244 Loss: 0.182571
2022-12-29 19:03: Train Epoch 4: 91/244 Loss: 0.187406
2022-12-29 19:03: Train Epoch 4: 95/244 Loss: 0.138358
2022-12-29 19:04: Train Epoch 4: 99/244 Loss: 0.205268
2022-12-29 19:04: Train Epoch 4: 103/244 Loss: 0.213588
2022-12-29 19:04: Train Epoch 4: 107/244 Loss: 0.197668
2022-12-29 19:04: Train Epoch 4: 111/244 Loss: 0.225351
2022-12-29 19:04: Train Epoch 4: 115/244 Loss: 0.176570
2022-12-29 19:04: Train Epoch 4: 119/244 Loss: 0.216366
2022-12-29 19:05: Train Epoch 4: 123/244 Loss: 0.184787
2022-12-29 19:05: Train Epoch 4: 127/244 Loss: 0.170737
2022-12-29 19:05: Train Epoch 4: 131/244 Loss: 0.150504
2022-12-29 19:05: Train Epoch 4: 135/244 Loss: 0.203410
2022-12-29 19:05: Train Epoch 4: 139/244 Loss: 0.181829
2022-12-29 19:06: Train Epoch 4: 143/244 Loss: 0.193492
2022-12-29 19:06: Train Epoch 4: 147/244 Loss: 0.179280
2022-12-29 19:06: Train Epoch 4: 151/244 Loss: 0.168705
2022-12-29 19:06: Train Epoch 4: 155/244 Loss: 0.206129
2022-12-29 19:06: Train Epoch 4: 159/244 Loss: 0.168872
2022-12-29 19:07: Train Epoch 4: 163/244 Loss: 0.196219
2022-12-29 19:07: Train Epoch 4: 167/244 Loss: 0.150282
2022-12-29 19:07: Train Epoch 4: 171/244 Loss: 0.218473
2022-12-29 19:07: Train Epoch 4: 175/244 Loss: 0.166575
2022-12-29 19:07: Train Epoch 4: 179/244 Loss: 0.191788
2022-12-29 19:08: Train Epoch 4: 183/244 Loss: 0.208904
2022-12-29 19:08: Train Epoch 4: 187/244 Loss: 0.164600
2022-12-29 19:08: Train Epoch 4: 191/244 Loss: 0.157971
2022-12-29 19:08: Train Epoch 4: 195/244 Loss: 0.213566
2022-12-29 19:08: Train Epoch 4: 199/244 Loss: 0.192452
2022-12-29 19:09: Train Epoch 4: 203/244 Loss: 0.160833
2022-12-29 19:09: Train Epoch 4: 207/244 Loss: 0.192937
2022-12-29 19:09: Train Epoch 4: 211/244 Loss: 0.192551
2022-12-29 19:09: Train Epoch 4: 215/244 Loss: 0.192215
2022-12-29 19:09: Train Epoch 4: 219/244 Loss: 0.199481
2022-12-29 19:09: Train Epoch 4: 223/244 Loss: 0.178982
2022-12-29 19:10: Train Epoch 4: 227/244 Loss: 0.160704
2022-12-29 19:10: Train Epoch 4: 231/244 Loss: 0.183666
2022-12-29 19:10: Train Epoch 4: 235/244 Loss: 0.196515
2022-12-29 19:10: Train Epoch 4: 239/244 Loss: 0.238392
2022-12-29 19:10: Train Epoch 4: 243/244 Loss: 0.166475
2022-12-29 19:10: **********Train Epoch 4: averaged Loss: 0.196038 
2022-12-29 19:10: 
Epoch time elapsed: 685.3060312271118

2022-12-29 19:11: 
 metrics validation: {'precision': 0.6276816608996539, 'recall': 0.6976923076923077, 'f1-score': 0.6608378870673953, 'support': 1300, 'AUC': 0.7884789940828404, 'AUCPR': 0.7218515176468902, 'TP': 907, 'FP': 538, 'TN': 2062, 'FN': 393} 

2022-12-29 19:11: **********Val Epoch 4: average Loss: 0.318771
2022-12-29 19:12: 
 Testing metrics {'precision': 0.7950897072710104, 'recall': 0.6856677524429967, 'f1-score': 0.7363358111062528, 'support': 1228, 'AUC': 0.8497842152171375, 'AUCPR': 0.7970942869967189, 'TP': 842, 'FP': 217, 'TN': 2239, 'FN': 386} 

2022-12-29 19:12: Train Epoch 5: 3/244 Loss: 0.181328
2022-12-29 19:12: Train Epoch 5: 7/244 Loss: 0.298799
2022-12-29 19:13: Train Epoch 5: 11/244 Loss: 0.169276
2022-12-29 19:13: Train Epoch 5: 15/244 Loss: 0.224630
2022-12-29 19:13: Train Epoch 5: 19/244 Loss: 0.145829
2022-12-29 19:13: Train Epoch 5: 23/244 Loss: 0.186524
2022-12-29 19:13: Train Epoch 5: 27/244 Loss: 0.211931
2022-12-29 19:14: Train Epoch 5: 31/244 Loss: 0.266050
2022-12-29 19:14: Train Epoch 5: 35/244 Loss: 0.196801
2022-12-29 19:14: Train Epoch 5: 39/244 Loss: 0.225583
2022-12-29 19:14: Train Epoch 5: 43/244 Loss: 0.207283
2022-12-29 19:14: Train Epoch 5: 47/244 Loss: 0.214732
2022-12-29 19:15: Train Epoch 5: 51/244 Loss: 0.233429
2022-12-29 19:15: Train Epoch 5: 55/244 Loss: 0.202416
2022-12-29 19:15: Train Epoch 5: 59/244 Loss: 0.216411
2022-12-29 19:15: Train Epoch 5: 63/244 Loss: 0.181827
2022-12-29 19:15: Train Epoch 5: 67/244 Loss: 0.198135
2022-12-29 19:15: Train Epoch 5: 71/244 Loss: 0.240846
2022-12-29 19:16: Train Epoch 5: 75/244 Loss: 0.190856
2022-12-29 19:16: Train Epoch 5: 79/244 Loss: 0.212991
2022-12-29 19:16: Train Epoch 5: 83/244 Loss: 0.183820
2022-12-29 19:16: Train Epoch 5: 87/244 Loss: 0.188024
2022-12-29 19:16: Train Epoch 5: 91/244 Loss: 0.214444
2022-12-29 19:17: Train Epoch 5: 95/244 Loss: 0.211041
2022-12-29 19:17: Train Epoch 5: 99/244 Loss: 0.183260
2022-12-29 19:17: Train Epoch 5: 103/244 Loss: 0.162768
2022-12-29 19:17: Train Epoch 5: 107/244 Loss: 0.192406
2022-12-29 19:17: Train Epoch 5: 111/244 Loss: 0.227703
2022-12-29 19:18: Train Epoch 5: 115/244 Loss: 0.237270
2022-12-29 19:18: Train Epoch 5: 119/244 Loss: 0.183427
2022-12-29 19:18: Train Epoch 5: 123/244 Loss: 0.217202
2022-12-29 19:18: Train Epoch 5: 127/244 Loss: 0.184574
2022-12-29 19:18: Train Epoch 5: 131/244 Loss: 0.195694
2022-12-29 19:18: Train Epoch 5: 135/244 Loss: 0.176385
2022-12-29 19:19: Train Epoch 5: 139/244 Loss: 0.231514
2022-12-29 19:19: Train Epoch 5: 143/244 Loss: 0.171741
2022-12-29 19:19: Train Epoch 5: 147/244 Loss: 0.194842
2022-12-29 19:19: Train Epoch 5: 151/244 Loss: 0.177065
2022-12-29 19:19: Train Epoch 5: 155/244 Loss: 0.219812
2022-12-29 19:20: Train Epoch 5: 159/244 Loss: 0.167265
2022-12-29 19:20: Train Epoch 5: 163/244 Loss: 0.206291
2022-12-29 19:20: Train Epoch 5: 167/244 Loss: 0.168449
2022-12-29 19:20: Train Epoch 5: 171/244 Loss: 0.218744
2022-12-29 19:20: Train Epoch 5: 175/244 Loss: 0.191014
2022-12-29 19:21: Train Epoch 5: 179/244 Loss: 0.225658
2022-12-29 19:21: Train Epoch 5: 183/244 Loss: 0.199858
2022-12-29 19:21: Train Epoch 5: 187/244 Loss: 0.239630
2022-12-29 19:21: Train Epoch 5: 191/244 Loss: 0.201507
2022-12-29 19:21: Train Epoch 5: 195/244 Loss: 0.210313
2022-12-29 19:22: Train Epoch 5: 199/244 Loss: 0.199746
2022-12-29 19:22: Train Epoch 5: 203/244 Loss: 0.182525
2022-12-29 19:22: Train Epoch 5: 207/244 Loss: 0.203061
2022-12-29 19:22: Train Epoch 5: 211/244 Loss: 0.175849
2022-12-29 19:22: Train Epoch 5: 215/244 Loss: 0.220467
2022-12-29 19:22: Train Epoch 5: 219/244 Loss: 0.173625
2022-12-29 19:23: Train Epoch 5: 223/244 Loss: 0.194430
2022-12-29 19:23: Train Epoch 5: 227/244 Loss: 0.163857
2022-12-29 19:23: Train Epoch 5: 231/244 Loss: 0.174877
2022-12-29 19:23: Train Epoch 5: 235/244 Loss: 0.168983
2022-12-29 19:23: Train Epoch 5: 239/244 Loss: 0.171291
2022-12-29 19:23: Train Epoch 5: 243/244 Loss: 0.157717
2022-12-29 19:23: **********Train Epoch 5: averaged Loss: 0.199571 
2022-12-29 19:23: 
Epoch time elapsed: 685.2477011680603

2022-12-29 19:24: 
 metrics validation: {'precision': 0.8203434610303831, 'recall': 0.4776923076923077, 'f1-score': 0.6037919299951385, 'support': 1300, 'AUC': 0.7996816568047338, 'AUCPR': 0.7334630779415658, 'TP': 621, 'FP': 136, 'TN': 2464, 'FN': 679} 

2022-12-29 19:24: **********Val Epoch 5: average Loss: 0.324788
2022-12-29 19:25: 
 Testing metrics {'precision': 0.7950897072710104, 'recall': 0.6856677524429967, 'f1-score': 0.7363358111062528, 'support': 1228, 'AUC': 0.8497842152171375, 'AUCPR': 0.7970942869967189, 'TP': 842, 'FP': 217, 'TN': 2239, 'FN': 386} 

2022-12-29 19:25: Train Epoch 6: 3/244 Loss: 0.164393
2022-12-29 19:26: Train Epoch 6: 7/244 Loss: 0.203035
2022-12-29 19:26: Train Epoch 6: 11/244 Loss: 0.161462
2022-12-29 19:26: Train Epoch 6: 15/244 Loss: 0.176271
2022-12-29 19:26: Train Epoch 6: 19/244 Loss: 0.189694
2022-12-29 19:26: Train Epoch 6: 23/244 Loss: 0.218004
2022-12-29 19:27: Train Epoch 6: 27/244 Loss: 0.186387
2022-12-29 19:27: Train Epoch 6: 31/244 Loss: 0.219519
2022-12-29 19:27: Train Epoch 6: 35/244 Loss: 0.238275
2022-12-29 19:27: Train Epoch 6: 39/244 Loss: 0.224727
2022-12-29 19:27: Train Epoch 6: 43/244 Loss: 0.159487
2022-12-29 19:27: Train Epoch 6: 47/244 Loss: 0.216133
2022-12-29 19:28: Train Epoch 6: 51/244 Loss: 0.191726
2022-12-29 19:28: Train Epoch 6: 55/244 Loss: 0.195127
2022-12-29 19:28: Train Epoch 6: 59/244 Loss: 0.195344
2022-12-29 19:28: Train Epoch 6: 63/244 Loss: 0.228812
2022-12-29 19:28: Train Epoch 6: 67/244 Loss: 0.195123
2022-12-29 19:29: Train Epoch 6: 71/244 Loss: 0.204822
2022-12-29 19:29: Train Epoch 6: 75/244 Loss: 0.206413
2022-12-29 19:29: Train Epoch 6: 79/244 Loss: 0.161739
2022-12-29 19:29: Train Epoch 6: 83/244 Loss: 0.201237
2022-12-29 19:29: Train Epoch 6: 87/244 Loss: 0.199783
2022-12-29 19:30: Train Epoch 6: 91/244 Loss: 0.302338
2022-12-29 19:30: Train Epoch 6: 95/244 Loss: 0.201577
2022-12-29 19:30: Train Epoch 6: 99/244 Loss: 0.199327
2022-12-29 19:30: Train Epoch 6: 103/244 Loss: 0.244721
2022-12-29 19:30: Train Epoch 6: 107/244 Loss: 0.204732
2022-12-29 19:31: Train Epoch 6: 111/244 Loss: 0.206039
2022-12-29 19:31: Train Epoch 6: 115/244 Loss: 0.176280
2022-12-29 19:31: Train Epoch 6: 119/244 Loss: 0.218828
2022-12-29 19:31: Train Epoch 6: 123/244 Loss: 0.201309
2022-12-29 19:31: Train Epoch 6: 127/244 Loss: 0.180468
2022-12-29 19:32: Train Epoch 6: 131/244 Loss: 0.177983
2022-12-29 19:32: Train Epoch 6: 135/244 Loss: 0.174669
2022-12-29 19:32: Train Epoch 6: 139/244 Loss: 0.156332
2022-12-29 19:32: Train Epoch 6: 143/244 Loss: 0.163409
2022-12-29 19:32: Train Epoch 6: 147/244 Loss: 0.211025
2022-12-29 19:33: Train Epoch 6: 151/244 Loss: 0.183992
2022-12-29 19:33: Train Epoch 6: 155/244 Loss: 0.199481
2022-12-29 19:33: Train Epoch 6: 159/244 Loss: 0.152272
2022-12-29 19:33: Train Epoch 6: 163/244 Loss: 0.184627
2022-12-29 19:33: Train Epoch 6: 167/244 Loss: 0.161959
2022-12-29 19:33: Train Epoch 6: 171/244 Loss: 0.240947
2022-12-29 19:33: Train Epoch 6: 175/244 Loss: 0.162027
2022-12-29 19:34: Train Epoch 6: 179/244 Loss: 0.202692
2022-12-29 19:34: Train Epoch 6: 183/244 Loss: 0.168720
2022-12-29 19:34: Train Epoch 6: 187/244 Loss: 0.172494
2022-12-29 19:34: Train Epoch 6: 191/244 Loss: 0.166698
2022-12-29 19:34: Train Epoch 6: 195/244 Loss: 0.207110
2022-12-29 19:34: Train Epoch 6: 199/244 Loss: 0.247519
2022-12-29 19:35: Train Epoch 6: 203/244 Loss: 0.418744
2022-12-29 19:35: Train Epoch 6: 207/244 Loss: 0.193835
2022-12-29 19:35: Train Epoch 6: 211/244 Loss: 0.295578
2022-12-29 19:35: Train Epoch 6: 215/244 Loss: 0.197079
2022-12-29 19:35: Train Epoch 6: 219/244 Loss: 0.280192
2022-12-29 19:35: Train Epoch 6: 223/244 Loss: 0.268452
2022-12-29 19:35: Train Epoch 6: 227/244 Loss: 0.242522
2022-12-29 19:36: Train Epoch 6: 231/244 Loss: 0.243531
2022-12-29 19:36: Train Epoch 6: 235/244 Loss: 0.263008
2022-12-29 19:36: Train Epoch 6: 239/244 Loss: 0.246592
2022-12-29 19:36: Train Epoch 6: 243/244 Loss: 0.185881
2022-12-29 19:36: **********Train Epoch 6: averaged Loss: 0.207254 
2022-12-29 19:36: 
Epoch time elapsed: 643.1781075000763

2022-12-29 19:37: 
 metrics validation: {'precision': 0.8837209302325582, 'recall': 0.2923076923076923, 'f1-score': 0.4393063583815029, 'support': 1300, 'AUC': 0.7906008875739645, 'AUCPR': 0.7275549437880512, 'TP': 380, 'FP': 50, 'TN': 2550, 'FN': 920} 

2022-12-29 19:37: **********Val Epoch 6: average Loss: 0.401364
2022-12-29 19:37: 
 Testing metrics {'precision': 0.7950897072710104, 'recall': 0.6856677524429967, 'f1-score': 0.7363358111062528, 'support': 1228, 'AUC': 0.8497842152171375, 'AUCPR': 0.7970942869967189, 'TP': 842, 'FP': 217, 'TN': 2239, 'FN': 386} 

2022-12-29 19:37: Train Epoch 7: 3/244 Loss: 0.197150
2022-12-29 19:37: Train Epoch 7: 7/244 Loss: 0.198711
2022-12-29 19:38: Train Epoch 7: 11/244 Loss: 0.150677
2022-12-29 19:38: Train Epoch 7: 15/244 Loss: 0.211037
2022-12-29 19:38: Train Epoch 7: 19/244 Loss: 0.187362
2022-12-29 19:38: Train Epoch 7: 23/244 Loss: 0.188761
2022-12-29 19:38: Train Epoch 7: 27/244 Loss: 0.197305
2022-12-29 19:38: Train Epoch 7: 31/244 Loss: 0.151354
2022-12-29 19:38: Train Epoch 7: 35/244 Loss: 0.189702
2022-12-29 19:38: Train Epoch 7: 39/244 Loss: 0.160229
2022-12-29 19:39: Train Epoch 7: 43/244 Loss: 0.234581
2022-12-29 19:39: Train Epoch 7: 47/244 Loss: 0.173258
2022-12-29 19:39: Train Epoch 7: 51/244 Loss: 0.216478
2022-12-29 19:39: Train Epoch 7: 55/244 Loss: 0.200735
2022-12-29 19:39: Train Epoch 7: 59/244 Loss: 0.193245
2022-12-29 19:39: Train Epoch 7: 63/244 Loss: 0.189510
2022-12-29 19:40: Train Epoch 7: 67/244 Loss: 0.201215
2022-12-29 19:40: Train Epoch 7: 71/244 Loss: 0.183186
2022-12-29 19:40: Train Epoch 7: 75/244 Loss: 0.184307
2022-12-29 19:40: Train Epoch 7: 79/244 Loss: 0.227776
2022-12-29 19:40: Train Epoch 7: 83/244 Loss: 0.208264
2022-12-29 19:41: Train Epoch 7: 87/244 Loss: 0.231705
2022-12-29 19:41: Train Epoch 7: 91/244 Loss: 0.204018
2022-12-29 19:41: Train Epoch 7: 95/244 Loss: 0.268683
2022-12-29 19:41: Train Epoch 7: 99/244 Loss: 0.259378
2022-12-29 19:41: Train Epoch 7: 103/244 Loss: 0.312001
2022-12-29 19:42: Train Epoch 7: 107/244 Loss: 0.222153
2022-12-29 19:42: Train Epoch 7: 111/244 Loss: 0.215998
2022-12-29 19:42: Train Epoch 7: 115/244 Loss: 0.184896
2022-12-29 19:42: Train Epoch 7: 119/244 Loss: 0.197783
2022-12-29 19:42: Train Epoch 7: 123/244 Loss: 0.188180
2022-12-29 19:42: Train Epoch 7: 127/244 Loss: 0.219820
2022-12-29 19:43: Train Epoch 7: 131/244 Loss: 0.182924
2022-12-29 19:43: Train Epoch 7: 135/244 Loss: 0.236786
2022-12-29 19:43: Train Epoch 7: 139/244 Loss: 0.171872
2022-12-29 19:43: Train Epoch 7: 143/244 Loss: 0.185967
2022-12-29 19:43: Train Epoch 7: 147/244 Loss: 0.213524
2022-12-29 19:43: Train Epoch 7: 151/244 Loss: 0.155363
2022-12-29 19:44: Train Epoch 7: 155/244 Loss: 0.193187
2022-12-29 19:44: Train Epoch 7: 159/244 Loss: 0.197301
2022-12-29 19:44: Train Epoch 7: 163/244 Loss: 0.199759
2022-12-29 19:44: Train Epoch 7: 167/244 Loss: 0.199937
2022-12-29 19:44: Train Epoch 7: 171/244 Loss: 0.208745
2022-12-29 19:45: Train Epoch 7: 175/244 Loss: 0.192054
2022-12-29 19:45: Train Epoch 7: 179/244 Loss: 0.180669
2022-12-29 19:45: Train Epoch 7: 183/244 Loss: 0.207750
2022-12-29 19:45: Train Epoch 7: 187/244 Loss: 0.206216
2022-12-29 19:45: Train Epoch 7: 191/244 Loss: 0.209759
2022-12-29 19:45: Train Epoch 7: 195/244 Loss: 0.214728
2022-12-29 19:46: Train Epoch 7: 199/244 Loss: 0.210301
2022-12-29 19:46: Train Epoch 7: 203/244 Loss: 0.215700
2022-12-29 19:46: Train Epoch 7: 207/244 Loss: 0.151298
2022-12-29 19:46: Train Epoch 7: 211/244 Loss: 0.185823
2022-12-29 19:46: Train Epoch 7: 215/244 Loss: 0.200590
2022-12-29 19:47: Train Epoch 7: 219/244 Loss: 0.189847
2022-12-29 19:47: Train Epoch 7: 223/244 Loss: 0.279367
2022-12-29 19:47: Train Epoch 7: 227/244 Loss: 0.197627
2022-12-29 19:47: Train Epoch 7: 231/244 Loss: 0.179034
2022-12-29 19:47: Train Epoch 7: 235/244 Loss: 0.156530
2022-12-29 19:47: Train Epoch 7: 239/244 Loss: 0.190447
2022-12-29 19:48: Train Epoch 7: 243/244 Loss: 0.157728
2022-12-29 19:48: **********Train Epoch 7: averaged Loss: 0.200333 
2022-12-29 19:48: 
Epoch time elapsed: 626.9834730625153

2022-12-29 19:48: 
 metrics validation: {'precision': 0.8274456521739131, 'recall': 0.4684615384615385, 'f1-score': 0.5982318271119843, 'support': 1300, 'AUC': 0.793799704142012, 'AUCPR': 0.72682753516506, 'TP': 609, 'FP': 127, 'TN': 2473, 'FN': 691} 

2022-12-29 19:48: **********Val Epoch 7: average Loss: 0.339317
2022-12-29 19:49: 
 Testing metrics {'precision': 0.7950897072710104, 'recall': 0.6856677524429967, 'f1-score': 0.7363358111062528, 'support': 1228, 'AUC': 0.8497842152171375, 'AUCPR': 0.7970942869967189, 'TP': 842, 'FP': 217, 'TN': 2239, 'FN': 386} 

2022-12-29 19:49: Train Epoch 8: 3/244 Loss: 0.165810
2022-12-29 19:50: Train Epoch 8: 7/244 Loss: 0.172747
2022-12-29 19:50: Train Epoch 8: 11/244 Loss: 0.166288
2022-12-29 19:50: Train Epoch 8: 15/244 Loss: 0.213308
2022-12-29 19:50: Train Epoch 8: 19/244 Loss: 0.195864
2022-12-29 19:50: Train Epoch 8: 23/244 Loss: 0.178530
2022-12-29 19:50: Train Epoch 8: 27/244 Loss: 0.182173
2022-12-29 19:51: Train Epoch 8: 31/244 Loss: 0.163797
2022-12-29 19:51: Train Epoch 8: 35/244 Loss: 0.174422
2022-12-29 19:51: Train Epoch 8: 39/244 Loss: 0.215855
2022-12-29 19:51: Train Epoch 8: 43/244 Loss: 0.180720
2022-12-29 19:51: Train Epoch 8: 47/244 Loss: 0.256294
2022-12-29 19:52: Train Epoch 8: 51/244 Loss: 0.164168
2022-12-29 19:52: Train Epoch 8: 55/244 Loss: 0.180028
2022-12-29 19:52: Train Epoch 8: 59/244 Loss: 0.210738
2022-12-29 19:52: Train Epoch 8: 63/244 Loss: 0.197507
2022-12-29 19:52: Train Epoch 8: 67/244 Loss: 0.198646
2022-12-29 19:53: Train Epoch 8: 71/244 Loss: 0.192817
2022-12-29 19:53: Train Epoch 8: 75/244 Loss: 0.168626
2022-12-29 19:53: Train Epoch 8: 79/244 Loss: 0.187626
2022-12-29 19:53: Train Epoch 8: 83/244 Loss: 0.153944
2022-12-29 19:53: Train Epoch 8: 87/244 Loss: 0.158607
2022-12-29 19:54: Train Epoch 8: 91/244 Loss: 0.180645
2022-12-29 19:54: Train Epoch 8: 95/244 Loss: 0.164940
2022-12-29 19:54: Train Epoch 8: 99/244 Loss: 0.194670
2022-12-29 19:54: Train Epoch 8: 103/244 Loss: 0.204245
2022-12-29 19:54: Train Epoch 8: 107/244 Loss: 0.206227
2022-12-29 19:54: Train Epoch 8: 111/244 Loss: 0.216860
2022-12-29 19:55: Train Epoch 8: 115/244 Loss: 0.193165
2022-12-29 19:55: Train Epoch 8: 119/244 Loss: 0.219350
2022-12-29 19:55: Train Epoch 8: 123/244 Loss: 0.157785
2022-12-29 19:55: Train Epoch 8: 127/244 Loss: 0.209981
2022-12-29 19:55: Train Epoch 8: 131/244 Loss: 0.177110
2022-12-29 19:55: Train Epoch 8: 135/244 Loss: 0.178407
2022-12-29 19:56: Train Epoch 8: 139/244 Loss: 0.185637
2022-12-29 19:56: Train Epoch 8: 143/244 Loss: 0.179142
2022-12-29 19:56: Train Epoch 8: 147/244 Loss: 0.164671
2022-12-29 19:56: Train Epoch 8: 151/244 Loss: 0.156542
2022-12-29 19:56: Train Epoch 8: 155/244 Loss: 0.205303
2022-12-29 19:57: Train Epoch 8: 159/244 Loss: 0.215328
2022-12-29 19:57: Train Epoch 8: 163/244 Loss: 0.169911
2022-12-29 19:57: Train Epoch 8: 167/244 Loss: 0.148950
2022-12-29 19:57: Train Epoch 8: 171/244 Loss: 0.165290
2022-12-29 19:57: Train Epoch 8: 175/244 Loss: 0.190752
2022-12-29 19:57: Train Epoch 8: 179/244 Loss: 0.181416
2022-12-29 19:58: Train Epoch 8: 183/244 Loss: 0.185203
2022-12-29 19:58: Train Epoch 8: 187/244 Loss: 0.183413
2022-12-29 19:58: Train Epoch 8: 191/244 Loss: 0.200090
2022-12-29 19:58: Train Epoch 8: 195/244 Loss: 0.178750
2022-12-29 19:58: Train Epoch 8: 199/244 Loss: 0.283823
2022-12-29 19:58: Train Epoch 8: 203/244 Loss: 0.229223
2022-12-29 19:59: Train Epoch 8: 207/244 Loss: 0.153111
2022-12-29 19:59: Train Epoch 8: 211/244 Loss: 0.251836
2022-12-29 19:59: Train Epoch 8: 215/244 Loss: 0.175765
2022-12-29 19:59: Train Epoch 8: 219/244 Loss: 0.192049
2022-12-29 19:59: Train Epoch 8: 223/244 Loss: 0.174649
2022-12-29 19:59: Train Epoch 8: 227/244 Loss: 0.196349
2022-12-29 20:00: Train Epoch 8: 231/244 Loss: 0.170802
2022-12-29 20:00: Train Epoch 8: 235/244 Loss: 0.188735
2022-12-29 20:00: Train Epoch 8: 239/244 Loss: 0.190084
2022-12-29 20:00: Train Epoch 8: 243/244 Loss: 0.183480
2022-12-29 20:00: **********Train Epoch 8: averaged Loss: 0.188233 
2022-12-29 20:00: 
Epoch time elapsed: 662.932389497757

2022-12-29 20:01: 
 metrics validation: {'precision': 0.899736147757256, 'recall': 0.2623076923076923, 'f1-score': 0.40619416319237645, 'support': 1300, 'AUC': 0.8011609467455622, 'AUCPR': 0.7384774719852659, 'TP': 341, 'FP': 38, 'TN': 2562, 'FN': 959} 

2022-12-29 20:01: **********Val Epoch 8: average Loss: 0.352714
2022-12-29 20:02: 
 Testing metrics {'precision': 0.7950897072710104, 'recall': 0.6856677524429967, 'f1-score': 0.7363358111062528, 'support': 1228, 'AUC': 0.8497842152171375, 'AUCPR': 0.7970942869967189, 'TP': 842, 'FP': 217, 'TN': 2239, 'FN': 386} 

2022-12-29 20:02: Train Epoch 9: 3/244 Loss: 0.187169
2022-12-29 20:02: Train Epoch 9: 7/244 Loss: 0.288820
2022-12-29 20:02: Train Epoch 9: 11/244 Loss: 0.256631
2022-12-29 20:03: Train Epoch 9: 15/244 Loss: 0.185662
2022-12-29 20:03: Train Epoch 9: 19/244 Loss: 0.197684
2022-12-29 20:03: Train Epoch 9: 23/244 Loss: 0.269437
2022-12-29 20:03: Train Epoch 9: 27/244 Loss: 0.211582
2022-12-29 20:03: Train Epoch 9: 31/244 Loss: 0.198735
2022-12-29 20:03: Train Epoch 9: 35/244 Loss: 0.204100
2022-12-29 20:04: Train Epoch 9: 39/244 Loss: 0.253211
2022-12-29 20:04: Train Epoch 9: 43/244 Loss: 0.213828
2022-12-29 20:04: Train Epoch 9: 47/244 Loss: 0.194283
2022-12-29 20:04: Train Epoch 9: 51/244 Loss: 0.188723
2022-12-29 20:04: Train Epoch 9: 55/244 Loss: 0.224057
2022-12-29 20:05: Train Epoch 9: 59/244 Loss: 0.178245
2022-12-29 20:05: Train Epoch 9: 63/244 Loss: 0.180237
2022-12-29 20:05: Train Epoch 9: 67/244 Loss: 0.173501
2022-12-29 20:05: Train Epoch 9: 71/244 Loss: 0.215085
2022-12-29 20:05: Train Epoch 9: 75/244 Loss: 0.215868
2022-12-29 20:06: Train Epoch 9: 79/244 Loss: 0.169850
2022-12-29 20:06: Train Epoch 9: 83/244 Loss: 0.191956
2022-12-29 20:06: Train Epoch 9: 87/244 Loss: 0.170364
2022-12-29 20:06: Train Epoch 9: 91/244 Loss: 0.184899
2022-12-29 20:06: Train Epoch 9: 95/244 Loss: 0.187224
2022-12-29 20:06: Train Epoch 9: 99/244 Loss: 0.198068
2022-12-29 20:07: Train Epoch 9: 103/244 Loss: 0.212487
2022-12-29 20:07: Train Epoch 9: 107/244 Loss: 0.191053
2022-12-29 20:07: Train Epoch 9: 111/244 Loss: 0.182806
2022-12-29 20:07: Train Epoch 9: 115/244 Loss: 0.221415
2022-12-29 20:07: Train Epoch 9: 119/244 Loss: 0.182836
2022-12-29 20:08: Train Epoch 9: 123/244 Loss: 0.290716
2022-12-29 20:08: Train Epoch 9: 127/244 Loss: 0.229450
2022-12-29 20:08: Train Epoch 9: 131/244 Loss: 0.233088
2022-12-29 20:08: Train Epoch 9: 135/244 Loss: 0.207241
2022-12-29 20:08: Train Epoch 9: 139/244 Loss: 0.216347
2022-12-29 20:09: Train Epoch 9: 143/244 Loss: 0.214277
2022-12-29 20:09: Train Epoch 9: 147/244 Loss: 0.245185
2022-12-29 20:09: Train Epoch 9: 151/244 Loss: 0.230774
2022-12-29 20:09: Train Epoch 9: 155/244 Loss: 0.204657
2022-12-29 20:09: Train Epoch 9: 159/244 Loss: 0.184511
2022-12-29 20:09: Train Epoch 9: 163/244 Loss: 0.250576
2022-12-29 20:10: Train Epoch 9: 167/244 Loss: 0.185495
2022-12-29 20:10: Train Epoch 9: 171/244 Loss: 0.300668
2022-12-29 20:10: Train Epoch 9: 175/244 Loss: 0.184305
2022-12-29 20:10: Train Epoch 9: 179/244 Loss: 0.392403
2022-12-29 20:10: Train Epoch 9: 183/244 Loss: 0.181295
2022-12-29 20:11: Train Epoch 9: 187/244 Loss: 0.292837
2022-12-29 20:11: Train Epoch 9: 191/244 Loss: 0.230367
2022-12-29 20:11: Train Epoch 9: 195/244 Loss: 0.238535
2022-12-29 20:11: Train Epoch 9: 199/244 Loss: 0.217876
2022-12-29 20:11: Train Epoch 9: 203/244 Loss: 0.228454
2022-12-29 20:11: Train Epoch 9: 207/244 Loss: 0.241117
2022-12-29 20:12: Train Epoch 9: 211/244 Loss: 0.195387
2022-12-29 20:12: Train Epoch 9: 215/244 Loss: 0.235423
2022-12-29 20:12: Train Epoch 9: 219/244 Loss: 0.210349
2022-12-29 20:12: Train Epoch 9: 223/244 Loss: 0.177013
2022-12-29 20:12: Train Epoch 9: 227/244 Loss: 0.183175
2022-12-29 20:12: Train Epoch 9: 231/244 Loss: 0.177267
2022-12-29 20:13: Train Epoch 9: 235/244 Loss: 0.197530
2022-12-29 20:13: Train Epoch 9: 239/244 Loss: 0.246290
2022-12-29 20:13: Train Epoch 9: 243/244 Loss: 0.169471
2022-12-29 20:13: **********Train Epoch 9: averaged Loss: 0.215113 
2022-12-29 20:13: 
Epoch time elapsed: 671.0912890434265

2022-12-29 20:14: 
 metrics validation: {'precision': 0.5863579474342928, 'recall': 0.7207692307692307, 'f1-score': 0.6466528640441684, 'support': 1300, 'AUC': 0.7973279585798816, 'AUCPR': 0.7325242577068933, 'TP': 937, 'FP': 661, 'TN': 1939, 'FN': 363} 

2022-12-29 20:14: **********Val Epoch 9: average Loss: 0.296559
2022-12-29 20:14: *********************************Current best model saved!
2022-12-29 20:15: 
 Testing metrics {'precision': 0.6613965744400527, 'recall': 0.8175895765472313, 'f1-score': 0.7312454479242535, 'support': 1228, 'AUC': 0.8538983172235248, 'AUCPR': 0.8045587923422383, 'TP': 1004, 'FP': 514, 'TN': 1942, 'FN': 224} 

2022-12-29 20:15: Train Epoch 10: 3/244 Loss: 0.238008
2022-12-29 20:15: Train Epoch 10: 7/244 Loss: 0.194430
2022-12-29 20:15: Train Epoch 10: 11/244 Loss: 0.256095
2022-12-29 20:16: Train Epoch 10: 15/244 Loss: 0.168400
2022-12-29 20:16: Train Epoch 10: 19/244 Loss: 0.232565
2022-12-29 20:16: Train Epoch 10: 23/244 Loss: 0.173998
2022-12-29 20:16: Train Epoch 10: 27/244 Loss: 0.172831
2022-12-29 20:16: Train Epoch 10: 31/244 Loss: 0.326693
2022-12-29 20:17: Train Epoch 10: 35/244 Loss: 0.163775
2022-12-29 20:17: Train Epoch 10: 39/244 Loss: 0.284863
2022-12-29 20:17: Train Epoch 10: 43/244 Loss: 0.189247
2022-12-29 20:17: Train Epoch 10: 47/244 Loss: 0.235438
2022-12-29 20:17: Train Epoch 10: 51/244 Loss: 0.170163
2022-12-29 20:17: Train Epoch 10: 55/244 Loss: 0.229654
2022-12-29 20:18: Train Epoch 10: 59/244 Loss: 0.216259
2022-12-29 20:18: Train Epoch 10: 63/244 Loss: 0.189375
2022-12-29 20:18: Train Epoch 10: 67/244 Loss: 0.202256
2022-12-29 20:18: Train Epoch 10: 71/244 Loss: 0.194925
2022-12-29 20:18: Train Epoch 10: 75/244 Loss: 0.146893
2022-12-29 20:19: Train Epoch 10: 79/244 Loss: 0.175606
2022-12-29 20:19: Train Epoch 10: 83/244 Loss: 0.157249
2022-12-29 20:19: Train Epoch 10: 87/244 Loss: 0.170580
2022-12-29 20:19: Train Epoch 10: 91/244 Loss: 0.188638
2022-12-29 20:19: Train Epoch 10: 95/244 Loss: 0.165444
2022-12-29 20:19: Train Epoch 10: 99/244 Loss: 0.194299
2022-12-29 20:20: Train Epoch 10: 103/244 Loss: 0.164944
2022-12-29 20:20: Train Epoch 10: 107/244 Loss: 0.197938
2022-12-29 20:20: Train Epoch 10: 111/244 Loss: 0.188911
2022-12-29 20:20: Train Epoch 10: 115/244 Loss: 0.211560
2022-12-29 20:21: Train Epoch 10: 119/244 Loss: 0.159386
2022-12-29 20:21: Train Epoch 10: 123/244 Loss: 0.185172
2022-12-29 20:21: Train Epoch 10: 127/244 Loss: 0.148865
2022-12-29 20:21: Train Epoch 10: 131/244 Loss: 0.197439
2022-12-29 20:21: Train Epoch 10: 135/244 Loss: 0.194456
2022-12-29 20:21: Train Epoch 10: 139/244 Loss: 0.163356
2022-12-29 20:22: Train Epoch 10: 143/244 Loss: 0.165052
2022-12-29 20:22: Train Epoch 10: 147/244 Loss: 0.179750
2022-12-29 20:22: Train Epoch 10: 151/244 Loss: 0.187376
2022-12-29 20:22: Train Epoch 10: 155/244 Loss: 0.193353
2022-12-29 20:22: Train Epoch 10: 159/244 Loss: 0.185512
2022-12-29 20:23: Train Epoch 10: 163/244 Loss: 0.187762
2022-12-29 20:23: Train Epoch 10: 167/244 Loss: 0.188689
2022-12-29 20:23: Train Epoch 10: 171/244 Loss: 0.193798
2022-12-29 20:23: Train Epoch 10: 175/244 Loss: 0.203006
2022-12-29 20:23: Train Epoch 10: 179/244 Loss: 0.203130
2022-12-29 20:24: Train Epoch 10: 183/244 Loss: 0.181879
2022-12-29 20:24: Train Epoch 10: 187/244 Loss: 0.246097
2022-12-29 20:24: Train Epoch 10: 191/244 Loss: 0.201827
2022-12-29 20:24: Train Epoch 10: 195/244 Loss: 0.179889
2022-12-29 20:24: Train Epoch 10: 199/244 Loss: 0.183928
2022-12-29 20:24: Train Epoch 10: 203/244 Loss: 0.175984
2022-12-29 20:25: Train Epoch 10: 207/244 Loss: 0.188471
2022-12-29 20:25: Train Epoch 10: 211/244 Loss: 0.155742
2022-12-29 20:25: Train Epoch 10: 215/244 Loss: 0.189983
2022-12-29 20:25: Train Epoch 10: 219/244 Loss: 0.150190
2022-12-29 20:25: Train Epoch 10: 223/244 Loss: 0.155699
2022-12-29 20:26: Train Epoch 10: 227/244 Loss: 0.154005
2022-12-29 20:26: Train Epoch 10: 231/244 Loss: 0.159645
2022-12-29 20:26: Train Epoch 10: 235/244 Loss: 0.201655
2022-12-29 20:26: Train Epoch 10: 239/244 Loss: 0.154002
2022-12-29 20:26: Train Epoch 10: 243/244 Loss: 0.179517
2022-12-29 20:26: **********Train Epoch 10: averaged Loss: 0.190093 
2022-12-29 20:26: 
Epoch time elapsed: 680.397547006607

2022-12-29 20:27: 
 metrics validation: {'precision': 0.7582304526748971, 'recall': 0.566923076923077, 'f1-score': 0.6487676056338028, 'support': 1300, 'AUC': 0.8123198224852071, 'AUCPR': 0.7476144536827737, 'TP': 737, 'FP': 235, 'TN': 2365, 'FN': 563} 

2022-12-29 20:27: **********Val Epoch 10: average Loss: 0.285277
2022-12-29 20:27: *********************************Current best model saved!
2022-12-29 20:28: 
 Testing metrics {'precision': 0.829979879275654, 'recall': 0.6718241042345277, 'f1-score': 0.7425742574257426, 'support': 1228, 'AUC': 0.8581752193657227, 'AUCPR': 0.8177618787641692, 'TP': 825, 'FP': 169, 'TN': 2287, 'FN': 403} 

2022-12-29 20:28: Train Epoch 11: 3/244 Loss: 0.138414
2022-12-29 20:28: Train Epoch 11: 7/244 Loss: 0.151173
2022-12-29 20:28: Train Epoch 11: 11/244 Loss: 0.183674
2022-12-29 20:28: Train Epoch 11: 15/244 Loss: 0.133328
2022-12-29 20:29: Train Epoch 11: 19/244 Loss: 0.148975
2022-12-29 20:29: Train Epoch 11: 23/244 Loss: 0.147603
2022-12-29 20:29: Train Epoch 11: 27/244 Loss: 0.254269
2022-12-29 20:29: Train Epoch 11: 31/244 Loss: 0.167242
2022-12-29 20:29: Train Epoch 11: 35/244 Loss: 0.186828
2022-12-29 20:30: Train Epoch 11: 39/244 Loss: 0.190003
2022-12-29 20:30: Train Epoch 11: 43/244 Loss: 0.194084
2022-12-29 20:30: Train Epoch 11: 47/244 Loss: 0.138248
2022-12-29 20:30: Train Epoch 11: 51/244 Loss: 0.183409
2022-12-29 20:30: Train Epoch 11: 55/244 Loss: 0.168959
2022-12-29 20:31: Train Epoch 11: 59/244 Loss: 0.203302
2022-12-29 20:31: Train Epoch 11: 63/244 Loss: 0.199144
2022-12-29 20:31: Train Epoch 11: 67/244 Loss: 0.240873
2022-12-29 20:31: Train Epoch 11: 71/244 Loss: 0.206835
2022-12-29 20:31: Train Epoch 11: 75/244 Loss: 0.227549
2022-12-29 20:32: Train Epoch 11: 79/244 Loss: 0.172908
2022-12-29 20:32: Train Epoch 11: 83/244 Loss: 0.200189
2022-12-29 20:32: Train Epoch 11: 87/244 Loss: 0.157319
2022-12-29 20:32: Train Epoch 11: 91/244 Loss: 0.195635
2022-12-29 20:32: Train Epoch 11: 95/244 Loss: 0.181881
2022-12-29 20:32: Train Epoch 11: 99/244 Loss: 0.166628
2022-12-29 20:33: Train Epoch 11: 103/244 Loss: 0.188600
2022-12-29 20:33: Train Epoch 11: 107/244 Loss: 0.166800
2022-12-29 20:33: Train Epoch 11: 111/244 Loss: 0.201979
2022-12-29 20:33: Train Epoch 11: 115/244 Loss: 0.173492
2022-12-29 20:33: Train Epoch 11: 119/244 Loss: 0.220568
2022-12-29 20:34: Train Epoch 11: 123/244 Loss: 0.197729
2022-12-29 20:34: Train Epoch 11: 127/244 Loss: 0.203773
2022-12-29 20:34: Train Epoch 11: 131/244 Loss: 0.126600
2022-12-29 20:34: Train Epoch 11: 135/244 Loss: 0.199700
2022-12-29 20:34: Train Epoch 11: 139/244 Loss: 0.170789
2022-12-29 20:34: Train Epoch 11: 143/244 Loss: 0.173106
2022-12-29 20:35: Train Epoch 11: 147/244 Loss: 0.151134
2022-12-29 20:35: Train Epoch 11: 151/244 Loss: 0.145582
2022-12-29 20:35: Train Epoch 11: 155/244 Loss: 0.157204
2022-12-29 20:35: Train Epoch 11: 159/244 Loss: 0.179604
2022-12-29 20:35: Train Epoch 11: 163/244 Loss: 0.185435
2022-12-29 20:36: Train Epoch 11: 167/244 Loss: 0.182918
2022-12-29 20:36: Train Epoch 11: 171/244 Loss: 0.191931
2022-12-29 20:36: Train Epoch 11: 175/244 Loss: 0.184794
2022-12-29 20:36: Train Epoch 11: 179/244 Loss: 0.173650
2022-12-29 20:36: Train Epoch 11: 183/244 Loss: 0.158793
2022-12-29 20:36: Train Epoch 11: 187/244 Loss: 0.153583
2022-12-29 20:37: Train Epoch 11: 191/244 Loss: 0.171269
2022-12-29 20:37: Train Epoch 11: 195/244 Loss: 0.173582
2022-12-29 20:37: Train Epoch 11: 199/244 Loss: 0.163934
2022-12-29 20:37: Train Epoch 11: 203/244 Loss: 0.165043
2022-12-29 20:37: Train Epoch 11: 207/244 Loss: 0.160027
2022-12-29 20:38: Train Epoch 11: 211/244 Loss: 0.161668
2022-12-29 20:38: Train Epoch 11: 215/244 Loss: 0.187326
2022-12-29 20:38: Train Epoch 11: 219/244 Loss: 0.191697
2022-12-29 20:38: Train Epoch 11: 223/244 Loss: 0.144578
2022-12-29 20:38: Train Epoch 11: 227/244 Loss: 0.209318
2022-12-29 20:38: Train Epoch 11: 231/244 Loss: 0.202621
2022-12-29 20:38: Train Epoch 11: 235/244 Loss: 0.194275
2022-12-29 20:39: Train Epoch 11: 239/244 Loss: 0.153205
2022-12-29 20:39: Train Epoch 11: 243/244 Loss: 0.187405
2022-12-29 20:39: **********Train Epoch 11: averaged Loss: 0.178560 
2022-12-29 20:39: 
Epoch time elapsed: 663.8807106018066

2022-12-29 20:40: 
 metrics validation: {'precision': 0.8299039780521262, 'recall': 0.4653846153846154, 'f1-score': 0.5963528831936915, 'support': 1300, 'AUC': 0.8103236686390533, 'AUCPR': 0.7463051068490139, 'TP': 605, 'FP': 124, 'TN': 2476, 'FN': 695} 

2022-12-29 20:40: **********Val Epoch 11: average Loss: 0.338096
2022-12-29 20:41: 
 Testing metrics {'precision': 0.829979879275654, 'recall': 0.6718241042345277, 'f1-score': 0.7425742574257426, 'support': 1228, 'AUC': 0.8581752193657227, 'AUCPR': 0.8177618787641692, 'TP': 825, 'FP': 169, 'TN': 2287, 'FN': 403} 

2022-12-29 20:41: Train Epoch 12: 3/244 Loss: 0.179872
2022-12-29 20:41: Train Epoch 12: 7/244 Loss: 0.184647
2022-12-29 20:41: Train Epoch 12: 11/244 Loss: 0.161760
2022-12-29 20:41: Train Epoch 12: 15/244 Loss: 0.165174
2022-12-29 20:41: Train Epoch 12: 19/244 Loss: 0.147213
2022-12-29 20:42: Train Epoch 12: 23/244 Loss: 0.176056
2022-12-29 20:42: Train Epoch 12: 27/244 Loss: 0.162190
2022-12-29 20:42: Train Epoch 12: 31/244 Loss: 0.200562
2022-12-29 20:42: Train Epoch 12: 35/244 Loss: 0.181474
2022-12-29 20:42: Train Epoch 12: 39/244 Loss: 0.234470
2022-12-29 20:43: Train Epoch 12: 43/244 Loss: 0.199807
Traceback (most recent call last):
  File "/home/joel.chacon/tmp/WildFire_GCN/Run_Model.py", line 197, in <module>
    trainer.train()
  File "/home/joel.chacon/tmp/WildFire_GCN/Trainer.py", line 132, in train
    epoch_time = time.time()
  File "/home/joel.chacon/tmp/WildFire_GCN/Trainer.py", line 95, in train_epoch
    
  File "/home/joel.chacon/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/joel.chacon/tmp/WildFire_GCN/fire_modules_GCN.py", line 205, in forward
    x, _ = self.encoder(x, self.node_embeddings) #B, T, N, hidden_dim
  File "/home/joel.chacon/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/joel.chacon/tmp/WildFire_GCN/fire_modules_GCN.py", line 138, in forward
    state = self.cell_list[layer_idx](cur_layer_input[:, t, :, :], state, cur_layer_input, node_embeddings)
  File "/home/joel.chacon/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/joel.chacon/tmp/WildFire_GCN/fire_modules_GCN.py", line 102, in forward
    z_r = torch.sigmoid(self.gate(input_and_state, x_full, node_embeddings))
  File "/home/joel.chacon/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/joel.chacon/tmp/WildFire_GCN/fire_modules_GCN.py", line 62, in forward
    weights = torch.einsum('nd,dkio->nkio', node_embeddings, self.weights_pool) #N, link_len, dim_in, hidden_dim/2 : on E
  File "/home/joel.chacon/.local/lib/python3.6/site-packages/torch/functional.py", line 327, in einsum
    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]
KeyboardInterrupt
