/home/joel.chacon/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2022-12-28 00:10: log dir: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122800103483600188058
2022-12-28 00:10: Experiment log path in: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122800103483600188058
2022-12-28 00:10: Argument: Namespace(batch_size=256, clc='vec', cuda=True, dataset='2020', dataset_root='/home/joel.chacon/tmp/datasets_grl/', debug=False, default_graph=True, device='cpu', dynamic_features=['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh'], early_stop=True, early_stop_patience=8, embed_dim=32, epochs=30, grad_norm=False, horizon=1, input_dim=25, lag=10, link_len=3, log_dir='/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122800103483600188058', log_step=1, loss_func='nllloss', lr_decay=True, lr_decay_rate=0.1, lr_decay_step='15, 20', lr_init=0.0005, max_grad_norm=5, mode='train', model='fire_GCN', nan_fill=0.5, num_layers=1, num_nodes=625, num_workers=20, output_dim=2, patch_height=25, patch_width=25, persistent_workers=True, pin_memory=True, plot=False, positive_weight=0.5, prefetch_factor=2, real_value=True, rnn_units=32, seed=10000, static_features=['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density'], teacher_forcing=False, weight_decay=0.0, window_len=10)
2022-12-28 00:10: Argument batch_size: 256
2022-12-28 00:10: Argument clc: 'vec'
2022-12-28 00:10: Argument cuda: True
2022-12-28 00:10: Argument dataset: '2020'
2022-12-28 00:10: Argument dataset_root: '/home/joel.chacon/tmp/datasets_grl/'
2022-12-28 00:10: Argument debug: False
2022-12-28 00:10: Argument default_graph: True
2022-12-28 00:10: Argument device: 'cpu'
2022-12-28 00:10: Argument dynamic_features: ['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh']
2022-12-28 00:10: Argument early_stop: True
2022-12-28 00:10: Argument early_stop_patience: 8
2022-12-28 00:10: Argument embed_dim: 32
2022-12-28 00:10: Argument epochs: 30
2022-12-28 00:10: Argument grad_norm: False
2022-12-28 00:10: Argument horizon: 1
2022-12-28 00:10: Argument input_dim: 25
2022-12-28 00:10: Argument lag: 10
2022-12-28 00:10: Argument link_len: 3
2022-12-28 00:10: Argument log_dir: '/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122800103483600188058'
2022-12-28 00:10: Argument log_step: 1
2022-12-28 00:10: Argument loss_func: 'nllloss'
2022-12-28 00:10: Argument lr_decay: True
2022-12-28 00:10: Argument lr_decay_rate: 0.1
2022-12-28 00:10: Argument lr_decay_step: '15, 20'
2022-12-28 00:10: Argument lr_init: 0.0005
2022-12-28 00:10: Argument max_grad_norm: 5
2022-12-28 00:10: Argument mode: 'train'
2022-12-28 00:10: Argument model: 'fire_GCN'
2022-12-28 00:10: Argument nan_fill: 0.5
2022-12-28 00:10: Argument num_layers: 1
2022-12-28 00:10: Argument num_nodes: 625
2022-12-28 00:10: Argument num_workers: 20
2022-12-28 00:10: Argument output_dim: 2
2022-12-28 00:10: Argument patch_height: 25
2022-12-28 00:10: Argument patch_width: 25
2022-12-28 00:10: Argument persistent_workers: True
2022-12-28 00:10: Argument pin_memory: True
2022-12-28 00:10: Argument plot: False
2022-12-28 00:10: Argument positive_weight: 0.5
2022-12-28 00:10: Argument prefetch_factor: 2
2022-12-28 00:10: Argument real_value: True
2022-12-28 00:10: Argument rnn_units: 32
2022-12-28 00:10: Argument seed: 10000
2022-12-28 00:10: Argument static_features: ['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density']
2022-12-28 00:10: Argument teacher_forcing: False
2022-12-28 00:10: Argument weight_decay: 0.0
2022-12-28 00:10: Argument window_len: 10
++++++++++++++
2020_fire_GCN.conf
++++++++++++++
*****************Model Parameter*****************
node_embeddings torch.Size([625, 32]) True
ln1.weight torch.Size([25]) True
ln1.bias torch.Size([25]) True
encoder.cell_list.0.gate.weights_pool torch.Size([32, 3, 57, 32]) True
encoder.cell_list.0.gate.weights_window torch.Size([32, 1, 32]) True
encoder.cell_list.0.gate.bias_pool torch.Size([32, 64]) True
encoder.cell_list.0.gate.T torch.Size([10]) True
encoder.cell_list.0.update.weights_pool torch.Size([32, 3, 57, 16]) True
encoder.cell_list.0.update.weights_window torch.Size([32, 1, 16]) True
encoder.cell_list.0.update.bias_pool torch.Size([32, 32]) True
encoder.cell_list.0.update.T torch.Size([10]) True
end_conv.weight torch.Size([2, 1, 625, 32]) True
end_conv.bias torch.Size([2]) True
Total params num: 327336
*****************Finish Parameter****************
Positives: 13518 / Negatives: 27036
Dataset length 40554
Positives: 1300 / Negatives: 2600
Dataset length 3900
Positives: 1228 / Negatives: 2456
Dataset length 3684
Applying learning rate decay.
Creat Log File in:  /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122800103483600188058/run.log
2022-12-28 00:12: Train Epoch 1: 0/159 Loss: 0.674922
2022-12-28 00:14: Train Epoch 1: 1/159 Loss: 2.229102
2022-12-28 00:15: Train Epoch 1: 2/159 Loss: 0.949008
2022-12-28 00:17: Train Epoch 1: 3/159 Loss: 1.297690
2022-12-28 00:19: Train Epoch 1: 4/159 Loss: 0.950891
2022-12-28 00:20: Train Epoch 1: 5/159 Loss: 0.680330
2022-12-28 00:22: Train Epoch 1: 6/159 Loss: 0.910737
2022-12-28 00:24: Train Epoch 1: 7/159 Loss: 0.976683
2022-12-28 00:25: Train Epoch 1: 8/159 Loss: 0.702617
2022-12-28 00:27: Train Epoch 1: 9/159 Loss: 0.626862
2022-12-28 00:29: Train Epoch 1: 10/159 Loss: 0.778164
2022-12-28 00:30: Train Epoch 1: 11/159 Loss: 0.850814
2022-12-28 00:32: Train Epoch 1: 12/159 Loss: 0.663799
2022-12-28 00:34: Train Epoch 1: 13/159 Loss: 0.707011
2022-12-28 00:35: Train Epoch 1: 14/159 Loss: 0.816107
2022-12-28 00:37: Train Epoch 1: 15/159 Loss: 0.734139
2022-12-28 00:39: Train Epoch 1: 16/159 Loss: 0.670099
2022-12-28 00:40: Train Epoch 1: 17/159 Loss: 0.647142
2022-12-28 00:42: Train Epoch 1: 18/159 Loss: 0.731946
2022-12-28 00:44: Train Epoch 1: 19/159 Loss: 0.648169
2022-12-28 00:45: Train Epoch 1: 20/159 Loss: 0.601180
2022-12-28 00:47: Train Epoch 1: 21/159 Loss: 0.702350
2022-12-28 00:49: Train Epoch 1: 22/159 Loss: 0.645133
2022-12-28 00:50: Train Epoch 1: 23/159 Loss: 0.616431
2022-12-28 00:52: Train Epoch 1: 24/159 Loss: 0.581140
2022-12-28 00:54: Train Epoch 1: 25/159 Loss: 0.629163
2022-12-28 00:55: Train Epoch 1: 26/159 Loss: 0.621620
2022-12-28 00:57: Train Epoch 1: 27/159 Loss: 0.590595
2022-12-28 00:59: Train Epoch 1: 28/159 Loss: 0.555351
2022-12-28 01:01: Train Epoch 1: 29/159 Loss: 0.604147
2022-12-28 01:02: Train Epoch 1: 30/159 Loss: 0.536197
2022-12-28 01:04: Train Epoch 1: 31/159 Loss: 0.538212
2022-12-28 01:06: Train Epoch 1: 32/159 Loss: 0.532184
2022-12-28 01:07: Train Epoch 1: 33/159 Loss: 0.539696
2022-12-28 01:09: Train Epoch 1: 34/159 Loss: 0.503806
2022-12-28 01:11: Train Epoch 1: 35/159 Loss: 0.475059
2022-12-28 01:12: Train Epoch 1: 36/159 Loss: 0.468009
2022-12-28 01:14: Train Epoch 1: 37/159 Loss: 0.476278
2022-12-28 01:16: Train Epoch 1: 38/159 Loss: 0.449328
2022-12-28 01:18: Train Epoch 1: 39/159 Loss: 0.477447
2022-12-28 01:19: Train Epoch 1: 40/159 Loss: 0.453302
2022-12-28 01:21: Train Epoch 1: 41/159 Loss: 0.430373
2022-12-28 01:23: Train Epoch 1: 42/159 Loss: 0.441179
2022-12-28 01:25: Train Epoch 1: 43/159 Loss: 0.411400
2022-12-28 01:26: Train Epoch 1: 44/159 Loss: 0.389956
2022-12-28 01:28: Train Epoch 1: 45/159 Loss: 0.446221
2022-12-28 01:30: Train Epoch 1: 46/159 Loss: 0.375815
2022-12-28 01:31: Train Epoch 1: 47/159 Loss: 0.394806
2022-12-28 01:33: Train Epoch 1: 48/159 Loss: 0.416228
2022-12-28 01:35: Train Epoch 1: 49/159 Loss: 0.412946
2022-12-28 01:37: Train Epoch 1: 50/159 Loss: 0.373749
2022-12-28 01:38: Train Epoch 1: 51/159 Loss: 0.391753
2022-12-28 01:40: Train Epoch 1: 52/159 Loss: 0.323523
2022-12-28 01:42: Train Epoch 1: 53/159 Loss: 0.354443
2022-12-28 01:44: Train Epoch 1: 54/159 Loss: 0.373399
2022-12-28 01:45: Train Epoch 1: 55/159 Loss: 0.420864
2022-12-28 01:47: Train Epoch 1: 56/159 Loss: 0.336620
2022-12-28 01:49: Train Epoch 1: 57/159 Loss: 0.341455
2022-12-28 01:51: Train Epoch 1: 58/159 Loss: 0.395242
2022-12-28 01:52: Train Epoch 1: 59/159 Loss: 0.340939
2022-12-28 01:54: Train Epoch 1: 60/159 Loss: 0.290707
2022-12-28 01:56: Train Epoch 1: 61/159 Loss: 0.429015
2022-12-28 01:58: Train Epoch 1: 62/159 Loss: 0.346819
2022-12-28 01:59: Train Epoch 1: 63/159 Loss: 0.308511
2022-12-28 02:01: Train Epoch 1: 64/159 Loss: 0.349868
2022-12-28 02:03: Train Epoch 1: 65/159 Loss: 0.281656
2022-12-28 02:04: Train Epoch 1: 66/159 Loss: 0.360777
2022-12-28 02:06: Train Epoch 1: 67/159 Loss: 0.297485
2022-12-28 02:08: Train Epoch 1: 68/159 Loss: 0.391569
2022-12-28 02:10: Train Epoch 1: 69/159 Loss: 0.340822
2022-12-28 02:11: Train Epoch 1: 70/159 Loss: 0.342675
2022-12-28 02:13: Train Epoch 1: 71/159 Loss: 0.315221
2022-12-28 02:15: Train Epoch 1: 72/159 Loss: 0.345510
2022-12-28 02:17: Train Epoch 1: 73/159 Loss: 0.256634
2022-12-28 02:18: Train Epoch 1: 74/159 Loss: 0.335483
2022-12-28 02:20: Train Epoch 1: 75/159 Loss: 0.293254
2022-12-28 02:22: Train Epoch 1: 76/159 Loss: 0.307681
2022-12-28 02:24: Train Epoch 1: 77/159 Loss: 0.323810
2022-12-28 02:25: Train Epoch 1: 78/159 Loss: 0.327142
2022-12-28 02:27: Train Epoch 1: 79/159 Loss: 0.249525
2022-12-28 02:29: Train Epoch 1: 80/159 Loss: 0.261433
2022-12-28 02:31: Train Epoch 1: 81/159 Loss: 0.288739
2022-12-28 02:32: Train Epoch 1: 82/159 Loss: 0.359578
2022-12-28 02:34: Train Epoch 1: 83/159 Loss: 0.263037
2022-12-28 02:36: Train Epoch 1: 84/159 Loss: 0.367393
2022-12-28 02:38: Train Epoch 1: 85/159 Loss: 0.268663
2022-12-28 02:39: Train Epoch 1: 86/159 Loss: 0.227641
2022-12-28 02:41: Train Epoch 1: 87/159 Loss: 0.304654
2022-12-28 02:43: Train Epoch 1: 88/159 Loss: 0.311246
2022-12-28 02:45: Train Epoch 1: 89/159 Loss: 0.307535
2022-12-28 02:46: Train Epoch 1: 90/159 Loss: 0.268502
2022-12-28 02:48: Train Epoch 1: 91/159 Loss: 0.378638
2022-12-28 02:50: Train Epoch 1: 92/159 Loss: 0.272518
2022-12-28 02:52: Train Epoch 1: 93/159 Loss: 0.296617
2022-12-28 02:53: Train Epoch 1: 94/159 Loss: 0.258249
2022-12-28 02:55: Train Epoch 1: 95/159 Loss: 0.356379
2022-12-28 02:57: Train Epoch 1: 96/159 Loss: 0.287866
2022-12-28 02:59: Train Epoch 1: 97/159 Loss: 0.292406
2022-12-28 03:01: Train Epoch 1: 98/159 Loss: 0.290743
2022-12-28 03:02: Train Epoch 1: 99/159 Loss: 0.363783
2022-12-28 03:04: Train Epoch 1: 100/159 Loss: 0.254925
2022-12-28 03:06: Train Epoch 1: 101/159 Loss: 0.328080
2022-12-28 03:08: Train Epoch 1: 102/159 Loss: 0.285967
2022-12-28 03:09: Train Epoch 1: 103/159 Loss: 0.329200
2022-12-28 03:11: Train Epoch 1: 104/159 Loss: 0.231089
2022-12-28 03:13: Train Epoch 1: 105/159 Loss: 0.288804
2022-12-28 03:15: Train Epoch 1: 106/159 Loss: 0.242262
2022-12-28 03:16: Train Epoch 1: 107/159 Loss: 0.275903
2022-12-28 03:18: Train Epoch 1: 108/159 Loss: 0.326979
2022-12-28 03:20: Train Epoch 1: 109/159 Loss: 0.258843
2022-12-28 03:22: Train Epoch 1: 110/159 Loss: 0.278075
2022-12-28 03:24: Train Epoch 1: 111/159 Loss: 0.294282
2022-12-28 03:25: Train Epoch 1: 112/159 Loss: 0.273052
2022-12-28 03:27: Train Epoch 1: 113/159 Loss: 0.313702
2022-12-28 03:29: Train Epoch 1: 114/159 Loss: 0.254090
2022-12-28 03:31: Train Epoch 1: 115/159 Loss: 0.308797
2022-12-28 03:32: Train Epoch 1: 116/159 Loss: 0.300867
2022-12-28 03:34: Train Epoch 1: 117/159 Loss: 0.235540
2022-12-28 03:36: Train Epoch 1: 118/159 Loss: 0.274760
2022-12-28 03:38: Train Epoch 1: 119/159 Loss: 0.277731
2022-12-28 03:39: Train Epoch 1: 120/159 Loss: 0.258852
2022-12-28 03:41: Train Epoch 1: 121/159 Loss: 0.300732
2022-12-28 03:43: Train Epoch 1: 122/159 Loss: 0.295418
2022-12-28 03:45: Train Epoch 1: 123/159 Loss: 0.285769
2022-12-28 03:46: Train Epoch 1: 124/159 Loss: 0.350504
2022-12-28 03:48: Train Epoch 1: 125/159 Loss: 0.284426
2022-12-28 03:50: Train Epoch 1: 126/159 Loss: 0.274258
2022-12-28 03:52: Train Epoch 1: 127/159 Loss: 0.255506
2022-12-28 03:53: Train Epoch 1: 128/159 Loss: 0.252728
2022-12-28 03:55: Train Epoch 1: 129/159 Loss: 0.209298
2022-12-28 03:57: Train Epoch 1: 130/159 Loss: 0.226754
2022-12-28 03:59: Train Epoch 1: 131/159 Loss: 0.368170
2022-12-28 04:00: Train Epoch 1: 132/159 Loss: 0.344469
2022-12-28 04:02: Train Epoch 1: 133/159 Loss: 0.231843
2022-12-28 04:04: Train Epoch 1: 134/159 Loss: 0.270470
2022-12-28 04:06: Train Epoch 1: 135/159 Loss: 0.261900
2022-12-28 04:08: Train Epoch 1: 136/159 Loss: 0.366606
2022-12-28 04:09: Train Epoch 1: 137/159 Loss: 0.365971
2022-12-28 04:11: Train Epoch 1: 138/159 Loss: 0.240910
2022-12-28 04:13: Train Epoch 1: 139/159 Loss: 0.281930
2022-12-28 04:15: Train Epoch 1: 140/159 Loss: 0.395578
2022-12-28 04:16: Train Epoch 1: 141/159 Loss: 0.249501
2022-12-28 04:18: Train Epoch 1: 142/159 Loss: 0.215374
2022-12-28 04:20: Train Epoch 1: 143/159 Loss: 0.329933
2022-12-28 04:22: Train Epoch 1: 144/159 Loss: 0.274928
2022-12-28 04:23: Train Epoch 1: 145/159 Loss: 0.259141
2022-12-28 04:25: Train Epoch 1: 146/159 Loss: 0.338795
2022-12-28 04:27: Train Epoch 1: 147/159 Loss: 0.349689
2022-12-28 04:29: Train Epoch 1: 148/159 Loss: 0.264278
2022-12-28 04:30: Train Epoch 1: 149/159 Loss: 0.326775
2022-12-28 04:32: Train Epoch 1: 150/159 Loss: 0.346349
2022-12-28 04:34: Train Epoch 1: 151/159 Loss: 0.318233
2022-12-28 04:36: Train Epoch 1: 152/159 Loss: 0.293700
2022-12-28 04:37: Train Epoch 1: 153/159 Loss: 0.275631
2022-12-28 04:39: Train Epoch 1: 154/159 Loss: 0.273177
2022-12-28 04:41: Train Epoch 1: 155/159 Loss: 0.344084
2022-12-28 04:43: Train Epoch 1: 156/159 Loss: 0.238542
2022-12-28 04:44: Train Epoch 1: 157/159 Loss: 0.304563
2022-12-28 04:45: Train Epoch 1: 158/159 Loss: 0.302061
2022-12-28 04:45: **********Train Epoch 1: averaged Loss: 0.413795 
2022-12-28 04:45: 
Epoch time elapsed: 16498.05448102951

2022-12-28 04:53: 
 metrics validation: {'precision': 0.75, 'recall': 0.66, 'f1-score': 0.702127659574468, 'support': 1300, 'AUC': 0.8394636094674556, 'AUCPR': 0.7622448394616606, 'TP': 858, 'FP': 286, 'TN': 2314, 'FN': 442} 

2022-12-28 04:53: **********Val Epoch 1: average Loss: 0.553348
2022-12-28 04:53: *********************************Current best model saved!
/home/joel.chacon/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2022-12-28 05:01: 
 Testing metrics {'precision': 0.7988450433108758, 'recall': 0.6758957654723127, 'f1-score': 0.7322452580502866, 'support': 1228, 'AUC': 0.8705410667487188, 'AUCPR': 0.7938557623792589, 'TP': 830, 'FP': 209, 'TN': 2247, 'FN': 398} 

2022-12-28 05:03: Train Epoch 2: 0/159 Loss: 0.246904
2022-12-28 05:04: Train Epoch 2: 1/159 Loss: 0.294943
2022-12-28 05:06: Train Epoch 2: 2/159 Loss: 0.330722
2022-12-28 05:08: Train Epoch 2: 3/159 Loss: 0.337221
2022-12-28 05:09: Train Epoch 2: 4/159 Loss: 0.300906
2022-12-28 05:11: Train Epoch 2: 5/159 Loss: 0.249894
2022-12-28 05:13: Train Epoch 2: 6/159 Loss: 0.298473
2022-12-28 05:15: Train Epoch 2: 7/159 Loss: 0.264279
2022-12-28 05:16: Train Epoch 2: 8/159 Loss: 0.262439
2022-12-28 05:18: Train Epoch 2: 9/159 Loss: 0.295203
2022-12-28 05:20: Train Epoch 2: 10/159 Loss: 0.296896
2022-12-28 05:22: Train Epoch 2: 11/159 Loss: 0.362108
2022-12-28 05:23: Train Epoch 2: 12/159 Loss: 0.291919
2022-12-28 05:25: Train Epoch 2: 13/159 Loss: 0.254735
2022-12-28 05:27: Train Epoch 2: 14/159 Loss: 0.281842
2022-12-28 05:28: Train Epoch 2: 15/159 Loss: 0.268341
2022-12-28 05:30: Train Epoch 2: 16/159 Loss: 0.308613
2022-12-28 05:32: Train Epoch 2: 17/159 Loss: 0.331647
2022-12-28 05:34: Train Epoch 2: 18/159 Loss: 0.306985
2022-12-28 05:35: Train Epoch 2: 19/159 Loss: 0.246862
2022-12-28 05:37: Train Epoch 2: 20/159 Loss: 0.266587
2022-12-28 05:39: Train Epoch 2: 21/159 Loss: 0.284017
2022-12-28 05:41: Train Epoch 2: 22/159 Loss: 0.286486
2022-12-28 05:42: Train Epoch 2: 23/159 Loss: 0.254919
2022-12-28 05:44: Train Epoch 2: 24/159 Loss: 0.253885
2022-12-28 05:46: Train Epoch 2: 25/159 Loss: 0.217703
2022-12-28 05:47: Train Epoch 2: 26/159 Loss: 0.273174
2022-12-28 05:49: Train Epoch 2: 27/159 Loss: 0.259651
2022-12-28 05:51: Train Epoch 2: 28/159 Loss: 0.263484
2022-12-28 05:53: Train Epoch 2: 29/159 Loss: 0.312602
2022-12-28 05:54: Train Epoch 2: 30/159 Loss: 0.305299
2022-12-28 05:56: Train Epoch 2: 31/159 Loss: 0.280392
2022-12-28 05:58: Train Epoch 2: 32/159 Loss: 0.296904
2022-12-28 06:00: Train Epoch 2: 33/159 Loss: 0.249297
2022-12-28 06:01: Train Epoch 2: 34/159 Loss: 0.234450
2022-12-28 06:03: Train Epoch 2: 35/159 Loss: 0.337338
2022-12-28 06:05: Train Epoch 2: 36/159 Loss: 0.270084
2022-12-28 06:06: Train Epoch 2: 37/159 Loss: 0.288660
2022-12-28 06:08: Train Epoch 2: 38/159 Loss: 0.324936
2022-12-28 06:10: Train Epoch 2: 39/159 Loss: 0.239888
2022-12-28 06:12: Train Epoch 2: 40/159 Loss: 0.291274
2022-12-28 06:13: Train Epoch 2: 41/159 Loss: 0.305123
2022-12-28 06:15: Train Epoch 2: 42/159 Loss: 0.252556
2022-12-28 06:17: Train Epoch 2: 43/159 Loss: 0.337489
2022-12-28 06:19: Train Epoch 2: 44/159 Loss: 0.277245
2022-12-28 06:20: Train Epoch 2: 45/159 Loss: 0.286383
2022-12-28 06:22: Train Epoch 2: 46/159 Loss: 0.312467
2022-12-28 06:24: Train Epoch 2: 47/159 Loss: 0.230070
2022-12-28 06:26: Train Epoch 2: 48/159 Loss: 0.214885
2022-12-28 06:27: Train Epoch 2: 49/159 Loss: 0.289108
2022-12-28 06:29: Train Epoch 2: 50/159 Loss: 0.307243
2022-12-28 06:31: Train Epoch 2: 51/159 Loss: 0.304833
2022-12-28 06:32: Train Epoch 2: 52/159 Loss: 0.237454
2022-12-28 06:34: Train Epoch 2: 53/159 Loss: 0.334208
2022-12-28 06:36: Train Epoch 2: 54/159 Loss: 0.290558
2022-12-28 06:38: Train Epoch 2: 55/159 Loss: 0.245848
2022-12-28 06:39: Train Epoch 2: 56/159 Loss: 0.234201
2022-12-28 06:41: Train Epoch 2: 57/159 Loss: 0.298687
2022-12-28 06:43: Train Epoch 2: 58/159 Loss: 0.334516
2022-12-28 06:44: Train Epoch 2: 59/159 Loss: 0.285656
2022-12-28 06:46: Train Epoch 2: 60/159 Loss: 0.305496
2022-12-28 06:48: Train Epoch 2: 61/159 Loss: 0.285861
2022-12-28 06:50: Train Epoch 2: 62/159 Loss: 0.319472
2022-12-28 06:51: Train Epoch 2: 63/159 Loss: 0.270653
2022-12-28 06:53: Train Epoch 2: 64/159 Loss: 0.270479
2022-12-28 06:55: Train Epoch 2: 65/159 Loss: 0.246743
2022-12-28 06:57: Train Epoch 2: 66/159 Loss: 0.211411
2022-12-28 06:58: Train Epoch 2: 67/159 Loss: 0.304455
2022-12-28 07:00: Train Epoch 2: 68/159 Loss: 0.230996
2022-12-28 07:02: Train Epoch 2: 69/159 Loss: 0.243240
2022-12-28 07:04: Train Epoch 2: 70/159 Loss: 0.297615
2022-12-28 07:05: Train Epoch 2: 71/159 Loss: 0.277069
2022-12-28 07:07: Train Epoch 2: 72/159 Loss: 0.237739
2022-12-28 07:09: Train Epoch 2: 73/159 Loss: 0.276886
2022-12-28 07:11: Train Epoch 2: 74/159 Loss: 0.340481
2022-12-28 07:12: Train Epoch 2: 75/159 Loss: 0.246000
2022-12-28 07:14: Train Epoch 2: 76/159 Loss: 0.258176
2022-12-28 07:16: Train Epoch 2: 77/159 Loss: 0.314254
2022-12-28 07:17: Train Epoch 2: 78/159 Loss: 0.288020
2022-12-28 07:19: Train Epoch 2: 79/159 Loss: 0.326445
2022-12-28 07:21: Train Epoch 2: 80/159 Loss: 0.283092
2022-12-28 07:23: Train Epoch 2: 81/159 Loss: 0.264217
2022-12-28 07:24: Train Epoch 2: 82/159 Loss: 0.290307
2022-12-28 07:26: Train Epoch 2: 83/159 Loss: 0.296359
2022-12-28 07:28: Train Epoch 2: 84/159 Loss: 0.243619
2022-12-28 07:30: Train Epoch 2: 85/159 Loss: 0.270111
2022-12-28 07:31: Train Epoch 2: 86/159 Loss: 0.324518
2022-12-28 07:33: Train Epoch 2: 87/159 Loss: 0.273580
2022-12-28 07:35: Train Epoch 2: 88/159 Loss: 0.304758
2022-12-28 07:37: Train Epoch 2: 89/159 Loss: 0.243413
2022-12-28 07:38: Train Epoch 2: 90/159 Loss: 0.280252
2022-12-28 07:40: Train Epoch 2: 91/159 Loss: 0.247729
2022-12-28 07:42: Train Epoch 2: 92/159 Loss: 0.277011
2022-12-28 07:44: Train Epoch 2: 93/159 Loss: 0.269532
2022-12-28 07:45: Train Epoch 2: 94/159 Loss: 0.281718
2022-12-28 07:47: Train Epoch 2: 95/159 Loss: 0.306499
2022-12-28 07:49: Train Epoch 2: 96/159 Loss: 0.253086
2022-12-28 07:50: Train Epoch 2: 97/159 Loss: 0.246690
2022-12-28 07:52: Train Epoch 2: 98/159 Loss: 0.215778
2022-12-28 07:54: Train Epoch 2: 99/159 Loss: 0.241534
2022-12-28 07:56: Train Epoch 2: 100/159 Loss: 0.244346
2022-12-28 07:57: Train Epoch 2: 101/159 Loss: 0.312808
2022-12-28 07:59: Train Epoch 2: 102/159 Loss: 0.267284
2022-12-28 08:01: Train Epoch 2: 103/159 Loss: 0.280194
2022-12-28 08:03: Train Epoch 2: 104/159 Loss: 0.231509
2022-12-28 08:04: Train Epoch 2: 105/159 Loss: 0.305720
2022-12-28 08:06: Train Epoch 2: 106/159 Loss: 0.297943
2022-12-28 08:08: Train Epoch 2: 107/159 Loss: 0.244028
2022-12-28 08:10: Train Epoch 2: 108/159 Loss: 0.263036
2022-12-28 08:11: Train Epoch 2: 109/159 Loss: 0.322155
2022-12-28 08:13: Train Epoch 2: 110/159 Loss: 0.249427
2022-12-28 08:15: Train Epoch 2: 111/159 Loss: 0.222336
2022-12-28 08:17: Train Epoch 2: 112/159 Loss: 0.281548
2022-12-28 08:18: Train Epoch 2: 113/159 Loss: 0.297465
2022-12-28 08:20: Train Epoch 2: 114/159 Loss: 0.290853
2022-12-28 08:22: Train Epoch 2: 115/159 Loss: 0.258983
2022-12-28 08:23: Train Epoch 2: 116/159 Loss: 0.203646
2022-12-28 08:25: Train Epoch 2: 117/159 Loss: 0.262119
2022-12-28 08:27: Train Epoch 2: 118/159 Loss: 0.274204
2022-12-28 08:29: Train Epoch 2: 119/159 Loss: 0.326991
2022-12-28 08:30: Train Epoch 2: 120/159 Loss: 0.240167
2022-12-28 08:32: Train Epoch 2: 121/159 Loss: 0.302260
2022-12-28 08:34: Train Epoch 2: 122/159 Loss: 0.234582
2022-12-28 08:36: Train Epoch 2: 123/159 Loss: 0.283281
2022-12-28 08:37: Train Epoch 2: 124/159 Loss: 0.318291
2022-12-28 08:39: Train Epoch 2: 125/159 Loss: 0.259459
2022-12-28 08:41: Train Epoch 2: 126/159 Loss: 0.259418
2022-12-28 08:43: Train Epoch 2: 127/159 Loss: 0.309226
2022-12-28 08:44: Train Epoch 2: 128/159 Loss: 0.239799
2022-12-28 08:46: Train Epoch 2: 129/159 Loss: 0.245248
2022-12-28 08:48: Train Epoch 2: 130/159 Loss: 0.351320
2022-12-28 08:49: Train Epoch 2: 131/159 Loss: 0.218307
2022-12-28 08:51: Train Epoch 2: 132/159 Loss: 0.252914
2022-12-28 08:53: Train Epoch 2: 133/159 Loss: 0.272243
2022-12-28 08:55: Train Epoch 2: 134/159 Loss: 0.335523
2022-12-28 08:56: Train Epoch 2: 135/159 Loss: 0.381828
2022-12-28 08:58: Train Epoch 2: 136/159 Loss: 0.293100
2022-12-28 09:00: Train Epoch 2: 137/159 Loss: 0.270801
2022-12-28 09:02: Train Epoch 2: 138/159 Loss: 0.342135
2022-12-28 09:03: Train Epoch 2: 139/159 Loss: 0.208919
2022-12-28 09:05: Train Epoch 2: 140/159 Loss: 0.277039
2022-12-28 09:07: Train Epoch 2: 141/159 Loss: 0.238279
2022-12-28 09:09: Train Epoch 2: 142/159 Loss: 0.276341
2022-12-28 09:10: Train Epoch 2: 143/159 Loss: 0.224513
2022-12-28 09:12: Train Epoch 2: 144/159 Loss: 0.269523
2022-12-28 09:14: Train Epoch 2: 145/159 Loss: 0.310554
2022-12-28 09:16: Train Epoch 2: 146/159 Loss: 0.279462
2022-12-28 09:17: Train Epoch 2: 147/159 Loss: 0.292852
2022-12-28 09:19: Train Epoch 2: 148/159 Loss: 0.303565
2022-12-28 09:21: Train Epoch 2: 149/159 Loss: 0.300647
2022-12-28 09:22: Train Epoch 2: 150/159 Loss: 0.254372
2022-12-28 09:24: Train Epoch 2: 151/159 Loss: 0.265415
2022-12-28 09:26: Train Epoch 2: 152/159 Loss: 0.243315
2022-12-28 09:27: Train Epoch 2: 153/159 Loss: 0.290660
2022-12-28 09:29: Train Epoch 2: 154/159 Loss: 0.244432
2022-12-28 09:31: Train Epoch 2: 155/159 Loss: 0.325402
2022-12-28 09:33: Train Epoch 2: 156/159 Loss: 0.347444
2022-12-28 09:34: Train Epoch 2: 157/159 Loss: 0.381952
2022-12-28 09:35: Train Epoch 2: 158/159 Loss: 0.378061
2022-12-28 09:35: **********Train Epoch 2: averaged Loss: 0.280032 
2022-12-28 09:35: 
Epoch time elapsed: 16464.51258611679

2022-12-28 09:42: 
 metrics validation: {'precision': 0.8107810781078107, 'recall': 0.566923076923077, 'f1-score': 0.6672702580353101, 'support': 1300, 'AUC': 0.8502609467455622, 'AUCPR': 0.7763516749643988, 'TP': 737, 'FP': 172, 'TN': 2428, 'FN': 563} 

2022-12-28 09:42: **********Val Epoch 2: average Loss: 0.576649
2022-12-28 09:49: 
 Testing metrics {'precision': 0.7988450433108758, 'recall': 0.6758957654723127, 'f1-score': 0.7322452580502866, 'support': 1228, 'AUC': 0.8705410667487188, 'AUCPR': 0.7938557623792589, 'TP': 830, 'FP': 209, 'TN': 2247, 'FN': 398} 

2022-12-28 09:51: Train Epoch 3: 0/159 Loss: 0.352397
2022-12-28 09:53: Train Epoch 3: 1/159 Loss: 0.267914
2022-12-28 09:55: Train Epoch 3: 2/159 Loss: 0.350582
2022-12-28 09:56: Train Epoch 3: 3/159 Loss: 0.286956
2022-12-28 09:58: Train Epoch 3: 4/159 Loss: 0.355791
2022-12-28 10:00: Train Epoch 3: 5/159 Loss: 0.269238
2022-12-28 10:02: Train Epoch 3: 6/159 Loss: 0.318239
2022-12-28 10:03: Train Epoch 3: 7/159 Loss: 0.232823
2022-12-28 10:05: Train Epoch 3: 8/159 Loss: 0.266604
2022-12-28 10:07: Train Epoch 3: 9/159 Loss: 0.284743
2022-12-28 10:08: Train Epoch 3: 10/159 Loss: 0.244394
2022-12-28 10:10: Train Epoch 3: 11/159 Loss: 0.270271
2022-12-28 10:12: Train Epoch 3: 12/159 Loss: 0.295976
2022-12-28 10:14: Train Epoch 3: 13/159 Loss: 0.225028
2022-12-28 10:15: Train Epoch 3: 14/159 Loss: 0.247385
2022-12-28 10:17: Train Epoch 3: 15/159 Loss: 0.299947
2022-12-28 10:19: Train Epoch 3: 16/159 Loss: 0.294634
2022-12-28 10:21: Train Epoch 3: 17/159 Loss: 0.250609
2022-12-28 10:22: Train Epoch 3: 18/159 Loss: 0.244300
2022-12-28 10:24: Train Epoch 3: 19/159 Loss: 0.241072
2022-12-28 10:26: Train Epoch 3: 20/159 Loss: 0.280082
2022-12-28 10:28: Train Epoch 3: 21/159 Loss: 0.270904
2022-12-28 10:29: Train Epoch 3: 22/159 Loss: 0.263504
2022-12-28 10:31: Train Epoch 3: 23/159 Loss: 0.294551
2022-12-28 10:33: Train Epoch 3: 24/159 Loss: 0.253577
2022-12-28 10:35: Train Epoch 3: 25/159 Loss: 0.217056
2022-12-28 10:36: Train Epoch 3: 26/159 Loss: 0.256719
2022-12-28 10:38: Train Epoch 3: 27/159 Loss: 0.288448
2022-12-28 10:40: Train Epoch 3: 28/159 Loss: 0.251676
2022-12-28 10:42: Train Epoch 3: 29/159 Loss: 0.276759
2022-12-28 10:43: Train Epoch 3: 30/159 Loss: 0.261313
2022-12-28 10:45: Train Epoch 3: 31/159 Loss: 0.286104
2022-12-28 10:47: Train Epoch 3: 32/159 Loss: 0.304162
2022-12-28 10:48: Train Epoch 3: 33/159 Loss: 0.270951
2022-12-28 10:50: Train Epoch 3: 34/159 Loss: 0.316594
2022-12-28 10:52: Train Epoch 3: 35/159 Loss: 0.238655
2022-12-28 10:54: Train Epoch 3: 36/159 Loss: 0.316778
2022-12-28 10:55: Train Epoch 3: 37/159 Loss: 0.309924
2022-12-28 10:57: Train Epoch 3: 38/159 Loss: 0.255415
2022-12-28 10:59: Train Epoch 3: 39/159 Loss: 0.317750
2022-12-28 11:01: Train Epoch 3: 40/159 Loss: 0.232753
2022-12-28 11:02: Train Epoch 3: 41/159 Loss: 0.286051
2022-12-28 11:04: Train Epoch 3: 42/159 Loss: 0.303232
2022-12-28 11:06: Train Epoch 3: 43/159 Loss: 0.228411
2022-12-28 11:08: Train Epoch 3: 44/159 Loss: 0.276487
2022-12-28 11:09: Train Epoch 3: 45/159 Loss: 0.283034
2022-12-28 11:11: Train Epoch 3: 46/159 Loss: 0.276162
2022-12-28 11:13: Train Epoch 3: 47/159 Loss: 0.274559
2022-12-28 11:15: Train Epoch 3: 48/159 Loss: 0.246555
2022-12-28 11:16: Train Epoch 3: 49/159 Loss: 0.218050
2022-12-28 11:18: Train Epoch 3: 50/159 Loss: 0.358166
2022-12-28 11:20: Train Epoch 3: 51/159 Loss: 0.274534
2022-12-28 11:21: Train Epoch 3: 52/159 Loss: 0.262467
2022-12-28 11:23: Train Epoch 3: 53/159 Loss: 0.315812
2022-12-28 11:25: Train Epoch 3: 54/159 Loss: 0.270799
2022-12-28 11:27: Train Epoch 3: 55/159 Loss: 0.254784
2022-12-28 11:28: Train Epoch 3: 56/159 Loss: 0.278705
2022-12-28 11:30: Train Epoch 3: 57/159 Loss: 0.294358
2022-12-28 11:32: Train Epoch 3: 58/159 Loss: 0.352766
2022-12-28 11:34: Train Epoch 3: 59/159 Loss: 0.290599
2022-12-28 11:35: Train Epoch 3: 60/159 Loss: 0.309551
2022-12-28 11:37: Train Epoch 3: 61/159 Loss: 0.289346
2022-12-28 11:39: Train Epoch 3: 62/159 Loss: 0.311745
2022-12-28 11:41: Train Epoch 3: 63/159 Loss: 0.281884
2022-12-28 11:42: Train Epoch 3: 64/159 Loss: 0.287859
2022-12-28 11:44: Train Epoch 3: 65/159 Loss: 0.252735
2022-12-28 11:46: Train Epoch 3: 66/159 Loss: 0.312902
2022-12-28 11:47: Train Epoch 3: 67/159 Loss: 0.230907
2022-12-28 11:49: Train Epoch 3: 68/159 Loss: 0.342649
2022-12-28 11:51: Train Epoch 3: 69/159 Loss: 0.340361
2022-12-28 11:53: Train Epoch 3: 70/159 Loss: 0.274676
2022-12-28 11:54: Train Epoch 3: 71/159 Loss: 0.456389
2022-12-28 11:56: Train Epoch 3: 72/159 Loss: 0.333775
2022-12-28 11:58: Train Epoch 3: 73/159 Loss: 0.310847
2022-12-28 12:00: Train Epoch 3: 74/159 Loss: 0.393463
2022-12-28 12:01: Train Epoch 3: 75/159 Loss: 0.298265
2022-12-28 12:03: Train Epoch 3: 76/159 Loss: 0.306293
2022-12-28 12:05: Train Epoch 3: 77/159 Loss: 0.296592
2022-12-28 12:07: Train Epoch 3: 78/159 Loss: 0.267048
2022-12-28 12:08: Train Epoch 3: 79/159 Loss: 0.290019
2022-12-28 12:10: Train Epoch 3: 80/159 Loss: 0.272515
2022-12-28 12:12: Train Epoch 3: 81/159 Loss: 0.296907
2022-12-28 12:13: Train Epoch 3: 82/159 Loss: 0.303174
2022-12-28 12:15: Train Epoch 3: 83/159 Loss: 0.362305
2022-12-28 12:17: Train Epoch 3: 84/159 Loss: 0.284311
2022-12-28 12:19: Train Epoch 3: 85/159 Loss: 0.271312
2022-12-28 12:20: Train Epoch 3: 86/159 Loss: 0.258618
2022-12-28 12:22: Train Epoch 3: 87/159 Loss: 0.294756
2022-12-28 12:24: Train Epoch 3: 88/159 Loss: 0.271768
2022-12-28 12:26: Train Epoch 3: 89/159 Loss: 0.282435
2022-12-28 12:27: Train Epoch 3: 90/159 Loss: 0.254296
2022-12-28 12:29: Train Epoch 3: 91/159 Loss: 0.212501
2022-12-28 12:31: Train Epoch 3: 92/159 Loss: 0.359940
2022-12-28 12:32: Train Epoch 3: 93/159 Loss: 0.304064
2022-12-28 12:34: Train Epoch 3: 94/159 Loss: 0.271098
2022-12-28 12:36: Train Epoch 3: 95/159 Loss: 0.312149
2022-12-28 12:38: Train Epoch 3: 96/159 Loss: 0.297550
2022-12-28 12:39: Train Epoch 3: 97/159 Loss: 0.325666
2022-12-28 12:41: Train Epoch 3: 98/159 Loss: 0.374698
2022-12-28 12:43: Train Epoch 3: 99/159 Loss: 0.272387
2022-12-28 12:45: Train Epoch 3: 100/159 Loss: 0.305820
2022-12-28 12:46: Train Epoch 3: 101/159 Loss: 0.334816
2022-12-28 12:48: Train Epoch 3: 102/159 Loss: 0.319095
2022-12-28 12:50: Train Epoch 3: 103/159 Loss: 0.252890
2022-12-28 12:52: Train Epoch 3: 104/159 Loss: 0.310054
2022-12-28 12:53: Train Epoch 3: 105/159 Loss: 0.311673
2022-12-28 12:55: Train Epoch 3: 106/159 Loss: 0.290306
2022-12-28 12:57: Train Epoch 3: 107/159 Loss: 0.324848
2022-12-28 12:58: Train Epoch 3: 108/159 Loss: 0.287240
2022-12-28 13:00: Train Epoch 3: 109/159 Loss: 0.327920
2022-12-28 13:02: Train Epoch 3: 110/159 Loss: 0.263787
2022-12-28 13:04: Train Epoch 3: 111/159 Loss: 0.250493
2022-12-28 13:05: Train Epoch 3: 112/159 Loss: 0.376990
2022-12-28 13:07: Train Epoch 3: 113/159 Loss: 0.268044
2022-12-28 13:09: Train Epoch 3: 114/159 Loss: 0.325612
2022-12-28 13:10: Train Epoch 3: 115/159 Loss: 0.286091
2022-12-28 13:12: Train Epoch 3: 116/159 Loss: 0.327873
2022-12-28 13:14: Train Epoch 3: 117/159 Loss: 0.345153
2022-12-28 13:16: Train Epoch 3: 118/159 Loss: 0.308081
2022-12-28 13:17: Train Epoch 3: 119/159 Loss: 0.279043
2022-12-28 13:19: Train Epoch 3: 120/159 Loss: 0.303035
2022-12-28 13:21: Train Epoch 3: 121/159 Loss: 0.413340
2022-12-28 13:23: Train Epoch 3: 122/159 Loss: 0.321198
2022-12-28 13:24: Train Epoch 3: 123/159 Loss: 0.282013
2022-12-28 13:26: Train Epoch 3: 124/159 Loss: 0.285283
2022-12-28 13:28: Train Epoch 3: 125/159 Loss: 0.266785
2022-12-28 13:30: Train Epoch 3: 126/159 Loss: 0.199952
2022-12-28 13:31: Train Epoch 3: 127/159 Loss: 0.310248
2022-12-28 13:33: Train Epoch 3: 128/159 Loss: 0.315455
2022-12-28 13:35: Train Epoch 3: 129/159 Loss: 0.329783
2022-12-28 13:37: Train Epoch 3: 130/159 Loss: 0.288504
2022-12-28 13:38: Train Epoch 3: 131/159 Loss: 0.309326
2022-12-28 13:40: Train Epoch 3: 132/159 Loss: 0.230579
2022-12-28 13:42: Train Epoch 3: 133/159 Loss: 0.295263
2022-12-28 13:43: Train Epoch 3: 134/159 Loss: 0.311055
2022-12-28 13:45: Train Epoch 3: 135/159 Loss: 0.331437
2022-12-28 13:47: Train Epoch 3: 136/159 Loss: 0.267557
2022-12-28 13:49: Train Epoch 3: 137/159 Loss: 0.330476
2022-12-28 13:50: Train Epoch 3: 138/159 Loss: 0.278958
2022-12-28 13:52: Train Epoch 3: 139/159 Loss: 0.218467
2022-12-28 13:54: Train Epoch 3: 140/159 Loss: 0.285378
2022-12-28 13:56: Train Epoch 3: 141/159 Loss: 0.269471
2022-12-28 13:57: Train Epoch 3: 142/159 Loss: 0.280792
2022-12-28 13:59: Train Epoch 3: 143/159 Loss: 0.267100
2022-12-28 14:01: Train Epoch 3: 144/159 Loss: 0.196560
2022-12-28 14:03: Train Epoch 3: 145/159 Loss: 0.249030
2022-12-28 14:04: Train Epoch 3: 146/159 Loss: 0.311119
2022-12-28 14:06: Train Epoch 3: 147/159 Loss: 0.279791
2022-12-28 14:08: Train Epoch 3: 148/159 Loss: 0.185952
2022-12-28 14:09: Train Epoch 3: 149/159 Loss: 0.281457
2022-12-28 14:11: Train Epoch 3: 150/159 Loss: 0.250468
2022-12-28 14:13: Train Epoch 3: 151/159 Loss: 0.321399
2022-12-28 14:15: Train Epoch 3: 152/159 Loss: 0.263446
2022-12-28 14:16: Train Epoch 3: 153/159 Loss: 0.273469
2022-12-28 14:18: Train Epoch 3: 154/159 Loss: 0.216492
2022-12-28 14:20: Train Epoch 3: 155/159 Loss: 0.303960
2022-12-28 14:22: Train Epoch 3: 156/159 Loss: 0.280093
2022-12-28 14:23: Train Epoch 3: 157/159 Loss: 0.296972
2022-12-28 14:24: Train Epoch 3: 158/159 Loss: 0.166597
2022-12-28 14:24: **********Train Epoch 3: averaged Loss: 0.287685 
2022-12-28 14:24: 
Epoch time elapsed: 16470.177018642426

2022-12-28 14:31: 
 metrics validation: {'precision': 0.7512820512820513, 'recall': 0.6761538461538461, 'f1-score': 0.7117408906882591, 'support': 1300, 'AUC': 0.8488461538461538, 'AUCPR': 0.7749527615179512, 'TP': 879, 'FP': 291, 'TN': 2309, 'FN': 421} 

2022-12-28 14:31: **********Val Epoch 3: average Loss: 0.524532
2022-12-28 14:31: *********************************Current best model saved!
2022-12-28 14:38: 
 Testing metrics {'precision': 0.8066037735849056, 'recall': 0.6962540716612378, 'f1-score': 0.7473776223776224, 'support': 1228, 'AUC': 0.8764940476822036, 'AUCPR': 0.812681849657886, 'TP': 855, 'FP': 205, 'TN': 2251, 'FN': 373} 

2022-12-28 14:40: Train Epoch 4: 0/159 Loss: 0.283766
2022-12-28 14:42: Train Epoch 4: 1/159 Loss: 0.319927
2022-12-28 14:44: Train Epoch 4: 2/159 Loss: 0.247076
2022-12-28 14:45: Train Epoch 4: 3/159 Loss: 0.247578
2022-12-28 14:47: Train Epoch 4: 4/159 Loss: 0.227136
2022-12-28 14:49: Train Epoch 4: 5/159 Loss: 0.272108
2022-12-28 14:51: Train Epoch 4: 6/159 Loss: 0.214888
2022-12-28 14:52: Train Epoch 4: 7/159 Loss: 0.235567
2022-12-28 14:54: Train Epoch 4: 8/159 Loss: 0.294378
2022-12-28 14:56: Train Epoch 4: 9/159 Loss: 0.293169
