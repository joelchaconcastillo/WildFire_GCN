2022-12-31 10:14: log dir: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022123110145043936554013
2022-12-31 10:14: Experiment log path in: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022123110145043936554013
2022-12-31 10:14: Argument: Namespace(batch_size=256, clc='vec', cuda=True, dataset='2020', dataset_root='/home/joel.chacon/tmp/datasets_grl/', debug=False, default_graph=True, device='cpu', dynamic_features=['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh'], early_stop=True, early_stop_patience=8, embed_dim=64, epochs=30, grad_norm=False, horizon=1, input_dim=25, lag=10, link_len=2, log_dir='/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022123110145043936554013', log_step=1, loss_func='nllloss', lr_decay=True, lr_decay_rate=0.1, lr_decay_step='15, 20', lr_init=0.0001, max_grad_norm=5, minbatch_size=64, mode='train', model='fire_GCN', nan_fill=-1.0, num_layers=1, num_nodes=625, num_workers=12, output_dim=2, patch_height=25, patch_width=25, persistent_workers=True, pin_memory=True, plot=False, positive_weight=0.5, prefetch_factor=2, real_value=True, rnn_units=128, seed=10000, static_features=['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density'], teacher_forcing=False, weight_decay=0.0, window_len=10)
2022-12-31 10:14: Argument batch_size: 256
2022-12-31 10:14: Argument clc: 'vec'
2022-12-31 10:14: Argument cuda: True
2022-12-31 10:14: Argument dataset: '2020'
2022-12-31 10:14: Argument dataset_root: '/home/joel.chacon/tmp/datasets_grl/'
2022-12-31 10:14: Argument debug: False
2022-12-31 10:14: Argument default_graph: True
2022-12-31 10:14: Argument device: 'cpu'
2022-12-31 10:14: Argument dynamic_features: ['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh']
2022-12-31 10:14: Argument early_stop: True
2022-12-31 10:14: Argument early_stop_patience: 8
2022-12-31 10:14: Argument embed_dim: 64
2022-12-31 10:14: Argument epochs: 30
2022-12-31 10:14: Argument grad_norm: False
2022-12-31 10:14: Argument horizon: 1
2022-12-31 10:14: Argument input_dim: 25
2022-12-31 10:14: Argument lag: 10
2022-12-31 10:14: Argument link_len: 2
2022-12-31 10:14: Argument log_dir: '/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022123110145043936554013'
2022-12-31 10:14: Argument log_step: 1
2022-12-31 10:14: Argument loss_func: 'nllloss'
2022-12-31 10:14: Argument lr_decay: True
2022-12-31 10:14: Argument lr_decay_rate: 0.1
2022-12-31 10:14: Argument lr_decay_step: '15, 20'
2022-12-31 10:14: Argument lr_init: 0.0001
2022-12-31 10:14: Argument max_grad_norm: 5
2022-12-31 10:14: Argument minbatch_size: 64
2022-12-31 10:14: Argument mode: 'train'
2022-12-31 10:14: Argument model: 'fire_GCN'
2022-12-31 10:14: Argument nan_fill: -1.0
2022-12-31 10:14: Argument num_layers: 1
2022-12-31 10:14: Argument num_nodes: 625
2022-12-31 10:14: Argument num_workers: 12
2022-12-31 10:14: Argument output_dim: 2
2022-12-31 10:14: Argument patch_height: 25
2022-12-31 10:14: Argument patch_width: 25
2022-12-31 10:14: Argument persistent_workers: True
2022-12-31 10:14: Argument pin_memory: True
2022-12-31 10:14: Argument plot: False
2022-12-31 10:14: Argument positive_weight: 0.5
2022-12-31 10:14: Argument prefetch_factor: 2
2022-12-31 10:14: Argument real_value: True
2022-12-31 10:14: Argument rnn_units: 128
2022-12-31 10:14: Argument seed: 10000
2022-12-31 10:14: Argument static_features: ['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density']
2022-12-31 10:14: Argument teacher_forcing: False
2022-12-31 10:14: Argument weight_decay: 0.0
2022-12-31 10:14: Argument window_len: 10
++++++++++++++
2020_fire_GCN.conf
++++++++++++++
*****************Model Parameter*****************
node_embeddings torch.Size([625, 64]) True
ln1.weight torch.Size([25]) True
ln1.bias torch.Size([25]) True
encoder.cell_list.0.gate.weights_pool torch.Size([64, 2, 153, 128]) True
encoder.cell_list.0.gate.weights_window torch.Size([64, 1, 128]) True
encoder.cell_list.0.gate.bias_pool torch.Size([64, 256]) True
encoder.cell_list.0.gate.T torch.Size([10]) True
encoder.cell_list.0.update.weights_pool torch.Size([64, 2, 153, 64]) True
encoder.cell_list.0.update.weights_window torch.Size([64, 1, 64]) True
encoder.cell_list.0.update.bias_pool torch.Size([64, 128]) True
encoder.cell_list.0.update.T torch.Size([10]) True
fc1.weight torch.Size([2, 80000]) True
fc1.bias torch.Size([2]) True
Total params num: 3997064
*****************Finish Parameter****************
Positives: 13518 / Negatives: 27036
Dataset length 40554
Positives: 1300 / Negatives: 2600
Dataset length 3900
Positives: 1228 / Negatives: 2456
Dataset length 3684
Positives: 4407 / Negatives: 8814
Dataset length 13221
Applying learning rate decay.
Creat Log File in:  /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022123110145043936554013/run.log
2022-12-31 10:15: Train Epoch 1: 3/634 Loss: 0.315737
2022-12-31 10:15: Train Epoch 1: 7/634 Loss: 3.934291
2022-12-31 10:16: Train Epoch 1: 11/634 Loss: 1.125355
2022-12-31 10:16: Train Epoch 1: 15/634 Loss: 1.426885
2022-12-31 10:17: Train Epoch 1: 19/634 Loss: 1.481072
2022-12-31 10:17: Train Epoch 1: 23/634 Loss: 0.617889
2022-12-31 10:18: Train Epoch 1: 27/634 Loss: 0.429067
2022-12-31 10:18: Train Epoch 1: 31/634 Loss: 0.618235
2022-12-31 10:19: Train Epoch 1: 35/634 Loss: 0.642204
2022-12-31 10:19: Train Epoch 1: 39/634 Loss: 0.298448
2022-12-31 10:20: Train Epoch 1: 43/634 Loss: 0.355207
2022-12-31 10:20: Train Epoch 1: 47/634 Loss: 0.505307
2022-12-31 10:21: Train Epoch 1: 51/634 Loss: 0.450939
2022-12-31 10:21: Train Epoch 1: 55/634 Loss: 0.354594
2022-12-31 10:22: Train Epoch 1: 59/634 Loss: 0.316406
2022-12-31 10:22: Train Epoch 1: 63/634 Loss: 0.385704
2022-12-31 10:23: Train Epoch 1: 67/634 Loss: 0.441737
2022-12-31 10:23: Train Epoch 1: 71/634 Loss: 0.361532
2022-12-31 10:24: Train Epoch 1: 75/634 Loss: 0.300926
2022-12-31 10:24: Train Epoch 1: 79/634 Loss: 0.310622
2022-12-31 10:25: Train Epoch 1: 83/634 Loss: 0.326213
2022-12-31 10:25: Train Epoch 1: 87/634 Loss: 0.326389
2022-12-31 10:26: Train Epoch 1: 91/634 Loss: 0.214496
2022-12-31 10:26: Train Epoch 1: 95/634 Loss: 0.301293
2022-12-31 10:27: Train Epoch 1: 99/634 Loss: 0.335687
2022-12-31 10:27: Train Epoch 1: 103/634 Loss: 0.314963
2022-12-31 10:28: Train Epoch 1: 107/634 Loss: 0.250781
2022-12-31 10:28: Train Epoch 1: 111/634 Loss: 0.275063
2022-12-31 10:29: Train Epoch 1: 115/634 Loss: 0.242868
2022-12-31 10:29: Train Epoch 1: 119/634 Loss: 0.274303
2022-12-31 10:30: Train Epoch 1: 123/634 Loss: 0.277879
2022-12-31 10:30: Train Epoch 1: 127/634 Loss: 0.197092
2022-12-31 10:31: Train Epoch 1: 131/634 Loss: 0.231369
2022-12-31 10:31: Train Epoch 1: 135/634 Loss: 0.242929
2022-12-31 10:32: Train Epoch 1: 139/634 Loss: 0.248341
2022-12-31 10:32: Train Epoch 1: 143/634 Loss: 0.236700
2022-12-31 10:33: Train Epoch 1: 147/634 Loss: 0.221630
2022-12-31 10:33: Train Epoch 1: 151/634 Loss: 0.233404
2022-12-31 10:34: Train Epoch 1: 155/634 Loss: 0.239680
2022-12-31 10:34: Train Epoch 1: 159/634 Loss: 0.220910
2022-12-31 10:35: Train Epoch 1: 163/634 Loss: 0.247201
2022-12-31 10:35: Train Epoch 1: 167/634 Loss: 0.240630
2022-12-31 10:36: Train Epoch 1: 171/634 Loss: 0.210105
2022-12-31 10:36: Train Epoch 1: 175/634 Loss: 0.222043
2022-12-31 10:37: Train Epoch 1: 179/634 Loss: 0.213962
2022-12-31 10:37: Train Epoch 1: 183/634 Loss: 0.215679
2022-12-31 10:38: Train Epoch 1: 187/634 Loss: 0.188807
2022-12-31 10:38: Train Epoch 1: 191/634 Loss: 0.218126
2022-12-31 10:39: Train Epoch 1: 195/634 Loss: 0.231270
2022-12-31 10:39: Train Epoch 1: 199/634 Loss: 0.234834
2022-12-31 10:40: Train Epoch 1: 203/634 Loss: 0.207665
2022-12-31 10:40: Train Epoch 1: 207/634 Loss: 0.225819
2022-12-31 10:41: Train Epoch 1: 211/634 Loss: 0.195434
2022-12-31 10:41: Train Epoch 1: 215/634 Loss: 0.199846
2022-12-31 10:42: Train Epoch 1: 219/634 Loss: 0.267301
2022-12-31 10:42: Train Epoch 1: 223/634 Loss: 0.215489
2022-12-31 10:43: Train Epoch 1: 227/634 Loss: 0.206594
2022-12-31 10:43: Train Epoch 1: 231/634 Loss: 0.220634
2022-12-31 10:44: Train Epoch 1: 235/634 Loss: 0.227837
2022-12-31 10:44: Train Epoch 1: 239/634 Loss: 0.228872
2022-12-31 10:44: Train Epoch 1: 243/634 Loss: 0.260566
2022-12-31 10:45: Train Epoch 1: 247/634 Loss: 0.231892
2022-12-31 10:46: Train Epoch 1: 251/634 Loss: 0.204640
2022-12-31 10:46: Train Epoch 1: 255/634 Loss: 0.216031
2022-12-31 10:47: Train Epoch 1: 259/634 Loss: 0.208183
2022-12-31 10:47: Train Epoch 1: 263/634 Loss: 0.213310
2022-12-31 10:48: Train Epoch 1: 267/634 Loss: 0.248477
2022-12-31 10:48: Train Epoch 1: 271/634 Loss: 0.209694
2022-12-31 10:48: Train Epoch 1: 275/634 Loss: 0.228740
2022-12-31 10:49: Train Epoch 1: 279/634 Loss: 0.204992
2022-12-31 10:49: Train Epoch 1: 283/634 Loss: 0.183004
2022-12-31 10:50: Train Epoch 1: 287/634 Loss: 0.232753
2022-12-31 10:50: Train Epoch 1: 291/634 Loss: 0.197703
2022-12-31 10:51: Train Epoch 1: 295/634 Loss: 0.254590
2022-12-31 10:51: Train Epoch 1: 299/634 Loss: 0.197703
2022-12-31 10:52: Train Epoch 1: 303/634 Loss: 0.206016
2022-12-31 10:52: Train Epoch 1: 307/634 Loss: 0.186033
2022-12-31 10:53: Train Epoch 1: 311/634 Loss: 0.207691
2022-12-31 10:53: Train Epoch 1: 315/634 Loss: 0.214972
2022-12-31 10:54: Train Epoch 1: 319/634 Loss: 0.200059
2022-12-31 10:54: Train Epoch 1: 323/634 Loss: 0.197728
2022-12-31 10:55: Train Epoch 1: 327/634 Loss: 0.202577
2022-12-31 10:55: Train Epoch 1: 331/634 Loss: 0.184446
2022-12-31 10:56: Train Epoch 1: 335/634 Loss: 0.186281
2022-12-31 10:56: Train Epoch 1: 339/634 Loss: 0.208155
2022-12-31 10:57: Train Epoch 1: 343/634 Loss: 0.166668
2022-12-31 10:57: Train Epoch 1: 347/634 Loss: 0.229723
2022-12-31 10:58: Train Epoch 1: 351/634 Loss: 0.170058
2022-12-31 10:58: Train Epoch 1: 355/634 Loss: 0.224811
2022-12-31 10:58: Train Epoch 1: 359/634 Loss: 0.250703
2022-12-31 10:59: Train Epoch 1: 363/634 Loss: 0.196816
2022-12-31 10:59: Train Epoch 1: 367/634 Loss: 0.218401
2022-12-31 11:00: Train Epoch 1: 371/634 Loss: 0.204773
2022-12-31 11:00: Train Epoch 1: 375/634 Loss: 0.206408
2022-12-31 11:01: Train Epoch 1: 379/634 Loss: 0.204010
2022-12-31 11:01: Train Epoch 1: 383/634 Loss: 0.233097
2022-12-31 11:02: Train Epoch 1: 387/634 Loss: 0.224598
2022-12-31 11:02: Train Epoch 1: 391/634 Loss: 0.211650
2022-12-31 11:03: Train Epoch 1: 395/634 Loss: 0.212684
2022-12-31 11:03: Train Epoch 1: 399/634 Loss: 0.201426
2022-12-31 11:04: Train Epoch 1: 403/634 Loss: 0.191038
2022-12-31 11:04: Train Epoch 1: 407/634 Loss: 0.219732
2022-12-31 11:05: Train Epoch 1: 411/634 Loss: 0.202206
2022-12-31 11:05: Train Epoch 1: 415/634 Loss: 0.219022
2022-12-31 11:06: Train Epoch 1: 419/634 Loss: 0.227993
2022-12-31 11:06: Train Epoch 1: 423/634 Loss: 0.196698
2022-12-31 11:07: Train Epoch 1: 427/634 Loss: 0.221073
2022-12-31 11:07: Train Epoch 1: 431/634 Loss: 0.213878
2022-12-31 11:08: Train Epoch 1: 435/634 Loss: 0.202558
2022-12-31 11:08: Train Epoch 1: 439/634 Loss: 0.203461
2022-12-31 11:09: Train Epoch 1: 443/634 Loss: 0.183008
2022-12-31 11:09: Train Epoch 1: 447/634 Loss: 0.208132
2022-12-31 11:10: Train Epoch 1: 451/634 Loss: 0.231076
2022-12-31 11:10: Train Epoch 1: 455/634 Loss: 0.211609
2022-12-31 11:11: Train Epoch 1: 459/634 Loss: 0.176938
2022-12-31 11:11: Train Epoch 1: 463/634 Loss: 0.221926
2022-12-31 11:12: Train Epoch 1: 467/634 Loss: 0.215951
2022-12-31 11:12: Train Epoch 1: 471/634 Loss: 0.198108
2022-12-31 11:13: Train Epoch 1: 475/634 Loss: 0.213566
2022-12-31 11:13: Train Epoch 1: 479/634 Loss: 0.220495
2022-12-31 11:14: Train Epoch 1: 483/634 Loss: 0.192337
2022-12-31 11:14: Train Epoch 1: 487/634 Loss: 0.218652
2022-12-31 11:15: Train Epoch 1: 491/634 Loss: 0.214092
2022-12-31 11:15: Train Epoch 1: 495/634 Loss: 0.189896
2022-12-31 11:16: Train Epoch 1: 499/634 Loss: 0.208539
2022-12-31 11:16: Train Epoch 1: 503/634 Loss: 0.208534
2022-12-31 11:17: Train Epoch 1: 507/634 Loss: 0.247632
2022-12-31 11:17: Train Epoch 1: 511/634 Loss: 0.184163
2022-12-31 11:18: Train Epoch 1: 515/634 Loss: 0.231094
2022-12-31 11:18: Train Epoch 1: 519/634 Loss: 0.189912
2022-12-31 11:19: Train Epoch 1: 523/634 Loss: 0.191454
2022-12-31 11:19: Train Epoch 1: 527/634 Loss: 0.201825
2022-12-31 11:20: Train Epoch 1: 531/634 Loss: 0.212068
2022-12-31 11:20: Train Epoch 1: 535/634 Loss: 0.225387
2022-12-31 11:21: Train Epoch 1: 539/634 Loss: 0.182441
2022-12-31 11:21: Train Epoch 1: 543/634 Loss: 0.209226
2022-12-31 11:22: Train Epoch 1: 547/634 Loss: 0.190617
2022-12-31 11:22: Train Epoch 1: 551/634 Loss: 0.178792
2022-12-31 11:23: Train Epoch 1: 555/634 Loss: 0.219789
2022-12-31 11:23: Train Epoch 1: 559/634 Loss: 0.229918
2022-12-31 11:24: Train Epoch 1: 563/634 Loss: 0.210825
2022-12-31 11:24: Train Epoch 1: 567/634 Loss: 0.210051
2022-12-31 11:25: Train Epoch 1: 571/634 Loss: 0.205150
2022-12-31 11:26: Train Epoch 1: 575/634 Loss: 0.207361
2022-12-31 11:26: Train Epoch 1: 579/634 Loss: 0.207499
