2022-12-28 22:06: log dir: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122822060495293869118
2022-12-28 22:06: Experiment log path in: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122822060495293869118
2022-12-28 22:06: Argument: Namespace(batch_size=256, clc='vec', cuda=True, dataset='2020', dataset_root='/home/joel.chacon/tmp/datasets_grl/', debug=False, default_graph=True, device='cpu', dynamic_features=['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh'], early_stop=True, early_stop_patience=8, embed_dim=128, epochs=30, grad_norm=False, horizon=1, input_dim=25, lag=10, link_len=2, log_dir='/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122822060495293869118', log_step=1, loss_func='nllloss', lr_decay=True, lr_decay_rate=0.1, lr_decay_step='15, 20', lr_init=0.0005, max_grad_norm=5, minbatch_size=256, mode='train', model='fire_GCN', nan_fill=0.5, num_layers=1, num_nodes=625, num_workers=20, output_dim=2, patch_height=25, patch_width=25, persistent_workers=True, pin_memory=True, plot=False, positive_weight=0.5, prefetch_factor=2, real_value=True, rnn_units=16, seed=10000, static_features=['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density'], teacher_forcing=False, weight_decay=0.0, window_len=10)
2022-12-28 22:06: Argument batch_size: 256
2022-12-28 22:06: Argument clc: 'vec'
2022-12-28 22:06: Argument cuda: True
2022-12-28 22:06: Argument dataset: '2020'
2022-12-28 22:06: Argument dataset_root: '/home/joel.chacon/tmp/datasets_grl/'
2022-12-28 22:06: Argument debug: False
2022-12-28 22:06: Argument default_graph: True
2022-12-28 22:06: Argument device: 'cpu'
2022-12-28 22:06: Argument dynamic_features: ['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh']
2022-12-28 22:06: Argument early_stop: True
2022-12-28 22:06: Argument early_stop_patience: 8
2022-12-28 22:06: Argument embed_dim: 128
2022-12-28 22:06: Argument epochs: 30
2022-12-28 22:06: Argument grad_norm: False
2022-12-28 22:06: Argument horizon: 1
2022-12-28 22:06: Argument input_dim: 25
2022-12-28 22:06: Argument lag: 10
2022-12-28 22:06: Argument link_len: 2
2022-12-28 22:06: Argument log_dir: '/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122822060495293869118'
2022-12-28 22:06: Argument log_step: 1
2022-12-28 22:06: Argument loss_func: 'nllloss'
2022-12-28 22:06: Argument lr_decay: True
2022-12-28 22:06: Argument lr_decay_rate: 0.1
2022-12-28 22:06: Argument lr_decay_step: '15, 20'
2022-12-28 22:06: Argument lr_init: 0.0005
2022-12-28 22:06: Argument max_grad_norm: 5
2022-12-28 22:06: Argument minbatch_size: 256
2022-12-28 22:06: Argument mode: 'train'
2022-12-28 22:06: Argument model: 'fire_GCN'
2022-12-28 22:06: Argument nan_fill: 0.5
2022-12-28 22:06: Argument num_layers: 1
2022-12-28 22:06: Argument num_nodes: 625
2022-12-28 22:06: Argument num_workers: 20
2022-12-28 22:06: Argument output_dim: 2
2022-12-28 22:06: Argument patch_height: 25
2022-12-28 22:06: Argument patch_width: 25
2022-12-28 22:06: Argument persistent_workers: True
2022-12-28 22:06: Argument pin_memory: True
2022-12-28 22:06: Argument plot: False
2022-12-28 22:06: Argument positive_weight: 0.5
2022-12-28 22:06: Argument prefetch_factor: 2
2022-12-28 22:06: Argument real_value: True
2022-12-28 22:06: Argument rnn_units: 16
2022-12-28 22:06: Argument seed: 10000
2022-12-28 22:06: Argument static_features: ['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density']
2022-12-28 22:06: Argument teacher_forcing: False
2022-12-28 22:06: Argument weight_decay: 0.0
2022-12-28 22:06: Argument window_len: 10
++++++++++++++
2020_fire_GCN.conf
++++++++++++++
*****************Model Parameter*****************
node_embeddings torch.Size([625, 128]) True
encoder.cell_list.0.gate.weights_pool torch.Size([128, 2, 41, 16]) True
encoder.cell_list.0.gate.weights_window torch.Size([128, 1, 16]) True
encoder.cell_list.0.gate.bias_pool torch.Size([128, 32]) True
encoder.cell_list.0.gate.T torch.Size([10]) True
encoder.cell_list.0.gate.ln1.weight torch.Size([16]) True
encoder.cell_list.0.gate.ln1.bias torch.Size([16]) True
encoder.cell_list.0.gate.ln2.weight torch.Size([16]) True
encoder.cell_list.0.gate.ln2.bias torch.Size([16]) True
encoder.cell_list.0.update.weights_pool torch.Size([128, 2, 41, 8]) True
encoder.cell_list.0.update.weights_window torch.Size([128, 1, 8]) True
encoder.cell_list.0.update.bias_pool torch.Size([128, 16]) True
encoder.cell_list.0.update.T torch.Size([10]) True
encoder.cell_list.0.update.ln1.weight torch.Size([8]) True
encoder.cell_list.0.update.ln1.bias torch.Size([8]) True
encoder.cell_list.0.update.ln2.weight torch.Size([8]) True
encoder.cell_list.0.update.ln2.bias torch.Size([8]) True
conv1.weight torch.Size([16, 16, 3, 3]) True
conv1.bias torch.Size([16]) True
fc1.weight torch.Size([32, 2304]) True
fc1.bias torch.Size([32]) True
fc2.weight torch.Size([16, 32]) True
fc2.bias torch.Size([16]) True
fc3.weight torch.Size([2, 16]) True
fc3.bias torch.Size([2]) True
Total params num: 417878
*****************Finish Parameter****************
Positives: 5201 / Negatives: 10402
Dataset length 15603
Positives: 1300 / Negatives: 2600
Dataset length 3900
Positives: 1228 / Negatives: 2456
Dataset length 3684
Applying learning rate decay.
Creat Log File in:  /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122822060495293869118/run.log
2022-12-28 22:06: Train Epoch 1: 0/61 Loss: 0.666698
2022-12-28 22:06: Train Epoch 1: 1/61 Loss: 1.027993
2022-12-28 22:06: Train Epoch 1: 2/61 Loss: 0.619488
2022-12-28 22:06: Train Epoch 1: 3/61 Loss: 0.634939
2022-12-28 22:07: Train Epoch 1: 4/61 Loss: 0.650613
2022-12-28 22:07: Train Epoch 1: 5/61 Loss: 0.626077
2022-12-28 22:07: Train Epoch 1: 6/61 Loss: 0.573650
2022-12-28 22:07: Train Epoch 1: 7/61 Loss: 0.615692
2022-12-28 22:07: Train Epoch 1: 8/61 Loss: 0.569344
2022-12-28 22:07: Train Epoch 1: 9/61 Loss: 0.627181
2022-12-28 22:07: Train Epoch 1: 10/61 Loss: 0.584810
2022-12-28 22:08: Train Epoch 1: 11/61 Loss: 0.584489
2022-12-28 22:08: Train Epoch 1: 12/61 Loss: 0.582001
2022-12-28 22:08: Train Epoch 1: 13/61 Loss: 0.551617
2022-12-28 22:08: Train Epoch 1: 14/61 Loss: 0.526654
2022-12-28 22:08: Train Epoch 1: 15/61 Loss: 0.537404
2022-12-28 22:08: Train Epoch 1: 16/61 Loss: 0.537995
2022-12-28 22:08: Train Epoch 1: 17/61 Loss: 0.552777
2022-12-28 22:09: Train Epoch 1: 18/61 Loss: 0.504997
2022-12-28 22:09: Train Epoch 1: 19/61 Loss: 0.473122
2022-12-28 22:09: Train Epoch 1: 20/61 Loss: 0.496182
2022-12-28 22:09: Train Epoch 1: 21/61 Loss: 0.470517
2022-12-28 22:09: Train Epoch 1: 22/61 Loss: 0.444916
2022-12-28 22:09: Train Epoch 1: 23/61 Loss: 0.469385
2022-12-28 22:10: Train Epoch 1: 24/61 Loss: 0.439688
2022-12-28 22:10: Train Epoch 1: 25/61 Loss: 0.416271
2022-12-28 22:10: Train Epoch 1: 26/61 Loss: 0.419527
2022-12-28 22:10: Train Epoch 1: 27/61 Loss: 0.382554
2022-12-28 22:10: Train Epoch 1: 28/61 Loss: 0.396404
2022-12-28 22:10: Train Epoch 1: 29/61 Loss: 0.393209
2022-12-28 22:11: Train Epoch 1: 30/61 Loss: 0.354038
2022-12-28 22:11: Train Epoch 1: 31/61 Loss: 0.318739
2022-12-28 22:11: Train Epoch 1: 32/61 Loss: 0.317872
2022-12-28 22:11: Train Epoch 1: 33/61 Loss: 0.347591
2022-12-28 22:11: Train Epoch 1: 34/61 Loss: 0.306904
2022-12-28 22:11: Train Epoch 1: 35/61 Loss: 0.303286
2022-12-28 22:12: Train Epoch 1: 36/61 Loss: 0.279752
2022-12-28 22:12: Train Epoch 1: 37/61 Loss: 0.299193
2022-12-28 22:12: Train Epoch 1: 38/61 Loss: 0.362633
2022-12-28 22:12: Train Epoch 1: 39/61 Loss: 0.263740
