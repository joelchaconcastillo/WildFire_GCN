2022-12-31 11:15: log dir: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022123111154879899654013
2022-12-31 11:15: Experiment log path in: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022123111154879899654013
2022-12-31 11:15: Argument: Namespace(batch_size=256, clc='vec', cuda=True, dataset='2020', dataset_root='/home/joel.chacon/tmp/datasets_grl/', debug=False, default_graph=True, device='cpu', dynamic_features=['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh'], early_stop=True, early_stop_patience=8, embed_dim=128, epochs=30, grad_norm=False, horizon=1, input_dim=25, lag=10, link_len=2, log_dir='/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022123111154879899654013', log_step=1, loss_func='nllloss', lr_decay=True, lr_decay_rate=0.1, lr_decay_step='15, 20', lr_init=0.0001, max_grad_norm=5, minbatch_size=64, mode='train', model='fire_GCN', nan_fill=-1.0, num_layers=1, num_nodes=625, num_workers=12, output_dim=2, patch_height=25, patch_width=25, persistent_workers=True, pin_memory=True, plot=False, positive_weight=0.5, prefetch_factor=2, real_value=True, rnn_units=128, seed=10000, static_features=['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density'], teacher_forcing=False, weight_decay=0.0, window_len=10)
2022-12-31 11:15: Argument batch_size: 256
2022-12-31 11:15: Argument clc: 'vec'
2022-12-31 11:15: Argument cuda: True
2022-12-31 11:15: Argument dataset: '2020'
2022-12-31 11:15: Argument dataset_root: '/home/joel.chacon/tmp/datasets_grl/'
2022-12-31 11:15: Argument debug: False
2022-12-31 11:15: Argument default_graph: True
2022-12-31 11:15: Argument device: 'cpu'
2022-12-31 11:15: Argument dynamic_features: ['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh']
2022-12-31 11:15: Argument early_stop: True
2022-12-31 11:15: Argument early_stop_patience: 8
2022-12-31 11:15: Argument embed_dim: 128
2022-12-31 11:15: Argument epochs: 30
2022-12-31 11:15: Argument grad_norm: False
2022-12-31 11:15: Argument horizon: 1
2022-12-31 11:15: Argument input_dim: 25
2022-12-31 11:15: Argument lag: 10
2022-12-31 11:15: Argument link_len: 2
2022-12-31 11:15: Argument log_dir: '/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022123111154879899654013'
2022-12-31 11:15: Argument log_step: 1
2022-12-31 11:15: Argument loss_func: 'nllloss'
2022-12-31 11:15: Argument lr_decay: True
2022-12-31 11:15: Argument lr_decay_rate: 0.1
2022-12-31 11:15: Argument lr_decay_step: '15, 20'
2022-12-31 11:15: Argument lr_init: 0.0001
2022-12-31 11:15: Argument max_grad_norm: 5
2022-12-31 11:15: Argument minbatch_size: 64
2022-12-31 11:15: Argument mode: 'train'
2022-12-31 11:15: Argument model: 'fire_GCN'
2022-12-31 11:15: Argument nan_fill: -1.0
2022-12-31 11:15: Argument num_layers: 1
2022-12-31 11:15: Argument num_nodes: 625
2022-12-31 11:15: Argument num_workers: 12
2022-12-31 11:15: Argument output_dim: 2
2022-12-31 11:15: Argument patch_height: 25
2022-12-31 11:15: Argument patch_width: 25
2022-12-31 11:15: Argument persistent_workers: True
2022-12-31 11:15: Argument pin_memory: True
2022-12-31 11:15: Argument plot: False
2022-12-31 11:15: Argument positive_weight: 0.5
2022-12-31 11:15: Argument prefetch_factor: 2
2022-12-31 11:15: Argument real_value: True
2022-12-31 11:15: Argument rnn_units: 128
2022-12-31 11:15: Argument seed: 10000
2022-12-31 11:15: Argument static_features: ['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density']
2022-12-31 11:15: Argument teacher_forcing: False
2022-12-31 11:15: Argument weight_decay: 0.0
2022-12-31 11:15: Argument window_len: 10
++++++++++++++
2020_fire_GCN.conf
++++++++++++++
*****************Model Parameter*****************
node_embeddings torch.Size([625, 128]) True
ln1.weight torch.Size([25]) True
ln1.bias torch.Size([25]) True
encoder.cell_list.0.gate.weights_pool torch.Size([128, 2, 153, 128]) True
encoder.cell_list.0.gate.weights_window torch.Size([128, 1, 128]) True
encoder.cell_list.0.gate.bias_pool torch.Size([128, 256]) True
encoder.cell_list.0.gate.T torch.Size([10]) True
encoder.cell_list.0.update.weights_pool torch.Size([128, 2, 153, 64]) True
encoder.cell_list.0.update.weights_window torch.Size([128, 1, 64]) True
encoder.cell_list.0.update.bias_pool torch.Size([128, 128]) True
encoder.cell_list.0.update.T torch.Size([10]) True
fc1.weight torch.Size([2, 80000]) True
fc1.bias torch.Size([2]) True
Total params num: 7834056
*****************Finish Parameter****************
Positives: 13518 / Negatives: 27036
Dataset length 40554
Positives: 1300 / Negatives: 2600
Dataset length 3900
Positives: 1228 / Negatives: 2456
Dataset length 3684
Positives: 4407 / Negatives: 8814
Dataset length 13221
Applying learning rate decay.
Creat Log File in:  /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022123111154879899654013/run.log
2022-12-31 11:16: Train Epoch 1: 3/634 Loss: 0.368219
2022-12-31 11:17: Train Epoch 1: 7/634 Loss: 4.430859
2022-12-31 11:17: Train Epoch 1: 11/634 Loss: 1.936314
2022-12-31 11:18: Train Epoch 1: 15/634 Loss: 1.317997
2022-12-31 11:18: Train Epoch 1: 19/634 Loss: 0.682654
2022-12-31 11:19: Train Epoch 1: 23/634 Loss: 0.959595
2022-12-31 11:19: Train Epoch 1: 27/634 Loss: 0.442330
2022-12-31 11:20: Train Epoch 1: 31/634 Loss: 0.211855
2022-12-31 11:20: Train Epoch 1: 35/634 Loss: 0.412359
2022-12-31 11:21: Train Epoch 1: 39/634 Loss: 0.451431
2022-12-31 11:21: Train Epoch 1: 43/634 Loss: 0.520465
2022-12-31 11:22: Train Epoch 1: 47/634 Loss: 0.334219
2022-12-31 11:23: Train Epoch 1: 51/634 Loss: 0.308126
2022-12-31 11:23: Train Epoch 1: 55/634 Loss: 0.249770
2022-12-31 11:24: Train Epoch 1: 59/634 Loss: 0.362897
2022-12-31 11:24: Train Epoch 1: 63/634 Loss: 0.474947
2022-12-31 11:25: Train Epoch 1: 67/634 Loss: 0.383460
2022-12-31 11:25: Train Epoch 1: 71/634 Loss: 0.264350
2022-12-31 11:26: Train Epoch 1: 75/634 Loss: 0.257316
2022-12-31 11:27: Train Epoch 1: 79/634 Loss: 0.320359
2022-12-31 11:27: Train Epoch 1: 83/634 Loss: 0.274691
2022-12-31 11:28: Train Epoch 1: 87/634 Loss: 0.373312
2022-12-31 11:28: Train Epoch 1: 91/634 Loss: 0.279975
2022-12-31 11:29: Train Epoch 1: 95/634 Loss: 0.214196
2022-12-31 11:29: Train Epoch 1: 99/634 Loss: 0.265151
2022-12-31 11:30: Train Epoch 1: 103/634 Loss: 0.270899
2022-12-31 11:31: Train Epoch 1: 107/634 Loss: 0.322927
2022-12-31 11:31: Train Epoch 1: 111/634 Loss: 0.308356
2022-12-31 11:32: Train Epoch 1: 115/634 Loss: 0.267105
2022-12-31 11:32: Train Epoch 1: 119/634 Loss: 0.252100
2022-12-31 11:33: Train Epoch 1: 123/634 Loss: 0.250703
2022-12-31 11:33: Train Epoch 1: 127/634 Loss: 0.270008
2022-12-31 11:34: Train Epoch 1: 131/634 Loss: 0.310303
2022-12-31 11:35: Train Epoch 1: 135/634 Loss: 0.242202
2022-12-31 11:35: Train Epoch 1: 139/634 Loss: 0.199078
2022-12-31 11:36: Train Epoch 1: 143/634 Loss: 0.203263
2022-12-31 11:37: Train Epoch 1: 147/634 Loss: 0.217432
2022-12-31 11:37: Train Epoch 1: 151/634 Loss: 0.253091
2022-12-31 11:38: Train Epoch 1: 155/634 Loss: 0.273308
2022-12-31 11:38: Train Epoch 1: 159/634 Loss: 0.210347
2022-12-31 11:39: Train Epoch 1: 163/634 Loss: 0.225555
2022-12-31 11:40: Train Epoch 1: 167/634 Loss: 0.236638
2022-12-31 11:40: Train Epoch 1: 171/634 Loss: 0.319928
2022-12-31 11:41: Train Epoch 1: 175/634 Loss: 0.236811
2022-12-31 11:42: Train Epoch 1: 179/634 Loss: 0.260295
2022-12-31 11:42: Train Epoch 1: 183/634 Loss: 0.208807
2022-12-31 11:43: Train Epoch 1: 187/634 Loss: 0.212435
2022-12-31 11:43: Train Epoch 1: 191/634 Loss: 0.232619
2022-12-31 11:44: Train Epoch 1: 195/634 Loss: 0.198708
2022-12-31 11:45: Train Epoch 1: 199/634 Loss: 0.243168
2022-12-31 11:45: Train Epoch 1: 203/634 Loss: 0.202568
2022-12-31 11:46: Train Epoch 1: 207/634 Loss: 0.201262
2022-12-31 11:47: Train Epoch 1: 211/634 Loss: 0.204464
2022-12-31 11:47: Train Epoch 1: 215/634 Loss: 0.241246
2022-12-31 11:48: Train Epoch 1: 219/634 Loss: 0.209276
2022-12-31 11:49: Train Epoch 1: 223/634 Loss: 0.205884
2022-12-31 11:49: Train Epoch 1: 227/634 Loss: 0.191347
2022-12-31 11:50: Train Epoch 1: 231/634 Loss: 0.214523
2022-12-31 11:50: Train Epoch 1: 235/634 Loss: 0.220208
2022-12-31 11:51: Train Epoch 1: 239/634 Loss: 0.215497
2022-12-31 11:52: Train Epoch 1: 243/634 Loss: 0.211330
2022-12-31 11:52: Train Epoch 1: 247/634 Loss: 0.200895
2022-12-31 11:53: Train Epoch 1: 251/634 Loss: 0.242361
2022-12-31 11:54: Train Epoch 1: 255/634 Loss: 0.245877
2022-12-31 11:54: Train Epoch 1: 259/634 Loss: 0.171515
2022-12-31 11:55: Train Epoch 1: 263/634 Loss: 0.229135
2022-12-31 11:55: Train Epoch 1: 267/634 Loss: 0.284911
2022-12-31 11:56: Train Epoch 1: 271/634 Loss: 0.206779
2022-12-31 11:57: Train Epoch 1: 275/634 Loss: 0.211225
2022-12-31 11:57: Train Epoch 1: 279/634 Loss: 0.225757
2022-12-31 11:58: Train Epoch 1: 283/634 Loss: 0.207675
2022-12-31 11:58: Train Epoch 1: 287/634 Loss: 0.191374
2022-12-31 11:59: Train Epoch 1: 291/634 Loss: 0.280802
2022-12-31 12:00: Train Epoch 1: 295/634 Loss: 0.185817
2022-12-31 12:00: Train Epoch 1: 299/634 Loss: 0.212388
2022-12-31 12:01: Train Epoch 1: 303/634 Loss: 0.192769
2022-12-31 12:02: Train Epoch 1: 307/634 Loss: 0.232277
2022-12-31 12:02: Train Epoch 1: 311/634 Loss: 0.215745
2022-12-31 12:03: Train Epoch 1: 315/634 Loss: 0.205199
2022-12-31 12:03: Train Epoch 1: 319/634 Loss: 0.203316
2022-12-31 12:04: Train Epoch 1: 323/634 Loss: 0.212473
2022-12-31 12:05: Train Epoch 1: 327/634 Loss: 0.184538
2022-12-31 12:05: Train Epoch 1: 331/634 Loss: 0.211792
2022-12-31 12:06: Train Epoch 1: 335/634 Loss: 0.192163
2022-12-31 12:06: Train Epoch 1: 339/634 Loss: 0.228631
2022-12-31 12:07: Train Epoch 1: 343/634 Loss: 0.197680
2022-12-31 12:08: Train Epoch 1: 347/634 Loss: 0.208098
2022-12-31 12:08: Train Epoch 1: 351/634 Loss: 0.218347
2022-12-31 12:09: Train Epoch 1: 355/634 Loss: 0.256236
2022-12-31 12:10: Train Epoch 1: 359/634 Loss: 0.209719
2022-12-31 12:10: Train Epoch 1: 363/634 Loss: 0.251096
2022-12-31 12:11: Train Epoch 1: 367/634 Loss: 0.198084
2022-12-31 12:11: Train Epoch 1: 371/634 Loss: 0.210038
2022-12-31 12:12: Train Epoch 1: 375/634 Loss: 0.202735
2022-12-31 12:13: Train Epoch 1: 379/634 Loss: 0.199985
2022-12-31 12:13: Train Epoch 1: 383/634 Loss: 0.203407
2022-12-31 12:14: Train Epoch 1: 387/634 Loss: 0.233657
2022-12-31 12:15: Train Epoch 1: 391/634 Loss: 0.205382
2022-12-31 12:15: Train Epoch 1: 395/634 Loss: 0.169921
2022-12-31 12:16: Train Epoch 1: 399/634 Loss: 0.237128
2022-12-31 12:16: Train Epoch 1: 403/634 Loss: 0.216385
2022-12-31 12:17: Train Epoch 1: 407/634 Loss: 0.221565
2022-12-31 12:18: Train Epoch 1: 411/634 Loss: 0.189057
2022-12-31 12:18: Train Epoch 1: 415/634 Loss: 0.219320
2022-12-31 12:19: Train Epoch 1: 419/634 Loss: 0.218020
2022-12-31 12:20: Train Epoch 1: 423/634 Loss: 0.174064
2022-12-31 12:20: Train Epoch 1: 427/634 Loss: 0.209291
2022-12-31 12:21: Train Epoch 1: 431/634 Loss: 0.224874
2022-12-31 12:22: Train Epoch 1: 435/634 Loss: 0.206889
2022-12-31 12:22: Train Epoch 1: 439/634 Loss: 0.238408
2022-12-31 12:23: Train Epoch 1: 443/634 Loss: 0.195699
2022-12-31 12:23: Train Epoch 1: 447/634 Loss: 0.215198
2022-12-31 12:24: Train Epoch 1: 451/634 Loss: 0.238438
2022-12-31 12:25: Train Epoch 1: 455/634 Loss: 0.154510
2022-12-31 12:25: Train Epoch 1: 459/634 Loss: 0.309012
2022-12-31 12:26: Train Epoch 1: 463/634 Loss: 0.206603
2022-12-31 12:27: Train Epoch 1: 467/634 Loss: 0.264696
2022-12-31 12:27: Train Epoch 1: 471/634 Loss: 0.251455
2022-12-31 12:28: Train Epoch 1: 475/634 Loss: 0.209567
2022-12-31 12:28: Train Epoch 1: 479/634 Loss: 0.316875
2022-12-31 12:29: Train Epoch 1: 483/634 Loss: 0.197372
2022-12-31 12:30: Train Epoch 1: 487/634 Loss: 0.245548
2022-12-31 12:30: Train Epoch 1: 491/634 Loss: 0.308413
2022-12-31 12:31: Train Epoch 1: 495/634 Loss: 0.197162
2022-12-31 12:32: Train Epoch 1: 499/634 Loss: 0.218790
2022-12-31 12:32: Train Epoch 1: 503/634 Loss: 0.230438
2022-12-31 12:33: Train Epoch 1: 507/634 Loss: 0.191369
2022-12-31 12:33: Train Epoch 1: 511/634 Loss: 0.234861
2022-12-31 12:34: Train Epoch 1: 515/634 Loss: 0.215404
2022-12-31 12:35: Train Epoch 1: 519/634 Loss: 0.222623
2022-12-31 12:35: Train Epoch 1: 523/634 Loss: 0.184032
2022-12-31 12:36: Train Epoch 1: 527/634 Loss: 0.228643
2022-12-31 12:37: Train Epoch 1: 531/634 Loss: 0.216396
2022-12-31 12:37: Train Epoch 1: 535/634 Loss: 0.322332
2022-12-31 12:38: Train Epoch 1: 539/634 Loss: 0.257119
2022-12-31 12:38: Train Epoch 1: 543/634 Loss: 0.215201
2022-12-31 12:39: Train Epoch 1: 547/634 Loss: 0.277382
2022-12-31 12:40: Train Epoch 1: 551/634 Loss: 0.268809
2022-12-31 12:40: Train Epoch 1: 555/634 Loss: 0.244311
2022-12-31 12:41: Train Epoch 1: 559/634 Loss: 0.286306
2022-12-31 12:42: Train Epoch 1: 563/634 Loss: 0.292657
2022-12-31 12:42: Train Epoch 1: 567/634 Loss: 0.223098
2022-12-31 12:43: Train Epoch 1: 571/634 Loss: 0.243615
2022-12-31 12:43: Train Epoch 1: 575/634 Loss: 0.224822
2022-12-31 12:44: Train Epoch 1: 579/634 Loss: 0.251218
2022-12-31 12:45: Train Epoch 1: 583/634 Loss: 0.244264
2022-12-31 12:45: Train Epoch 1: 587/634 Loss: 0.230064
2022-12-31 12:46: Train Epoch 1: 591/634 Loss: 0.183300
2022-12-31 12:47: Train Epoch 1: 595/634 Loss: 0.262273
2022-12-31 12:47: Train Epoch 1: 599/634 Loss: 0.203747
2022-12-31 12:48: Train Epoch 1: 603/634 Loss: 0.224219
2022-12-31 12:48: Train Epoch 1: 607/634 Loss: 0.258539
2022-12-31 12:49: Train Epoch 1: 611/634 Loss: 0.238159
2022-12-31 12:50: Train Epoch 1: 615/634 Loss: 0.191583
2022-12-31 12:50: Train Epoch 1: 619/634 Loss: 0.239819
2022-12-31 12:51: Train Epoch 1: 623/634 Loss: 0.180588
2022-12-31 12:52: Train Epoch 1: 627/634 Loss: 0.203195
2022-12-31 12:52: Train Epoch 1: 631/634 Loss: 0.224530
2022-12-31 12:52: Train Epoch 1: 633/634 Loss: 0.118233
2022-12-31 12:52: **********Train Epoch 1: averaged Loss: 0.292439 
2022-12-31 12:52: 
Epoch time elapsed: 5824.9223845005035

2022-12-31 12:55: 
 metrics validation: {'precision': 0.7150928167877321, 'recall': 0.6815384615384615, 'f1-score': 0.6979125640015754, 'support': 1300, 'AUC': 0.8396644970414201, 'AUCPR': 0.7588272447919565, 'TP': 886, 'FP': 353, 'TN': 2247, 'FN': 414} 

2022-12-31 12:55: **********Val Epoch 1: average Loss: 0.229852
2022-12-31 12:55: *********************************Current best model saved!
2022-12-31 12:57: 
 Testing metrics {'precision': 0.7675675675675676, 'recall': 0.6938110749185668, 'f1-score': 0.7288280581693756, 'support': 1228, 'AUC': 0.8641086377574296, 'AUCPR': 0.7950793558941145, 'TP': 852, 'FP': 258, 'TN': 2198, 'FN': 376} 

2022-12-31 13:05: 
 Testing metrics {'precision': 0.8792990142387733, 'recall': 0.9108236895847516, 'f1-score': 0.8947837717342844, 'support': 4407, 'AUC': 0.9703618884266727, 'AUCPR': 0.9417289455829225, 'TP': 4014, 'FP': 551, 'TN': 8263, 'FN': 393} 

2022-12-31 13:06: Train Epoch 2: 3/634 Loss: 0.195921
2022-12-31 13:07: Train Epoch 2: 7/634 Loss: 0.182918
2022-12-31 13:07: Train Epoch 2: 11/634 Loss: 0.197129
2022-12-31 13:08: Train Epoch 2: 15/634 Loss: 0.210475
2022-12-31 13:08: Train Epoch 2: 19/634 Loss: 0.210698
2022-12-31 13:09: Train Epoch 2: 23/634 Loss: 0.218587
2022-12-31 13:10: Train Epoch 2: 27/634 Loss: 0.198447
2022-12-31 13:10: Train Epoch 2: 31/634 Loss: 0.217654
2022-12-31 13:11: Train Epoch 2: 35/634 Loss: 0.241914
2022-12-31 13:11: Train Epoch 2: 39/634 Loss: 0.196427
2022-12-31 13:12: Train Epoch 2: 43/634 Loss: 0.211190
2022-12-31 13:13: Train Epoch 2: 47/634 Loss: 0.204850
2022-12-31 13:13: Train Epoch 2: 51/634 Loss: 0.209262
2022-12-31 13:14: Train Epoch 2: 55/634 Loss: 0.221961
2022-12-31 13:14: Train Epoch 2: 59/634 Loss: 0.229784
2022-12-31 13:15: Train Epoch 2: 63/634 Loss: 0.189487
2022-12-31 13:16: Train Epoch 2: 67/634 Loss: 0.235663
2022-12-31 13:16: Train Epoch 2: 71/634 Loss: 0.217223
2022-12-31 13:17: Train Epoch 2: 75/634 Loss: 0.197629
2022-12-31 13:17: Train Epoch 2: 79/634 Loss: 0.206791
2022-12-31 13:18: Train Epoch 2: 83/634 Loss: 0.206467
2022-12-31 13:18: Train Epoch 2: 87/634 Loss: 0.225211
2022-12-31 13:19: Train Epoch 2: 91/634 Loss: 0.189925
2022-12-31 13:20: Train Epoch 2: 95/634 Loss: 0.226106
2022-12-31 13:20: Train Epoch 2: 99/634 Loss: 0.197523
2022-12-31 13:21: Train Epoch 2: 103/634 Loss: 0.188886
2022-12-31 13:22: Train Epoch 2: 107/634 Loss: 0.198334
2022-12-31 13:22: Train Epoch 2: 111/634 Loss: 0.214238
2022-12-31 13:23: Train Epoch 2: 115/634 Loss: 0.238396
2022-12-31 13:23: Train Epoch 2: 119/634 Loss: 0.204321
2022-12-31 13:24: Train Epoch 2: 123/634 Loss: 0.183838
2022-12-31 13:24: Train Epoch 2: 127/634 Loss: 0.190850
2022-12-31 13:25: Train Epoch 2: 131/634 Loss: 0.206359
2022-12-31 13:26: Train Epoch 2: 135/634 Loss: 0.197308
2022-12-31 13:26: Train Epoch 2: 139/634 Loss: 0.198290
2022-12-31 13:27: Train Epoch 2: 143/634 Loss: 0.200371
2022-12-31 13:27: Train Epoch 2: 147/634 Loss: 0.191560
2022-12-31 13:28: Train Epoch 2: 151/634 Loss: 0.194608
2022-12-31 13:29: Train Epoch 2: 155/634 Loss: 0.193147
2022-12-31 13:29: Train Epoch 2: 159/634 Loss: 0.179126
2022-12-31 13:30: Train Epoch 2: 163/634 Loss: 0.192580
2022-12-31 13:31: Train Epoch 2: 167/634 Loss: 0.194839
2022-12-31 13:31: Train Epoch 2: 171/634 Loss: 0.178777
2022-12-31 13:32: Train Epoch 2: 175/634 Loss: 0.198787
2022-12-31 13:33: Train Epoch 2: 179/634 Loss: 0.180903
2022-12-31 13:33: Train Epoch 2: 183/634 Loss: 0.184948
2022-12-31 13:34: Train Epoch 2: 187/634 Loss: 0.208594
2022-12-31 13:35: Train Epoch 2: 191/634 Loss: 0.189784
2022-12-31 13:35: Train Epoch 2: 195/634 Loss: 0.172157
2022-12-31 13:36: Train Epoch 2: 199/634 Loss: 0.224116
2022-12-31 13:37: Train Epoch 2: 203/634 Loss: 0.203224
2022-12-31 13:37: Train Epoch 2: 207/634 Loss: 0.236413
2022-12-31 13:38: Train Epoch 2: 211/634 Loss: 0.188856
2022-12-31 13:39: Train Epoch 2: 215/634 Loss: 0.184216
2022-12-31 13:39: Train Epoch 2: 219/634 Loss: 0.165433
2022-12-31 13:40: Train Epoch 2: 223/634 Loss: 0.191301
2022-12-31 13:41: Train Epoch 2: 227/634 Loss: 0.195103
2022-12-31 13:41: Train Epoch 2: 231/634 Loss: 0.181476
2022-12-31 13:42: Train Epoch 2: 235/634 Loss: 0.213363
2022-12-31 13:43: Train Epoch 2: 239/634 Loss: 0.192900
2022-12-31 13:43: Train Epoch 2: 243/634 Loss: 0.209021
2022-12-31 13:44: Train Epoch 2: 247/634 Loss: 0.228300
2022-12-31 13:44: Train Epoch 2: 251/634 Loss: 0.236343
2022-12-31 13:45: Train Epoch 2: 255/634 Loss: 0.193829
2022-12-31 13:46: Train Epoch 2: 259/634 Loss: 0.185030
2022-12-31 13:46: Train Epoch 2: 263/634 Loss: 0.227974
2022-12-31 13:47: Train Epoch 2: 267/634 Loss: 0.226908
2022-12-31 13:48: Train Epoch 2: 271/634 Loss: 0.191981
2022-12-31 13:48: Train Epoch 2: 275/634 Loss: 0.179688
2022-12-31 13:49: Train Epoch 2: 279/634 Loss: 0.218112
2022-12-31 13:50: Train Epoch 2: 283/634 Loss: 0.218092
2022-12-31 13:50: Train Epoch 2: 287/634 Loss: 0.226319
2022-12-31 13:51: Train Epoch 2: 291/634 Loss: 0.203724
2022-12-31 13:52: Train Epoch 2: 295/634 Loss: 0.191882
2022-12-31 13:52: Train Epoch 2: 299/634 Loss: 0.265016
2022-12-31 13:53: Train Epoch 2: 303/634 Loss: 0.211696
2022-12-31 13:54: Train Epoch 2: 307/634 Loss: 0.262177
2022-12-31 13:54: Train Epoch 2: 311/634 Loss: 0.206614
2022-12-31 13:55: Train Epoch 2: 315/634 Loss: 0.213695
2022-12-31 13:56: Train Epoch 2: 319/634 Loss: 0.230291
2022-12-31 13:56: Train Epoch 2: 323/634 Loss: 0.171632
2022-12-31 13:57: Train Epoch 2: 327/634 Loss: 0.210570
2022-12-31 13:58: Train Epoch 2: 331/634 Loss: 0.183294
2022-12-31 13:58: Train Epoch 2: 335/634 Loss: 0.206572
2022-12-31 13:59: Train Epoch 2: 339/634 Loss: 0.208430
2022-12-31 14:00: Train Epoch 2: 343/634 Loss: 0.175974
2022-12-31 14:00: Train Epoch 2: 347/634 Loss: 0.240312
2022-12-31 14:01: Train Epoch 2: 351/634 Loss: 0.186924
2022-12-31 14:02: Train Epoch 2: 355/634 Loss: 0.170220
2022-12-31 14:02: Train Epoch 2: 359/634 Loss: 0.207242
2022-12-31 14:03: Train Epoch 2: 363/634 Loss: 0.193559
2022-12-31 14:04: Train Epoch 2: 367/634 Loss: 0.262268
2022-12-31 14:04: Train Epoch 2: 371/634 Loss: 0.212442
2022-12-31 14:05: Train Epoch 2: 375/634 Loss: 0.200822
2022-12-31 14:06: Train Epoch 2: 379/634 Loss: 0.240939
2022-12-31 14:06: Train Epoch 2: 383/634 Loss: 0.178150
2022-12-31 14:07: Train Epoch 2: 387/634 Loss: 0.211831
2022-12-31 14:08: Train Epoch 2: 391/634 Loss: 0.217355
2022-12-31 14:08: Train Epoch 2: 395/634 Loss: 0.231062
2022-12-31 14:09: Train Epoch 2: 399/634 Loss: 0.239950
2022-12-31 14:10: Train Epoch 2: 403/634 Loss: 0.258930
2022-12-31 14:11: Train Epoch 2: 407/634 Loss: 0.226917
2022-12-31 14:11: Train Epoch 2: 411/634 Loss: 0.208081
2022-12-31 14:12: Train Epoch 2: 415/634 Loss: 0.226769
2022-12-31 14:13: Train Epoch 2: 419/634 Loss: 0.186059
2022-12-31 14:13: Train Epoch 2: 423/634 Loss: 0.219132
2022-12-31 14:14: Train Epoch 2: 427/634 Loss: 0.193985
2022-12-31 14:15: Train Epoch 2: 431/634 Loss: 0.209177
2022-12-31 14:16: Train Epoch 2: 435/634 Loss: 0.180405
2022-12-31 14:16: Train Epoch 2: 439/634 Loss: 0.208328
2022-12-31 14:17: Train Epoch 2: 443/634 Loss: 0.181738
2022-12-31 14:18: Train Epoch 2: 447/634 Loss: 0.186981
2022-12-31 14:18: Train Epoch 2: 451/634 Loss: 0.197220
2022-12-31 14:19: Train Epoch 2: 455/634 Loss: 0.185785
2022-12-31 14:20: Train Epoch 2: 459/634 Loss: 0.222096
2022-12-31 14:20: Train Epoch 2: 463/634 Loss: 0.223055
2022-12-31 14:21: Train Epoch 2: 467/634 Loss: 0.188040
2022-12-31 14:22: Train Epoch 2: 471/634 Loss: 0.251747
2022-12-31 14:22: Train Epoch 2: 475/634 Loss: 0.192228
2022-12-31 14:23: Train Epoch 2: 479/634 Loss: 0.211442
2022-12-31 14:23: Train Epoch 2: 483/634 Loss: 0.186742
2022-12-31 14:24: Train Epoch 2: 487/634 Loss: 0.186720
2022-12-31 14:25: Train Epoch 2: 491/634 Loss: 0.189866
2022-12-31 14:25: Train Epoch 2: 495/634 Loss: 0.175390
2022-12-31 14:26: Train Epoch 2: 499/634 Loss: 0.180343
2022-12-31 14:27: Train Epoch 2: 503/634 Loss: 0.210400
2022-12-31 14:27: Train Epoch 2: 507/634 Loss: 0.194137
2022-12-31 14:28: Train Epoch 2: 511/634 Loss: 0.190670
2022-12-31 14:29: Train Epoch 2: 515/634 Loss: 0.197298
2022-12-31 14:29: Train Epoch 2: 519/634 Loss: 0.230354
2022-12-31 14:30: Train Epoch 2: 523/634 Loss: 0.175230
2022-12-31 14:31: Train Epoch 2: 527/634 Loss: 0.192179
2022-12-31 14:31: Train Epoch 2: 531/634 Loss: 0.233792
2022-12-31 14:32: Train Epoch 2: 535/634 Loss: 0.227361
2022-12-31 14:33: Train Epoch 2: 539/634 Loss: 0.178261
2022-12-31 14:33: Train Epoch 2: 543/634 Loss: 0.215461
2022-12-31 14:34: Train Epoch 2: 547/634 Loss: 0.257894
2022-12-31 14:35: Train Epoch 2: 551/634 Loss: 0.201823
2022-12-31 14:35: Train Epoch 2: 555/634 Loss: 0.188205
2022-12-31 14:36: Train Epoch 2: 559/634 Loss: 0.173276
2022-12-31 14:37: Train Epoch 2: 563/634 Loss: 0.191421
2022-12-31 14:37: Train Epoch 2: 567/634 Loss: 0.215749
2022-12-31 14:38: Train Epoch 2: 571/634 Loss: 0.180842
2022-12-31 14:39: Train Epoch 2: 575/634 Loss: 0.187559
2022-12-31 14:39: Train Epoch 2: 579/634 Loss: 0.201451
2022-12-31 14:40: Train Epoch 2: 583/634 Loss: 0.245747
2022-12-31 14:41: Train Epoch 2: 587/634 Loss: 0.209679
2022-12-31 14:41: Train Epoch 2: 591/634 Loss: 0.189377
2022-12-31 14:42: Train Epoch 2: 595/634 Loss: 0.204243
2022-12-31 14:43: Train Epoch 2: 599/634 Loss: 0.176616
2022-12-31 14:43: Train Epoch 2: 603/634 Loss: 0.240414
2022-12-31 14:44: Train Epoch 2: 607/634 Loss: 0.204375
2022-12-31 14:45: Train Epoch 2: 611/634 Loss: 0.221646
2022-12-31 14:45: Train Epoch 2: 615/634 Loss: 0.193902
2022-12-31 14:46: Train Epoch 2: 619/634 Loss: 0.185334
2022-12-31 14:47: Train Epoch 2: 623/634 Loss: 0.183304
2022-12-31 14:47: Train Epoch 2: 627/634 Loss: 0.176028
2022-12-31 14:48: Train Epoch 2: 631/634 Loss: 0.181129
2022-12-31 14:48: Train Epoch 2: 633/634 Loss: 0.076766
2022-12-31 14:48: **********Train Epoch 2: averaged Loss: 0.203660 
2022-12-31 14:48: 
Epoch time elapsed: 6178.473130941391

2022-12-31 14:50: 
 metrics validation: {'precision': 0.741042345276873, 'recall': 0.7, 'f1-score': 0.7199367088607596, 'support': 1300, 'AUC': 0.859407100591716, 'AUCPR': 0.7845527665225793, 'TP': 910, 'FP': 318, 'TN': 2282, 'FN': 390} 

2022-12-31 14:50: **********Val Epoch 2: average Loss: 0.213314
2022-12-31 14:50: *********************************Current best model saved!
2022-12-31 14:52: 
 Testing metrics {'precision': 0.7731958762886598, 'recall': 0.6718241042345277, 'f1-score': 0.7189542483660131, 'support': 1228, 'AUC': 0.8719714532780188, 'AUCPR': 0.8041723404499461, 'TP': 825, 'FP': 242, 'TN': 2214, 'FN': 403} 

2022-12-31 15:00: 
 Testing metrics {'precision': 0.8811511423550088, 'recall': 0.910142954390742, 'f1-score': 0.8954124344234848, 'support': 4407, 'AUC': 0.9723697508898445, 'AUCPR': 0.9462346026246912, 'TP': 4011, 'FP': 541, 'TN': 8273, 'FN': 396} 

2022-12-31 15:00: Train Epoch 3: 3/634 Loss: 0.154271
2022-12-31 15:01: Train Epoch 3: 7/634 Loss: 0.174286
2022-12-31 15:02: Train Epoch 3: 11/634 Loss: 0.200077
2022-12-31 15:03: Train Epoch 3: 15/634 Loss: 0.206524
2022-12-31 15:03: Train Epoch 3: 19/634 Loss: 0.189603
2022-12-31 15:04: Train Epoch 3: 23/634 Loss: 0.193977
2022-12-31 15:05: Train Epoch 3: 27/634 Loss: 0.218914
2022-12-31 15:05: Train Epoch 3: 31/634 Loss: 0.192989
2022-12-31 15:06: Train Epoch 3: 35/634 Loss: 0.193383
2022-12-31 15:07: Train Epoch 3: 39/634 Loss: 0.183779
2022-12-31 15:07: Train Epoch 3: 43/634 Loss: 0.184052
2022-12-31 15:08: Train Epoch 3: 47/634 Loss: 0.190798
2022-12-31 15:09: Train Epoch 3: 51/634 Loss: 0.192204
2022-12-31 15:09: Train Epoch 3: 55/634 Loss: 0.178230
2022-12-31 15:10: Train Epoch 3: 59/634 Loss: 0.196402
2022-12-31 15:11: Train Epoch 3: 63/634 Loss: 0.201538
2022-12-31 15:12: Train Epoch 3: 67/634 Loss: 0.220312
2022-12-31 15:12: Train Epoch 3: 71/634 Loss: 0.195808
2022-12-31 15:13: Train Epoch 3: 75/634 Loss: 0.192193
2022-12-31 15:14: Train Epoch 3: 79/634 Loss: 0.200466
2022-12-31 15:15: Train Epoch 3: 83/634 Loss: 0.175974
2022-12-31 15:15: Train Epoch 3: 87/634 Loss: 0.184020
2022-12-31 15:16: Train Epoch 3: 91/634 Loss: 0.218244
2022-12-31 15:17: Train Epoch 3: 95/634 Loss: 0.153384
2022-12-31 15:17: Train Epoch 3: 99/634 Loss: 0.169628
2022-12-31 15:18: Train Epoch 3: 103/634 Loss: 0.216359
2022-12-31 15:19: Train Epoch 3: 107/634 Loss: 0.211263
2022-12-31 15:20: Train Epoch 3: 111/634 Loss: 0.180113
2022-12-31 15:21: Train Epoch 3: 115/634 Loss: 0.173515
2022-12-31 15:21: Train Epoch 3: 119/634 Loss: 0.191901
2022-12-31 15:22: Train Epoch 3: 123/634 Loss: 0.190382
2022-12-31 15:23: Train Epoch 3: 127/634 Loss: 0.202340
2022-12-31 15:23: Train Epoch 3: 131/634 Loss: 0.207360
2022-12-31 15:24: Train Epoch 3: 135/634 Loss: 0.222969
2022-12-31 15:25: Train Epoch 3: 139/634 Loss: 0.232893
2022-12-31 15:25: Train Epoch 3: 143/634 Loss: 0.176330
2022-12-31 15:26: Train Epoch 3: 147/634 Loss: 0.197472
2022-12-31 15:27: Train Epoch 3: 151/634 Loss: 0.186600
2022-12-31 15:27: Train Epoch 3: 155/634 Loss: 0.215497
2022-12-31 15:28: Train Epoch 3: 159/634 Loss: 0.240925
2022-12-31 15:29: Train Epoch 3: 163/634 Loss: 0.239869
2022-12-31 15:29: Train Epoch 3: 167/634 Loss: 0.238788
2022-12-31 15:30: Train Epoch 3: 171/634 Loss: 0.190711
2022-12-31 15:31: Train Epoch 3: 175/634 Loss: 0.211235
2022-12-31 15:32: Train Epoch 3: 179/634 Loss: 0.221531
2022-12-31 15:32: Train Epoch 3: 183/634 Loss: 0.243463
2022-12-31 15:33: Train Epoch 3: 187/634 Loss: 0.159224
2022-12-31 15:34: Train Epoch 3: 191/634 Loss: 0.278784
2022-12-31 15:34: Train Epoch 3: 195/634 Loss: 0.218872
2022-12-31 15:35: Train Epoch 3: 199/634 Loss: 0.257551
2022-12-31 15:36: Train Epoch 3: 203/634 Loss: 0.174410
2022-12-31 15:36: Train Epoch 3: 207/634 Loss: 0.182941
2022-12-31 15:37: Train Epoch 3: 211/634 Loss: 0.185886
2022-12-31 15:38: Train Epoch 3: 215/634 Loss: 0.212828
2022-12-31 15:38: Train Epoch 3: 219/634 Loss: 0.192168
2022-12-31 15:39: Train Epoch 3: 223/634 Loss: 0.190103
2022-12-31 15:40: Train Epoch 3: 227/634 Loss: 0.170140
2022-12-31 15:41: Train Epoch 3: 231/634 Loss: 0.165174
2022-12-31 15:41: Train Epoch 3: 235/634 Loss: 0.175539
2022-12-31 15:42: Train Epoch 3: 239/634 Loss: 0.189713
2022-12-31 15:43: Train Epoch 3: 243/634 Loss: 0.212314
2022-12-31 15:43: Train Epoch 3: 247/634 Loss: 0.199880
2022-12-31 15:44: Train Epoch 3: 251/634 Loss: 0.187541
2022-12-31 15:45: Train Epoch 3: 255/634 Loss: 0.167125
2022-12-31 15:45: Train Epoch 3: 259/634 Loss: 0.208047
2022-12-31 15:46: Train Epoch 3: 263/634 Loss: 0.227031
2022-12-31 15:47: Train Epoch 3: 267/634 Loss: 0.221979
2022-12-31 15:47: Train Epoch 3: 271/634 Loss: 0.208613
2022-12-31 15:48: Train Epoch 3: 275/634 Loss: 0.227381
2022-12-31 15:49: Train Epoch 3: 279/634 Loss: 0.178401
2022-12-31 15:49: Train Epoch 3: 283/634 Loss: 0.215251
2022-12-31 15:50: Train Epoch 3: 287/634 Loss: 0.169340
2022-12-31 15:51: Train Epoch 3: 291/634 Loss: 0.212921
2022-12-31 15:52: Train Epoch 3: 295/634 Loss: 0.208079
2022-12-31 15:52: Train Epoch 3: 299/634 Loss: 0.209366
2022-12-31 15:53: Train Epoch 3: 303/634 Loss: 0.191301
2022-12-31 15:54: Train Epoch 3: 307/634 Loss: 0.209171
2022-12-31 15:54: Train Epoch 3: 311/634 Loss: 0.186757
2022-12-31 15:55: Train Epoch 3: 315/634 Loss: 0.190539
2022-12-31 15:56: Train Epoch 3: 319/634 Loss: 0.198575
2022-12-31 15:56: Train Epoch 3: 323/634 Loss: 0.180850
2022-12-31 15:57: Train Epoch 3: 327/634 Loss: 0.183952
2022-12-31 15:58: Train Epoch 3: 331/634 Loss: 0.207449
2022-12-31 15:58: Train Epoch 3: 335/634 Loss: 0.201242
2022-12-31 15:59: Train Epoch 3: 339/634 Loss: 0.234673
2022-12-31 16:00: Train Epoch 3: 343/634 Loss: 0.181800
2022-12-31 16:01: Train Epoch 3: 347/634 Loss: 0.179133
2022-12-31 16:01: Train Epoch 3: 351/634 Loss: 0.216907
2022-12-31 16:02: Train Epoch 3: 355/634 Loss: 0.180535
2022-12-31 16:03: Train Epoch 3: 359/634 Loss: 0.223401
2022-12-31 16:03: Train Epoch 3: 363/634 Loss: 0.224551
2022-12-31 16:04: Train Epoch 3: 367/634 Loss: 0.210546
2022-12-31 16:05: Train Epoch 3: 371/634 Loss: 0.214704
2022-12-31 16:05: Train Epoch 3: 375/634 Loss: 0.166390
2022-12-31 16:06: Train Epoch 3: 379/634 Loss: 0.175626
2022-12-31 16:07: Train Epoch 3: 383/634 Loss: 0.164240
2022-12-31 16:07: Train Epoch 3: 387/634 Loss: 0.196139
2022-12-31 16:08: Train Epoch 3: 391/634 Loss: 0.207153
2022-12-31 16:09: Train Epoch 3: 395/634 Loss: 0.191003
2022-12-31 16:09: Train Epoch 3: 399/634 Loss: 0.149436
2022-12-31 16:10: Train Epoch 3: 403/634 Loss: 0.185275
2022-12-31 16:11: Train Epoch 3: 407/634 Loss: 0.199062
2022-12-31 16:12: Train Epoch 3: 411/634 Loss: 0.179369
2022-12-31 16:12: Train Epoch 3: 415/634 Loss: 0.165444
2022-12-31 16:13: Train Epoch 3: 419/634 Loss: 0.193132
2022-12-31 16:14: Train Epoch 3: 423/634 Loss: 0.169781
2022-12-31 16:14: Train Epoch 3: 427/634 Loss: 0.207263
2022-12-31 16:15: Train Epoch 3: 431/634 Loss: 0.158039
2022-12-31 16:16: Train Epoch 3: 435/634 Loss: 0.171564
2022-12-31 16:16: Train Epoch 3: 439/634 Loss: 0.205781
2022-12-31 16:17: Train Epoch 3: 443/634 Loss: 0.191702
2022-12-31 16:18: Train Epoch 3: 447/634 Loss: 0.190606
2022-12-31 16:18: Train Epoch 3: 451/634 Loss: 0.157005
2022-12-31 16:19: Train Epoch 3: 455/634 Loss: 0.242093
2022-12-31 16:20: Train Epoch 3: 459/634 Loss: 0.194460
2022-12-31 16:20: Train Epoch 3: 463/634 Loss: 0.197951
2022-12-31 16:21: Train Epoch 3: 467/634 Loss: 0.178154
2022-12-31 16:22: Train Epoch 3: 471/634 Loss: 0.211245
2022-12-31 16:22: Train Epoch 3: 475/634 Loss: 0.205865
2022-12-31 16:23: Train Epoch 3: 479/634 Loss: 0.214883
2022-12-31 16:24: Train Epoch 3: 483/634 Loss: 0.209358
2022-12-31 16:25: Train Epoch 3: 487/634 Loss: 0.178086
2022-12-31 16:25: Train Epoch 3: 491/634 Loss: 0.175380
2022-12-31 16:26: Train Epoch 3: 495/634 Loss: 0.205541
2022-12-31 16:27: Train Epoch 3: 499/634 Loss: 0.180046
2022-12-31 16:27: Train Epoch 3: 503/634 Loss: 0.201436
2022-12-31 16:28: Train Epoch 3: 507/634 Loss: 0.190727
2022-12-31 16:29: Train Epoch 3: 511/634 Loss: 0.212224
2022-12-31 16:29: Train Epoch 3: 515/634 Loss: 0.160745
2022-12-31 16:30: Train Epoch 3: 519/634 Loss: 0.192033
2022-12-31 16:31: Train Epoch 3: 523/634 Loss: 0.180128
2022-12-31 16:31: Train Epoch 3: 527/634 Loss: 0.212689
2022-12-31 16:32: Train Epoch 3: 531/634 Loss: 0.204412
2022-12-31 16:33: Train Epoch 3: 535/634 Loss: 0.203900
2022-12-31 16:33: Train Epoch 3: 539/634 Loss: 0.206579
2022-12-31 16:34: Train Epoch 3: 543/634 Loss: 0.173217
2022-12-31 16:35: Train Epoch 3: 547/634 Loss: 0.199512
2022-12-31 16:36: Train Epoch 3: 551/634 Loss: 0.221650
2022-12-31 16:36: Train Epoch 3: 555/634 Loss: 0.213083
2022-12-31 16:37: Train Epoch 3: 559/634 Loss: 0.178415
2022-12-31 16:38: Train Epoch 3: 563/634 Loss: 0.204931
2022-12-31 16:38: Train Epoch 3: 567/634 Loss: 0.195231
2022-12-31 16:39: Train Epoch 3: 571/634 Loss: 0.179332
2022-12-31 16:40: Train Epoch 3: 575/634 Loss: 0.190150
2022-12-31 16:40: Train Epoch 3: 579/634 Loss: 0.190328
2022-12-31 16:41: Train Epoch 3: 583/634 Loss: 0.181557
2022-12-31 16:42: Train Epoch 3: 587/634 Loss: 0.176906
2022-12-31 16:42: Train Epoch 3: 591/634 Loss: 0.195037
2022-12-31 16:43: Train Epoch 3: 595/634 Loss: 0.196872
2022-12-31 16:44: Train Epoch 3: 599/634 Loss: 0.192143
2022-12-31 16:44: Train Epoch 3: 603/634 Loss: 0.167136
2022-12-31 16:45: Train Epoch 3: 607/634 Loss: 0.214505
2022-12-31 16:46: Train Epoch 3: 611/634 Loss: 0.184613
2022-12-31 16:46: Train Epoch 3: 615/634 Loss: 0.188172
2022-12-31 16:47: Train Epoch 3: 619/634 Loss: 0.208227
2022-12-31 16:48: Train Epoch 3: 623/634 Loss: 0.204056
2022-12-31 16:48: Train Epoch 3: 627/634 Loss: 0.203557
2022-12-31 16:49: Train Epoch 3: 631/634 Loss: 0.167592
2022-12-31 16:49: Train Epoch 3: 633/634 Loss: 0.074187
2022-12-31 16:49: **********Train Epoch 3: averaged Loss: 0.195331 
2022-12-31 16:49: 
Epoch time elapsed: 6574.2766699790955

2022-12-31 16:51: 
 metrics validation: {'precision': 0.7631133671742809, 'recall': 0.6938461538461539, 'f1-score': 0.7268331990330379, 'support': 1300, 'AUC': 0.8721511834319526, 'AUCPR': 0.7960550496446157, 'TP': 902, 'FP': 280, 'TN': 2320, 'FN': 398} 

2022-12-31 16:51: **********Val Epoch 3: average Loss: 0.203672
2022-12-31 16:51: *********************************Current best model saved!
2022-12-31 16:53: 
 Testing metrics {'precision': 0.7991967871485943, 'recall': 0.6482084690553745, 'f1-score': 0.7158273381294964, 'support': 1228, 'AUC': 0.8826569114791669, 'AUCPR': 0.8174684539718938, 'TP': 796, 'FP': 200, 'TN': 2256, 'FN': 432} 

2022-12-31 17:01: 
 Testing metrics {'precision': 0.8949023130473838, 'recall': 0.9042432493759928, 'f1-score': 0.8995485327313769, 'support': 4407, 'AUC': 0.9747130895013085, 'AUCPR': 0.9520529393044626, 'TP': 3985, 'FP': 468, 'TN': 8346, 'FN': 422} 

2022-12-31 17:02: Train Epoch 4: 3/634 Loss: 0.195662
2022-12-31 17:02: Train Epoch 4: 7/634 Loss: 0.182720
2022-12-31 17:03: Train Epoch 4: 11/634 Loss: 0.203621
2022-12-31 17:04: Train Epoch 4: 15/634 Loss: 0.186370
2022-12-31 17:04: Train Epoch 4: 19/634 Loss: 0.229375
