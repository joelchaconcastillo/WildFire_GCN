# @package _global_

# to execute this experiment run:
# python run.py experiment=gcn_spatiotemporal_cls

defaults:
  - override /trainer: default.yaml # choose trainer from 'configs/trainer/'
  - override /model: greecefire_GCN_model.yaml
  - override /datamodule: fireds_spatiotemporal_gcn_datamodule.yaml
  - override /callbacks: default.yaml
  - override /logger: wandb.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

model:
   _target_: wildfire_forecasting.models.greece_fire_models_gcn.GCN_fire_model
   dynamic_features: ${sel_dynamic_features}
   static_features: ${sel_static_features}
   positive_weight: 0.5
   lr: 0.0001
   lr_scheduler_step: 15
   lr_scheduler_gamma: 0.1
   weight_decay: 0.01
   input_dim: 25
   output_dim: 2 #binary classification..
   embed_dim: 50
   rnn_units: 16 #32 #64
   num_layers: 1
   link_len: 2
   window_len: 10
   patch_width: 25
   patch_height: 25
   horizon: 1
   clc: ${clc}

datamodule:
   _target_: wildfire_forecasting.datamodules.greecefire_gcn_datamodule.FireDSDataModuleGCN
   batch_size: 2 #256
   num_workers: 4 #12
   pin_memory: False
   nan_fill: -1.0
   clc: ${clc}
   dataset_root: /home/jcc/wildfire_forecastingGNN/data/datasets_grl/
   access_mode: 'spatiotemporal'
   problem_class: 'classification'
   sel_dynamic_features: ${sel_dynamic_features}
   sel_static_features: ${sel_static_features}

trainer:
   min_epochs: 1
   max_epochs: 10
   _target_: pytorch_lightning.Trainer
   gpus: 0
   check_val_every_n_epoch: 1
   weights_summary: full
   progress_bar_refresh_rate: 5
   resume_from_checkpoint: null
   #   gradient_clip_val: 0.5

callbacks:
   model_checkpoint:
      _target_: pytorch_lightning.callbacks.ModelCheckpoint
      monitor: "val/loss" # name of the logged metric which determines when model is improving
      mode: "min" # can be "max" or "min"
      save_top_k: 1 # save k best models (determined by above metric)
      save_last: True # additionaly always save model from last epoch
      verbose: True
      dirpath: "checkpoints/"
      filename: "epoch_{epoch:03d}"
      auto_insert_metric_name: False
   log_f1_precision_recall_heatmap:
     _target_: wildfire_forecasting.callbacks.wandb_callbacks.LogF1PrecRecHeatmap
   log_confusion_matrix:
     _target_: wildfire_forecasting.callbacks.wandb_callbacks.LogConfusionMatrix

logger:
   wandb:
      _target_: pytorch_lightning.loggers.wandb.WandbLogger
      project: ${oc.env:WANDB_PROJECT}
      name: ${oc.env:WANDB_NAME_PREFIX}_GCN_opt_${now:%Y%m%d_%H%M}
      save_dir: "."
      offline: True # set True to store all logs only locally
      id: null # pass correct id to resume experiment!
      entity: ${oc.env:WANDB_ENTITY}  # set to name of your wandb team or just remove it
      log_model: False
      prefix: ""
      job_type: "train"
      group: ""
      tags: []
