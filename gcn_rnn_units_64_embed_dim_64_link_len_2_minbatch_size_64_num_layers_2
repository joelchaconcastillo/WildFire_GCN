2023-01-01 05:33: log dir: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2023010105332965474854013
2023-01-01 05:33: Experiment log path in: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2023010105332965474854013
2023-01-01 05:33: Argument: Namespace(batch_size=256, clc='vec', cuda=True, dataset='2020', dataset_root='/home/joel.chacon/tmp/datasets_grl/', debug=False, default_graph=True, device='cpu', dynamic_features=['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh'], early_stop=True, early_stop_patience=8, embed_dim=64, epochs=30, grad_norm=False, horizon=1, input_dim=25, lag=10, link_len=2, log_dir='/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2023010105332965474854013', log_step=1, loss_func='nllloss', lr_decay=True, lr_decay_rate=0.1, lr_decay_step='15, 20', lr_init=0.0001, max_grad_norm=5, minbatch_size=64, mode='train', model='fire_GCN', nan_fill=-1.0, num_layers=1, num_nodes=625, num_workers=12, output_dim=2, patch_height=25, patch_width=25, persistent_workers=True, pin_memory=True, plot=False, positive_weight=0.5, prefetch_factor=2, real_value=True, rnn_units=64, seed=10000, static_features=['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density'], teacher_forcing=False, weight_decay=0.0, window_len=10)
2023-01-01 05:33: Argument batch_size: 256
2023-01-01 05:33: Argument clc: 'vec'
2023-01-01 05:33: Argument cuda: True
2023-01-01 05:33: Argument dataset: '2020'
2023-01-01 05:33: Argument dataset_root: '/home/joel.chacon/tmp/datasets_grl/'
2023-01-01 05:33: Argument debug: False
2023-01-01 05:33: Argument default_graph: True
2023-01-01 05:33: Argument device: 'cpu'
2023-01-01 05:33: Argument dynamic_features: ['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh']
2023-01-01 05:33: Argument early_stop: True
2023-01-01 05:33: Argument early_stop_patience: 8
2023-01-01 05:33: Argument embed_dim: 64
2023-01-01 05:33: Argument epochs: 30
2023-01-01 05:33: Argument grad_norm: False
2023-01-01 05:33: Argument horizon: 1
2023-01-01 05:33: Argument input_dim: 25
2023-01-01 05:33: Argument lag: 10
2023-01-01 05:33: Argument link_len: 2
2023-01-01 05:33: Argument log_dir: '/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2023010105332965474854013'
2023-01-01 05:33: Argument log_step: 1
2023-01-01 05:33: Argument loss_func: 'nllloss'
2023-01-01 05:33: Argument lr_decay: True
2023-01-01 05:33: Argument lr_decay_rate: 0.1
2023-01-01 05:33: Argument lr_decay_step: '15, 20'
2023-01-01 05:33: Argument lr_init: 0.0001
2023-01-01 05:33: Argument max_grad_norm: 5
2023-01-01 05:33: Argument minbatch_size: 64
2023-01-01 05:33: Argument mode: 'train'
2023-01-01 05:33: Argument model: 'fire_GCN'
2023-01-01 05:33: Argument nan_fill: -1.0
2023-01-01 05:33: Argument num_layers: 1
2023-01-01 05:33: Argument num_nodes: 625
2023-01-01 05:33: Argument num_workers: 12
2023-01-01 05:33: Argument output_dim: 2
2023-01-01 05:33: Argument patch_height: 25
2023-01-01 05:33: Argument patch_width: 25
2023-01-01 05:33: Argument persistent_workers: True
2023-01-01 05:33: Argument pin_memory: True
2023-01-01 05:33: Argument plot: False
2023-01-01 05:33: Argument positive_weight: 0.5
2023-01-01 05:33: Argument prefetch_factor: 2
2023-01-01 05:33: Argument real_value: True
2023-01-01 05:33: Argument rnn_units: 64
2023-01-01 05:33: Argument seed: 10000
2023-01-01 05:33: Argument static_features: ['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density']
2023-01-01 05:33: Argument teacher_forcing: False
2023-01-01 05:33: Argument weight_decay: 0.0
2023-01-01 05:33: Argument window_len: 10
++++++++++++++
2020_fire_GCN.conf
++++++++++++++
*****************Model Parameter*****************
node_embeddings torch.Size([625, 64]) True
ln1.weight torch.Size([25]) True
ln1.bias torch.Size([25]) True
encoder.cell_list.0.gate.weights_pool torch.Size([64, 2, 89, 64]) True
encoder.cell_list.0.gate.weights_window torch.Size([64, 1, 64]) True
encoder.cell_list.0.gate.bias_pool torch.Size([64, 128]) True
encoder.cell_list.0.gate.T torch.Size([10]) True
encoder.cell_list.0.update.weights_pool torch.Size([64, 2, 89, 32]) True
encoder.cell_list.0.update.weights_window torch.Size([64, 1, 32]) True
encoder.cell_list.0.update.bias_pool torch.Size([64, 64]) True
encoder.cell_list.0.update.T torch.Size([10]) True
fc1.weight torch.Size([2, 40000]) True
fc1.bias torch.Size([2]) True
Total params num: 1232136
*****************Finish Parameter****************
Positives: 13518 / Negatives: 27036
Dataset length 40554
Positives: 1300 / Negatives: 2600
Dataset length 3900
Positives: 1228 / Negatives: 2456
Dataset length 3684
Positives: 4407 / Negatives: 8814
Dataset length 13221
Applying learning rate decay.
Creat Log File in:  /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2023010105332965474854013/run.log
2023-01-01 05:33: Train Epoch 1: 3/634 Loss: 0.600289
2023-01-01 05:34: Train Epoch 1: 7/634 Loss: 0.989079
2023-01-01 05:34: Train Epoch 1: 11/634 Loss: 1.179627
2023-01-01 05:34: Train Epoch 1: 15/634 Loss: 0.704595
2023-01-01 05:35: Train Epoch 1: 19/634 Loss: 0.610596
2023-01-01 05:35: Train Epoch 1: 23/634 Loss: 0.548251
2023-01-01 05:35: Train Epoch 1: 27/634 Loss: 0.657401
2023-01-01 05:36: Train Epoch 1: 31/634 Loss: 0.337064
2023-01-01 05:36: Train Epoch 1: 35/634 Loss: 0.290150
2023-01-01 05:36: Train Epoch 1: 39/634 Loss: 0.368151
2023-01-01 05:37: Train Epoch 1: 43/634 Loss: 0.477828
2023-01-01 05:37: Train Epoch 1: 47/634 Loss: 0.524736
2023-01-01 05:37: Train Epoch 1: 51/634 Loss: 0.381593
2023-01-01 05:38: Train Epoch 1: 55/634 Loss: 0.252428
2023-01-01 05:38: Train Epoch 1: 59/634 Loss: 0.256916
2023-01-01 05:39: Train Epoch 1: 63/634 Loss: 0.284704
2023-01-01 05:39: Train Epoch 1: 67/634 Loss: 0.341872
2023-01-01 05:39: Train Epoch 1: 71/634 Loss: 0.316396
2023-01-01 05:40: Train Epoch 1: 75/634 Loss: 0.267478
2023-01-01 05:40: Train Epoch 1: 79/634 Loss: 0.288134
2023-01-01 05:40: Train Epoch 1: 83/634 Loss: 0.342993
2023-01-01 05:41: Train Epoch 1: 87/634 Loss: 0.263124
2023-01-01 05:41: Train Epoch 1: 91/634 Loss: 0.212581
2023-01-01 05:41: Train Epoch 1: 95/634 Loss: 0.198300
2023-01-01 05:42: Train Epoch 1: 99/634 Loss: 0.316027
2023-01-01 05:42: Train Epoch 1: 103/634 Loss: 0.278485
2023-01-01 05:42: Train Epoch 1: 107/634 Loss: 0.250375
2023-01-01 05:43: Train Epoch 1: 111/634 Loss: 0.230970
2023-01-01 05:43: Train Epoch 1: 115/634 Loss: 0.260529
2023-01-01 05:43: Train Epoch 1: 119/634 Loss: 0.262594
2023-01-01 05:44: Train Epoch 1: 123/634 Loss: 0.240717
2023-01-01 05:44: Train Epoch 1: 127/634 Loss: 0.255279
2023-01-01 05:44: Train Epoch 1: 131/634 Loss: 0.209123
2023-01-01 05:45: Train Epoch 1: 135/634 Loss: 0.238648
2023-01-01 05:45: Train Epoch 1: 139/634 Loss: 0.225362
2023-01-01 05:45: Train Epoch 1: 143/634 Loss: 0.199499
2023-01-01 05:46: Train Epoch 1: 147/634 Loss: 0.184361
2023-01-01 05:46: Train Epoch 1: 151/634 Loss: 0.230011
2023-01-01 05:46: Train Epoch 1: 155/634 Loss: 0.241498
2023-01-01 05:47: Train Epoch 1: 159/634 Loss: 0.215988
2023-01-01 05:47: Train Epoch 1: 163/634 Loss: 0.227367
2023-01-01 05:47: Train Epoch 1: 167/634 Loss: 0.242747
2023-01-01 05:48: Train Epoch 1: 171/634 Loss: 0.208529
2023-01-01 05:48: Train Epoch 1: 175/634 Loss: 0.201226
2023-01-01 05:48: Train Epoch 1: 179/634 Loss: 0.247237
2023-01-01 05:49: Train Epoch 1: 183/634 Loss: 0.222911
2023-01-01 05:49: Train Epoch 1: 187/634 Loss: 0.201994
2023-01-01 05:50: Train Epoch 1: 191/634 Loss: 0.195167
2023-01-01 05:50: Train Epoch 1: 195/634 Loss: 0.210283
2023-01-01 05:50: Train Epoch 1: 199/634 Loss: 0.196565
2023-01-01 05:51: Train Epoch 1: 203/634 Loss: 0.210381
2023-01-01 05:51: Train Epoch 1: 207/634 Loss: 0.236809
2023-01-01 05:51: Train Epoch 1: 211/634 Loss: 0.229199
2023-01-01 05:52: Train Epoch 1: 215/634 Loss: 0.200933
2023-01-01 05:52: Train Epoch 1: 219/634 Loss: 0.210375
2023-01-01 05:52: Train Epoch 1: 223/634 Loss: 0.186524
2023-01-01 05:53: Train Epoch 1: 227/634 Loss: 0.207662
2023-01-01 05:53: Train Epoch 1: 231/634 Loss: 0.210490
2023-01-01 05:53: Train Epoch 1: 235/634 Loss: 0.190403
2023-01-01 05:54: Train Epoch 1: 239/634 Loss: 0.210886
2023-01-01 05:54: Train Epoch 1: 243/634 Loss: 0.189687
2023-01-01 05:54: Train Epoch 1: 247/634 Loss: 0.204267
2023-01-01 05:55: Train Epoch 1: 251/634 Loss: 0.202523
2023-01-01 05:55: Train Epoch 1: 255/634 Loss: 0.201351
2023-01-01 05:55: Train Epoch 1: 259/634 Loss: 0.218843
2023-01-01 05:56: Train Epoch 1: 263/634 Loss: 0.206447
2023-01-01 05:56: Train Epoch 1: 267/634 Loss: 0.229252
2023-01-01 05:56: Train Epoch 1: 271/634 Loss: 0.196555
2023-01-01 05:57: Train Epoch 1: 275/634 Loss: 0.184039
2023-01-01 05:57: Train Epoch 1: 279/634 Loss: 0.219498
2023-01-01 05:57: Train Epoch 1: 283/634 Loss: 0.209174
2023-01-01 05:58: Train Epoch 1: 287/634 Loss: 0.233357
2023-01-01 05:58: Train Epoch 1: 291/634 Loss: 0.235506
2023-01-01 05:58: Train Epoch 1: 295/634 Loss: 0.211136
2023-01-01 05:59: Train Epoch 1: 299/634 Loss: 0.198034
2023-01-01 05:59: Train Epoch 1: 303/634 Loss: 0.197556
2023-01-01 05:59: Train Epoch 1: 307/634 Loss: 0.205222
2023-01-01 06:00: Train Epoch 1: 311/634 Loss: 0.220293
2023-01-01 06:00: Train Epoch 1: 315/634 Loss: 0.202476
2023-01-01 06:00: Train Epoch 1: 319/634 Loss: 0.193562
2023-01-01 06:01: Train Epoch 1: 323/634 Loss: 0.187235
2023-01-01 06:01: Train Epoch 1: 327/634 Loss: 0.192294
2023-01-01 06:01: Train Epoch 1: 331/634 Loss: 0.198438
2023-01-01 06:02: Train Epoch 1: 335/634 Loss: 0.185172
2023-01-01 06:02: Train Epoch 1: 339/634 Loss: 0.215798
2023-01-01 06:03: Train Epoch 1: 343/634 Loss: 0.215923
2023-01-01 06:03: Train Epoch 1: 347/634 Loss: 0.202172
2023-01-01 06:03: Train Epoch 1: 351/634 Loss: 0.226333
2023-01-01 06:04: Train Epoch 1: 355/634 Loss: 0.214256
2023-01-01 06:04: Train Epoch 1: 359/634 Loss: 0.193822
2023-01-01 06:04: Train Epoch 1: 363/634 Loss: 0.161910
2023-01-01 06:05: Train Epoch 1: 367/634 Loss: 0.205118
2023-01-01 06:05: Train Epoch 1: 371/634 Loss: 0.198186
2023-01-01 06:05: Train Epoch 1: 375/634 Loss: 0.192465
2023-01-01 06:05: Train Epoch 1: 379/634 Loss: 0.232315
2023-01-01 06:06: Train Epoch 1: 383/634 Loss: 0.208579
2023-01-01 06:06: Train Epoch 1: 387/634 Loss: 0.175309
2023-01-01 06:06: Train Epoch 1: 391/634 Loss: 0.190185
2023-01-01 06:07: Train Epoch 1: 395/634 Loss: 0.200592
2023-01-01 06:07: Train Epoch 1: 399/634 Loss: 0.220988
2023-01-01 06:07: Train Epoch 1: 403/634 Loss: 0.170565
2023-01-01 06:08: Train Epoch 1: 407/634 Loss: 0.183164
2023-01-01 06:08: Train Epoch 1: 411/634 Loss: 0.213243
2023-01-01 06:08: Train Epoch 1: 415/634 Loss: 0.194359
2023-01-01 06:09: Train Epoch 1: 419/634 Loss: 0.185477
2023-01-01 06:09: Train Epoch 1: 423/634 Loss: 0.178533
2023-01-01 06:10: Train Epoch 1: 427/634 Loss: 0.215913
2023-01-01 06:10: Train Epoch 1: 431/634 Loss: 0.194266
2023-01-01 06:10: Train Epoch 1: 435/634 Loss: 0.203269
2023-01-01 06:11: Train Epoch 1: 439/634 Loss: 0.242957
2023-01-01 06:11: Train Epoch 1: 443/634 Loss: 0.212517
2023-01-01 06:11: Train Epoch 1: 447/634 Loss: 0.206743
2023-01-01 06:12: Train Epoch 1: 451/634 Loss: 0.186410
2023-01-01 06:12: Train Epoch 1: 455/634 Loss: 0.218574
2023-01-01 06:12: Train Epoch 1: 459/634 Loss: 0.208226
2023-01-01 06:13: Train Epoch 1: 463/634 Loss: 0.208901
2023-01-01 06:13: Train Epoch 1: 467/634 Loss: 0.233824
2023-01-01 06:13: Train Epoch 1: 471/634 Loss: 0.191567
2023-01-01 06:14: Train Epoch 1: 475/634 Loss: 0.190553
2023-01-01 06:14: Train Epoch 1: 479/634 Loss: 0.179802
2023-01-01 06:14: Train Epoch 1: 483/634 Loss: 0.188714
2023-01-01 06:15: Train Epoch 1: 487/634 Loss: 0.185813
2023-01-01 06:15: Train Epoch 1: 491/634 Loss: 0.222023
2023-01-01 06:15: Train Epoch 1: 495/634 Loss: 0.232868
2023-01-01 06:16: Train Epoch 1: 499/634 Loss: 0.199763
2023-01-01 06:16: Train Epoch 1: 503/634 Loss: 0.215837
2023-01-01 06:16: Train Epoch 1: 507/634 Loss: 0.203599
2023-01-01 06:17: Train Epoch 1: 511/634 Loss: 0.185538
2023-01-01 06:17: Train Epoch 1: 515/634 Loss: 0.196229
2023-01-01 06:18: Train Epoch 1: 519/634 Loss: 0.197554
2023-01-01 06:18: Train Epoch 1: 523/634 Loss: 0.202429
2023-01-01 06:18: Train Epoch 1: 527/634 Loss: 0.211442
2023-01-01 06:19: Train Epoch 1: 531/634 Loss: 0.226516
2023-01-01 06:19: Train Epoch 1: 535/634 Loss: 0.207190
2023-01-01 06:19: Train Epoch 1: 539/634 Loss: 0.200950
2023-01-01 06:20: Train Epoch 1: 543/634 Loss: 0.225162
2023-01-01 06:20: Train Epoch 1: 547/634 Loss: 0.218607
2023-01-01 06:20: Train Epoch 1: 551/634 Loss: 0.203170
2023-01-01 06:21: Train Epoch 1: 555/634 Loss: 0.211551
2023-01-01 06:21: Train Epoch 1: 559/634 Loss: 0.190460
2023-01-01 06:21: Train Epoch 1: 563/634 Loss: 0.225828
2023-01-01 06:22: Train Epoch 1: 567/634 Loss: 0.216320
2023-01-01 06:22: Train Epoch 1: 571/634 Loss: 0.201656
2023-01-01 06:22: Train Epoch 1: 575/634 Loss: 0.204129
2023-01-01 06:23: Train Epoch 1: 579/634 Loss: 0.192564
2023-01-01 06:23: Train Epoch 1: 583/634 Loss: 0.179454
2023-01-01 06:23: Train Epoch 1: 587/634 Loss: 0.211335
2023-01-01 06:24: Train Epoch 1: 591/634 Loss: 0.226213
2023-01-01 06:24: Train Epoch 1: 595/634 Loss: 0.204017
2023-01-01 06:24: Train Epoch 1: 599/634 Loss: 0.197291
2023-01-01 06:25: Train Epoch 1: 603/634 Loss: 0.176508
2023-01-01 06:25: Train Epoch 1: 607/634 Loss: 0.196977
2023-01-01 06:25: Train Epoch 1: 611/634 Loss: 0.214219
2023-01-01 06:26: Train Epoch 1: 615/634 Loss: 0.195686
2023-01-01 06:26: Train Epoch 1: 619/634 Loss: 0.204529
2023-01-01 06:27: Train Epoch 1: 623/634 Loss: 0.212652
2023-01-01 06:27: Train Epoch 1: 627/634 Loss: 0.205114
2023-01-01 06:27: Train Epoch 1: 631/634 Loss: 0.196921
2023-01-01 06:27: Train Epoch 1: 633/634 Loss: 0.088858
2023-01-01 06:27: **********Train Epoch 1: averaged Loss: 0.244351 
2023-01-01 06:27: 
Epoch time elapsed: 3260.8232831954956

2023-01-01 06:29: 
 metrics validation: {'precision': 0.7604961832061069, 'recall': 0.6130769230769231, 'f1-score': 0.6788756388415674, 'support': 1300, 'AUC': 0.8555804733727811, 'AUCPR': 0.7435841414728025, 'TP': 797, 'FP': 251, 'TN': 2349, 'FN': 503} 

2023-01-01 06:29: **********Val Epoch 1: average Loss: 0.219612
2023-01-01 06:29: *********************************Current best model saved!
2023-01-01 06:30: 
 Testing metrics {'precision': 0.827016520894072, 'recall': 0.6929967426710097, 'f1-score': 0.7540983606557377, 'support': 1228, 'AUC': 0.8834155402179333, 'AUCPR': 0.8103653452820181, 'TP': 851, 'FP': 178, 'TN': 2278, 'FN': 377} 

2023-01-01 06:35: 
 Testing metrics {'precision': 0.8976150325222838, 'recall': 0.8454731109598366, 'f1-score': 0.8707641972423463, 'support': 4407, 'AUC': 0.9679237329435826, 'AUCPR': 0.9427097590846599, 'TP': 3726, 'FP': 425, 'TN': 8389, 'FN': 681} 

2023-01-01 06:36: Train Epoch 2: 3/634 Loss: 0.189428
2023-01-01 06:36: Train Epoch 2: 7/634 Loss: 0.194868
2023-01-01 06:37: Train Epoch 2: 11/634 Loss: 0.200069
2023-01-01 06:37: Train Epoch 2: 15/634 Loss: 0.196272
2023-01-01 06:37: Train Epoch 2: 19/634 Loss: 0.194856
2023-01-01 06:38: Train Epoch 2: 23/634 Loss: 0.203124
2023-01-01 06:38: Train Epoch 2: 27/634 Loss: 0.186547
2023-01-01 06:38: Train Epoch 2: 31/634 Loss: 0.212506
2023-01-01 06:39: Train Epoch 2: 35/634 Loss: 0.170076
2023-01-01 06:39: Train Epoch 2: 39/634 Loss: 0.162312
2023-01-01 06:39: Train Epoch 2: 43/634 Loss: 0.210391
2023-01-01 06:40: Train Epoch 2: 47/634 Loss: 0.188554
2023-01-01 06:40: Train Epoch 2: 51/634 Loss: 0.179363
2023-01-01 06:40: Train Epoch 2: 55/634 Loss: 0.176536
2023-01-01 06:41: Train Epoch 2: 59/634 Loss: 0.223507
2023-01-01 06:41: Train Epoch 2: 63/634 Loss: 0.217855
2023-01-01 06:42: Train Epoch 2: 67/634 Loss: 0.213258
2023-01-01 06:42: Train Epoch 2: 71/634 Loss: 0.178554
2023-01-01 06:42: Train Epoch 2: 75/634 Loss: 0.206526
2023-01-01 06:43: Train Epoch 2: 79/634 Loss: 0.198443
2023-01-01 06:43: Train Epoch 2: 83/634 Loss: 0.240846
2023-01-01 06:43: Train Epoch 2: 87/634 Loss: 0.185678
2023-01-01 06:44: Train Epoch 2: 91/634 Loss: 0.201569
2023-01-01 06:44: Train Epoch 2: 95/634 Loss: 0.222122
2023-01-01 06:44: Train Epoch 2: 99/634 Loss: 0.174298
2023-01-01 06:45: Train Epoch 2: 103/634 Loss: 0.194838
2023-01-01 06:45: Train Epoch 2: 107/634 Loss: 0.229152
2023-01-01 06:45: Train Epoch 2: 111/634 Loss: 0.209217
2023-01-01 06:46: Train Epoch 2: 115/634 Loss: 0.205635
2023-01-01 06:46: Train Epoch 2: 119/634 Loss: 0.228881
2023-01-01 06:46: Train Epoch 2: 123/634 Loss: 0.209556
2023-01-01 06:47: Train Epoch 2: 127/634 Loss: 0.184529
2023-01-01 06:47: Train Epoch 2: 131/634 Loss: 0.266016
2023-01-01 06:47: Train Epoch 2: 135/634 Loss: 0.179251
2023-01-01 06:48: Train Epoch 2: 139/634 Loss: 0.196098
2023-01-01 06:48: Train Epoch 2: 143/634 Loss: 0.215486
2023-01-01 06:49: Train Epoch 2: 147/634 Loss: 0.206358
2023-01-01 06:49: Train Epoch 2: 151/634 Loss: 0.189684
2023-01-01 06:49: Train Epoch 2: 155/634 Loss: 0.198299
2023-01-01 06:50: Train Epoch 2: 159/634 Loss: 0.165038
2023-01-01 06:50: Train Epoch 2: 163/634 Loss: 0.197541
2023-01-01 06:50: Train Epoch 2: 167/634 Loss: 0.170468
2023-01-01 06:51: Train Epoch 2: 171/634 Loss: 0.181998
2023-01-01 06:51: Train Epoch 2: 175/634 Loss: 0.205832
2023-01-01 06:51: Train Epoch 2: 179/634 Loss: 0.215635
2023-01-01 06:52: Train Epoch 2: 183/634 Loss: 0.188065
2023-01-01 06:52: Train Epoch 2: 187/634 Loss: 0.168810
2023-01-01 06:52: Train Epoch 2: 191/634 Loss: 0.189302
2023-01-01 06:53: Train Epoch 2: 195/634 Loss: 0.185936
2023-01-01 06:53: Train Epoch 2: 199/634 Loss: 0.185412
2023-01-01 06:54: Train Epoch 2: 203/634 Loss: 0.209562
2023-01-01 06:54: Train Epoch 2: 207/634 Loss: 0.201250
2023-01-01 06:54: Train Epoch 2: 211/634 Loss: 0.199869
2023-01-01 06:55: Train Epoch 2: 215/634 Loss: 0.185718
2023-01-01 06:55: Train Epoch 2: 219/634 Loss: 0.203720
2023-01-01 06:55: Train Epoch 2: 223/634 Loss: 0.160854
2023-01-01 06:56: Train Epoch 2: 227/634 Loss: 0.214311
2023-01-01 06:56: Train Epoch 2: 231/634 Loss: 0.173804
2023-01-01 06:56: Train Epoch 2: 235/634 Loss: 0.195985
2023-01-01 06:57: Train Epoch 2: 239/634 Loss: 0.183018
2023-01-01 06:57: Train Epoch 2: 243/634 Loss: 0.180164
2023-01-01 06:58: Train Epoch 2: 247/634 Loss: 0.207989
2023-01-01 06:58: Train Epoch 2: 251/634 Loss: 0.201188
2023-01-01 06:58: Train Epoch 2: 255/634 Loss: 0.179451
2023-01-01 06:59: Train Epoch 2: 259/634 Loss: 0.181132
2023-01-01 06:59: Train Epoch 2: 263/634 Loss: 0.189991
2023-01-01 06:59: Train Epoch 2: 267/634 Loss: 0.165534
2023-01-01 07:00: Train Epoch 2: 271/634 Loss: 0.203299
2023-01-01 07:00: Train Epoch 2: 275/634 Loss: 0.184205
2023-01-01 07:00: Train Epoch 2: 279/634 Loss: 0.191554
2023-01-01 07:01: Train Epoch 2: 283/634 Loss: 0.161287
2023-01-01 07:01: Train Epoch 2: 287/634 Loss: 0.179527
2023-01-01 07:01: Train Epoch 2: 291/634 Loss: 0.208602
2023-01-01 07:02: Train Epoch 2: 295/634 Loss: 0.169296
2023-01-01 07:02: Train Epoch 2: 299/634 Loss: 0.191264
2023-01-01 07:03: Train Epoch 2: 303/634 Loss: 0.239962
2023-01-01 07:03: Train Epoch 2: 307/634 Loss: 0.177752
2023-01-01 07:03: Train Epoch 2: 311/634 Loss: 0.239141
2023-01-01 07:04: Train Epoch 2: 315/634 Loss: 0.243952
2023-01-01 07:04: Train Epoch 2: 319/634 Loss: 0.196220
2023-01-01 07:04: Train Epoch 2: 323/634 Loss: 0.269799
2023-01-01 07:05: Train Epoch 2: 327/634 Loss: 0.172430
2023-01-01 07:05: Train Epoch 2: 331/634 Loss: 0.190815
2023-01-01 07:05: Train Epoch 2: 335/634 Loss: 0.192708
2023-01-01 07:06: Train Epoch 2: 339/634 Loss: 0.185722
2023-01-01 07:06: Train Epoch 2: 343/634 Loss: 0.200703
2023-01-01 07:07: Train Epoch 2: 347/634 Loss: 0.211640
2023-01-01 07:07: Train Epoch 2: 351/634 Loss: 0.183323
2023-01-01 07:07: Train Epoch 2: 355/634 Loss: 0.178743
2023-01-01 07:08: Train Epoch 2: 359/634 Loss: 0.203999
2023-01-01 07:08: Train Epoch 2: 363/634 Loss: 0.187163
2023-01-01 07:08: Train Epoch 2: 367/634 Loss: 0.215416
2023-01-01 07:09: Train Epoch 2: 371/634 Loss: 0.184476
2023-01-01 07:09: Train Epoch 2: 375/634 Loss: 0.187149
2023-01-01 07:09: Train Epoch 2: 379/634 Loss: 0.154745
2023-01-01 07:10: Train Epoch 2: 383/634 Loss: 0.200104
2023-01-01 07:10: Train Epoch 2: 387/634 Loss: 0.227748
2023-01-01 07:10: Train Epoch 2: 391/634 Loss: 0.229424
2023-01-01 07:11: Train Epoch 2: 395/634 Loss: 0.176036
2023-01-01 07:11: Train Epoch 2: 399/634 Loss: 0.185153
2023-01-01 07:11: Train Epoch 2: 403/634 Loss: 0.179333
2023-01-01 07:12: Train Epoch 2: 407/634 Loss: 0.220025
2023-01-01 07:12: Train Epoch 2: 411/634 Loss: 0.183231
2023-01-01 07:12: Train Epoch 2: 415/634 Loss: 0.201604
2023-01-01 07:13: Train Epoch 2: 419/634 Loss: 0.195461
2023-01-01 07:13: Train Epoch 2: 423/634 Loss: 0.183797
2023-01-01 07:13: Train Epoch 2: 427/634 Loss: 0.161654
2023-01-01 07:14: Train Epoch 2: 431/634 Loss: 0.196061
2023-01-01 07:14: Train Epoch 2: 435/634 Loss: 0.218133
2023-01-01 07:15: Train Epoch 2: 439/634 Loss: 0.186084
2023-01-01 07:15: Train Epoch 2: 443/634 Loss: 0.180090
2023-01-01 07:15: Train Epoch 2: 447/634 Loss: 0.210953
2023-01-01 07:16: Train Epoch 2: 451/634 Loss: 0.173444
2023-01-01 07:16: Train Epoch 2: 455/634 Loss: 0.190828
2023-01-01 07:16: Train Epoch 2: 459/634 Loss: 0.192242
2023-01-01 07:17: Train Epoch 2: 463/634 Loss: 0.183911
2023-01-01 07:17: Train Epoch 2: 467/634 Loss: 0.169626
2023-01-01 07:17: Train Epoch 2: 471/634 Loss: 0.181909
2023-01-01 07:18: Train Epoch 2: 475/634 Loss: 0.165913
2023-01-01 07:18: Train Epoch 2: 479/634 Loss: 0.188291
2023-01-01 07:19: Train Epoch 2: 483/634 Loss: 0.214919
2023-01-01 07:19: Train Epoch 2: 487/634 Loss: 0.203284
2023-01-01 07:19: Train Epoch 2: 491/634 Loss: 0.183795
2023-01-01 07:20: Train Epoch 2: 495/634 Loss: 0.181252
2023-01-01 07:20: Train Epoch 2: 499/634 Loss: 0.196367
2023-01-01 07:20: Train Epoch 2: 503/634 Loss: 0.190071
2023-01-01 07:21: Train Epoch 2: 507/634 Loss: 0.166974
2023-01-01 07:21: Train Epoch 2: 511/634 Loss: 0.210370
2023-01-01 07:22: Train Epoch 2: 515/634 Loss: 0.171768
2023-01-01 07:22: Train Epoch 2: 519/634 Loss: 0.182319
2023-01-01 07:22: Train Epoch 2: 523/634 Loss: 0.193230
2023-01-01 07:23: Train Epoch 2: 527/634 Loss: 0.179016
2023-01-01 07:23: Train Epoch 2: 531/634 Loss: 0.171685
2023-01-01 07:23: Train Epoch 2: 535/634 Loss: 0.195496
2023-01-01 07:24: Train Epoch 2: 539/634 Loss: 0.179389
2023-01-01 07:24: Train Epoch 2: 543/634 Loss: 0.214319
2023-01-01 07:24: Train Epoch 2: 547/634 Loss: 0.178887
2023-01-01 07:25: Train Epoch 2: 551/634 Loss: 0.174087
2023-01-01 07:25: Train Epoch 2: 555/634 Loss: 0.178710
2023-01-01 07:25: Train Epoch 2: 559/634 Loss: 0.193171
2023-01-01 07:26: Train Epoch 2: 563/634 Loss: 0.167903
2023-01-01 07:26: Train Epoch 2: 567/634 Loss: 0.197838
2023-01-01 07:26: Train Epoch 2: 571/634 Loss: 0.208281
2023-01-01 07:27: Train Epoch 2: 575/634 Loss: 0.184687
2023-01-01 07:27: Train Epoch 2: 579/634 Loss: 0.162265
2023-01-01 07:28: Train Epoch 2: 583/634 Loss: 0.218146
2023-01-01 07:28: Train Epoch 2: 587/634 Loss: 0.210588
2023-01-01 07:28: Train Epoch 2: 591/634 Loss: 0.191934
2023-01-01 07:29: Train Epoch 2: 595/634 Loss: 0.199411
2023-01-01 07:29: Train Epoch 2: 599/634 Loss: 0.206069
2023-01-01 07:29: Train Epoch 2: 603/634 Loss: 0.196565
2023-01-01 07:30: Train Epoch 2: 607/634 Loss: 0.209302
2023-01-01 07:30: Train Epoch 2: 611/634 Loss: 0.194631
2023-01-01 07:31: Train Epoch 2: 615/634 Loss: 0.211592
2023-01-01 07:31: Train Epoch 2: 619/634 Loss: 0.211767
2023-01-01 07:31: Train Epoch 2: 623/634 Loss: 0.189979
2023-01-01 07:32: Train Epoch 2: 627/634 Loss: 0.167517
2023-01-01 07:32: Train Epoch 2: 631/634 Loss: 0.187734
2023-01-01 07:32: Train Epoch 2: 633/634 Loss: 0.064663
2023-01-01 07:32: **********Train Epoch 2: averaged Loss: 0.193422 
2023-01-01 07:32: 
Epoch time elapsed: 3402.6146099567413

2023-01-01 07:34: 
 metrics validation: {'precision': 0.7846460618145563, 'recall': 0.6053846153846154, 'f1-score': 0.6834563612679114, 'support': 1300, 'AUC': 0.8912224852071007, 'AUCPR': 0.7885331477114983, 'TP': 787, 'FP': 216, 'TN': 2384, 'FN': 513} 

2023-01-01 07:34: **********Val Epoch 2: average Loss: 0.191189
2023-01-01 07:34: *********************************Current best model saved!
2023-01-01 07:35: 
 Testing metrics {'precision': 0.8458333333333333, 'recall': 0.6612377850162866, 'f1-score': 0.7422303473491773, 'support': 1228, 'AUC': 0.9019568510010715, 'AUCPR': 0.8379780616437654, 'TP': 812, 'FP': 148, 'TN': 2308, 'FN': 416} 

2023-01-01 07:40: 
 Testing metrics {'precision': 0.8982483256053581, 'recall': 0.7912412071704107, 'f1-score': 0.8413560139944505, 'support': 4407, 'AUC': 0.9667594651720871, 'AUCPR': 0.9364636515178052, 'TP': 3487, 'FP': 395, 'TN': 8419, 'FN': 920} 

2023-01-01 07:41: Train Epoch 3: 3/634 Loss: 0.175489
2023-01-01 07:41: Train Epoch 3: 7/634 Loss: 0.160787
2023-01-01 07:41: Train Epoch 3: 11/634 Loss: 0.192939
2023-01-01 07:42: Train Epoch 3: 15/634 Loss: 0.191434
2023-01-01 07:42: Train Epoch 3: 19/634 Loss: 0.192749
2023-01-01 07:42: Train Epoch 3: 23/634 Loss: 0.180322
2023-01-01 07:43: Train Epoch 3: 27/634 Loss: 0.170283
2023-01-01 07:43: Train Epoch 3: 31/634 Loss: 0.189225
2023-01-01 07:43: Train Epoch 3: 35/634 Loss: 0.231116
2023-01-01 07:44: Train Epoch 3: 39/634 Loss: 0.188238
2023-01-01 07:44: Train Epoch 3: 43/634 Loss: 0.192108
2023-01-01 07:45: Train Epoch 3: 47/634 Loss: 0.201619
2023-01-01 07:45: Train Epoch 3: 51/634 Loss: 0.197994
2023-01-01 07:45: Train Epoch 3: 55/634 Loss: 0.177777
2023-01-01 07:46: Train Epoch 3: 59/634 Loss: 0.185799
2023-01-01 07:46: Train Epoch 3: 63/634 Loss: 0.221658
2023-01-01 07:46: Train Epoch 3: 67/634 Loss: 0.185555
2023-01-01 07:47: Train Epoch 3: 71/634 Loss: 0.203757
2023-01-01 07:47: Train Epoch 3: 75/634 Loss: 0.177395
2023-01-01 07:47: Train Epoch 3: 79/634 Loss: 0.209886
2023-01-01 07:48: Train Epoch 3: 83/634 Loss: 0.187257
2023-01-01 07:48: Train Epoch 3: 87/634 Loss: 0.182305
2023-01-01 07:49: Train Epoch 3: 91/634 Loss: 0.189285
2023-01-01 07:49: Train Epoch 3: 95/634 Loss: 0.168993
2023-01-01 07:49: Train Epoch 3: 99/634 Loss: 0.173734
2023-01-01 07:50: Train Epoch 3: 103/634 Loss: 0.185171
2023-01-01 07:50: Train Epoch 3: 107/634 Loss: 0.189217
2023-01-01 07:50: Train Epoch 3: 111/634 Loss: 0.187501
2023-01-01 07:51: Train Epoch 3: 115/634 Loss: 0.198308
2023-01-01 07:51: Train Epoch 3: 119/634 Loss: 0.191135
2023-01-01 07:52: Train Epoch 3: 123/634 Loss: 0.201869
2023-01-01 07:52: Train Epoch 3: 127/634 Loss: 0.180360
2023-01-01 07:52: Train Epoch 3: 131/634 Loss: 0.149136
2023-01-01 07:53: Train Epoch 3: 135/634 Loss: 0.223154
2023-01-01 07:53: Train Epoch 3: 139/634 Loss: 0.243037
2023-01-01 07:53: Train Epoch 3: 143/634 Loss: 0.182465
2023-01-01 07:54: Train Epoch 3: 147/634 Loss: 0.170976
2023-01-01 07:54: Train Epoch 3: 151/634 Loss: 0.213887
2023-01-01 07:54: Train Epoch 3: 155/634 Loss: 0.159197
2023-01-01 07:55: Train Epoch 3: 159/634 Loss: 0.177285
2023-01-01 07:55: Train Epoch 3: 163/634 Loss: 0.190832
2023-01-01 07:56: Train Epoch 3: 167/634 Loss: 0.206736
2023-01-01 07:56: Train Epoch 3: 171/634 Loss: 0.188106
2023-01-01 07:56: Train Epoch 3: 175/634 Loss: 0.202438
2023-01-01 07:57: Train Epoch 3: 179/634 Loss: 0.179299
2023-01-01 07:57: Train Epoch 3: 183/634 Loss: 0.195680
2023-01-01 07:57: Train Epoch 3: 187/634 Loss: 0.207168
2023-01-01 07:58: Train Epoch 3: 191/634 Loss: 0.218008
2023-01-01 07:58: Train Epoch 3: 195/634 Loss: 0.165493
2023-01-01 07:58: Train Epoch 3: 199/634 Loss: 0.199307
2023-01-01 07:59: Train Epoch 3: 203/634 Loss: 0.150272
2023-01-01 07:59: Train Epoch 3: 207/634 Loss: 0.221538
2023-01-01 07:59: Train Epoch 3: 211/634 Loss: 0.187982
2023-01-01 08:00: Train Epoch 3: 215/634 Loss: 0.210442
2023-01-01 08:00: Train Epoch 3: 219/634 Loss: 0.189705
2023-01-01 08:01: Train Epoch 3: 223/634 Loss: 0.162412
2023-01-01 08:01: Train Epoch 3: 227/634 Loss: 0.179301
2023-01-01 08:01: Train Epoch 3: 231/634 Loss: 0.207044
2023-01-01 08:02: Train Epoch 3: 235/634 Loss: 0.188460
2023-01-01 08:02: Train Epoch 3: 239/634 Loss: 0.184632
2023-01-01 08:02: Train Epoch 3: 243/634 Loss: 0.201974
2023-01-01 08:03: Train Epoch 3: 247/634 Loss: 0.195089
2023-01-01 08:03: Train Epoch 3: 251/634 Loss: 0.207482
2023-01-01 08:03: Train Epoch 3: 255/634 Loss: 0.187570
2023-01-01 08:04: Train Epoch 3: 259/634 Loss: 0.183297
2023-01-01 08:04: Train Epoch 3: 263/634 Loss: 0.178486
2023-01-01 08:05: Train Epoch 3: 267/634 Loss: 0.197984
2023-01-01 08:05: Train Epoch 3: 271/634 Loss: 0.179155
2023-01-01 08:05: Train Epoch 3: 275/634 Loss: 0.170917
2023-01-01 08:06: Train Epoch 3: 279/634 Loss: 0.214414
2023-01-01 08:06: Train Epoch 3: 283/634 Loss: 0.155825
2023-01-01 08:06: Train Epoch 3: 287/634 Loss: 0.164978
2023-01-01 08:07: Train Epoch 3: 291/634 Loss: 0.159163
2023-01-01 08:07: Train Epoch 3: 295/634 Loss: 0.186199
2023-01-01 08:07: Train Epoch 3: 299/634 Loss: 0.174263
2023-01-01 08:08: Train Epoch 3: 303/634 Loss: 0.173744
2023-01-01 08:08: Train Epoch 3: 307/634 Loss: 0.199656
2023-01-01 08:09: Train Epoch 3: 311/634 Loss: 0.169965
2023-01-01 08:09: Train Epoch 3: 315/634 Loss: 0.174082
2023-01-01 08:09: Train Epoch 3: 319/634 Loss: 0.176773
2023-01-01 08:10: Train Epoch 3: 323/634 Loss: 0.179498
2023-01-01 08:10: Train Epoch 3: 327/634 Loss: 0.171966
2023-01-01 08:10: Train Epoch 3: 331/634 Loss: 0.194519
2023-01-01 08:11: Train Epoch 3: 335/634 Loss: 0.178247
2023-01-01 08:11: Train Epoch 3: 339/634 Loss: 0.181498
2023-01-01 08:11: Train Epoch 3: 343/634 Loss: 0.180651
2023-01-01 08:12: Train Epoch 3: 347/634 Loss: 0.153415
2023-01-01 08:12: Train Epoch 3: 351/634 Loss: 0.173713
2023-01-01 08:12: Train Epoch 3: 355/634 Loss: 0.177266
2023-01-01 08:13: Train Epoch 3: 359/634 Loss: 0.161417
2023-01-01 08:13: Train Epoch 3: 363/634 Loss: 0.177856
2023-01-01 08:14: Train Epoch 3: 367/634 Loss: 0.197583
2023-01-01 08:14: Train Epoch 3: 371/634 Loss: 0.230400
2023-01-01 08:14: Train Epoch 3: 375/634 Loss: 0.191161
2023-01-01 08:15: Train Epoch 3: 379/634 Loss: 0.179252
2023-01-01 08:15: Train Epoch 3: 383/634 Loss: 0.233622
2023-01-01 08:15: Train Epoch 3: 387/634 Loss: 0.153747
2023-01-01 08:16: Train Epoch 3: 391/634 Loss: 0.192431
2023-01-01 08:16: Train Epoch 3: 395/634 Loss: 0.199010
2023-01-01 08:16: Train Epoch 3: 399/634 Loss: 0.189304
2023-01-01 08:17: Train Epoch 3: 403/634 Loss: 0.173038
2023-01-01 08:17: Train Epoch 3: 407/634 Loss: 0.190967
2023-01-01 08:18: Train Epoch 3: 411/634 Loss: 0.179704
2023-01-01 08:18: Train Epoch 3: 415/634 Loss: 0.191341
2023-01-01 08:18: Train Epoch 3: 419/634 Loss: 0.196151
2023-01-01 08:19: Train Epoch 3: 423/634 Loss: 0.175388
2023-01-01 08:19: Train Epoch 3: 427/634 Loss: 0.190167
2023-01-01 08:19: Train Epoch 3: 431/634 Loss: 0.171500
2023-01-01 08:20: Train Epoch 3: 435/634 Loss: 0.193809
2023-01-01 08:20: Train Epoch 3: 439/634 Loss: 0.157524
2023-01-01 08:20: Train Epoch 3: 443/634 Loss: 0.171210
2023-01-01 08:21: Train Epoch 3: 447/634 Loss: 0.182250
2023-01-01 08:21: Train Epoch 3: 451/634 Loss: 0.182508
2023-01-01 08:21: Train Epoch 3: 455/634 Loss: 0.194168
2023-01-01 08:22: Train Epoch 3: 459/634 Loss: 0.178807
2023-01-01 08:22: Train Epoch 3: 463/634 Loss: 0.181139
2023-01-01 08:23: Train Epoch 3: 467/634 Loss: 0.171950
2023-01-01 08:23: Train Epoch 3: 471/634 Loss: 0.194402
2023-01-01 08:23: Train Epoch 3: 475/634 Loss: 0.142965
2023-01-01 08:24: Train Epoch 3: 479/634 Loss: 0.188602
2023-01-01 08:24: Train Epoch 3: 483/634 Loss: 0.179668
2023-01-01 08:24: Train Epoch 3: 487/634 Loss: 0.215843
2023-01-01 08:25: Train Epoch 3: 491/634 Loss: 0.175961
2023-01-01 08:25: Train Epoch 3: 495/634 Loss: 0.180198
2023-01-01 08:25: Train Epoch 3: 499/634 Loss: 0.193510
2023-01-01 08:26: Train Epoch 3: 503/634 Loss: 0.173298
2023-01-01 08:26: Train Epoch 3: 507/634 Loss: 0.184214
2023-01-01 08:27: Train Epoch 3: 511/634 Loss: 0.178826
2023-01-01 08:27: Train Epoch 3: 515/634 Loss: 0.166243
2023-01-01 08:27: Train Epoch 3: 519/634 Loss: 0.145596
2023-01-01 08:28: Train Epoch 3: 523/634 Loss: 0.177363
2023-01-01 08:28: Train Epoch 3: 527/634 Loss: 0.194804
2023-01-01 08:28: Train Epoch 3: 531/634 Loss: 0.181519
2023-01-01 08:29: Train Epoch 3: 535/634 Loss: 0.183476
2023-01-01 08:29: Train Epoch 3: 539/634 Loss: 0.200259
2023-01-01 08:29: Train Epoch 3: 543/634 Loss: 0.157824
2023-01-01 08:30: Train Epoch 3: 547/634 Loss: 0.177905
2023-01-01 08:30: Train Epoch 3: 551/634 Loss: 0.179154
2023-01-01 08:30: Train Epoch 3: 555/634 Loss: 0.177733
2023-01-01 08:31: Train Epoch 3: 559/634 Loss: 0.183606
2023-01-01 08:31: Train Epoch 3: 563/634 Loss: 0.169803
2023-01-01 08:31: Train Epoch 3: 567/634 Loss: 0.199779
2023-01-01 08:32: Train Epoch 3: 571/634 Loss: 0.175130
2023-01-01 08:32: Train Epoch 3: 575/634 Loss: 0.200368
2023-01-01 08:32: Train Epoch 3: 579/634 Loss: 0.180376
2023-01-01 08:33: Train Epoch 3: 583/634 Loss: 0.177579
2023-01-01 08:33: Train Epoch 3: 587/634 Loss: 0.193032
2023-01-01 08:34: Train Epoch 3: 591/634 Loss: 0.186103
2023-01-01 08:34: Train Epoch 3: 595/634 Loss: 0.213711
2023-01-01 08:34: Train Epoch 3: 599/634 Loss: 0.183510
2023-01-01 08:35: Train Epoch 3: 603/634 Loss: 0.184885
2023-01-01 08:35: Train Epoch 3: 607/634 Loss: 0.188239
2023-01-01 08:35: Train Epoch 3: 611/634 Loss: 0.159568
2023-01-01 08:36: Train Epoch 3: 615/634 Loss: 0.211313
2023-01-01 08:36: Train Epoch 3: 619/634 Loss: 0.191754
2023-01-01 08:36: Train Epoch 3: 623/634 Loss: 0.190675
2023-01-01 08:37: Train Epoch 3: 627/634 Loss: 0.162001
2023-01-01 08:37: Train Epoch 3: 631/634 Loss: 0.181452
2023-01-01 08:37: Train Epoch 3: 633/634 Loss: 0.099750
2023-01-01 08:37: **********Train Epoch 3: averaged Loss: 0.185072 
2023-01-01 08:37: 
Epoch time elapsed: 3413.3007242679596

2023-01-01 08:39: 
 metrics validation: {'precision': 0.7437956204379562, 'recall': 0.7838461538461539, 'f1-score': 0.7632958801498126, 'support': 1300, 'AUC': 0.9033751479289941, 'AUCPR': 0.8225724770768692, 'TP': 1019, 'FP': 351, 'TN': 2249, 'FN': 281} 

2023-01-01 08:39: **********Val Epoch 3: average Loss: 0.175832
2023-01-01 08:39: *********************************Current best model saved!
2023-01-01 08:40: 
 Testing metrics {'precision': 0.7885095753538718, 'recall': 0.7711726384364821, 'f1-score': 0.7797447509263071, 'support': 1228, 'AUC': 0.9120560960859001, 'AUCPR': 0.8595261692722443, 'TP': 947, 'FP': 254, 'TN': 2202, 'FN': 281} 

2023-01-01 08:45: 
 Testing metrics {'precision': 0.8628328008519702, 'recall': 0.9192194236442024, 'f1-score': 0.8901340364754999, 'support': 4407, 'AUC': 0.9742860145397539, 'AUCPR': 0.952571609380292, 'TP': 4051, 'FP': 644, 'TN': 8170, 'FN': 356} 

2023-01-01 08:46: Train Epoch 4: 3/634 Loss: 0.220212
2023-01-01 08:46: Train Epoch 4: 7/634 Loss: 0.160605
2023-01-01 08:46: Train Epoch 4: 11/634 Loss: 0.204185
2023-01-01 08:47: Train Epoch 4: 15/634 Loss: 0.173107
2023-01-01 08:47: Train Epoch 4: 19/634 Loss: 0.145517
2023-01-01 08:48: Train Epoch 4: 23/634 Loss: 0.172879
2023-01-01 08:48: Train Epoch 4: 27/634 Loss: 0.194731
2023-01-01 08:48: Train Epoch 4: 31/634 Loss: 0.187820
2023-01-01 08:49: Train Epoch 4: 35/634 Loss: 0.195868
2023-01-01 08:49: Train Epoch 4: 39/634 Loss: 0.178322
2023-01-01 08:49: Train Epoch 4: 43/634 Loss: 0.232404
2023-01-01 08:50: Train Epoch 4: 47/634 Loss: 0.169069
2023-01-01 08:50: Train Epoch 4: 51/634 Loss: 0.198034
2023-01-01 08:50: Train Epoch 4: 55/634 Loss: 0.165428
2023-01-01 08:51: Train Epoch 4: 59/634 Loss: 0.181915
2023-01-01 08:51: Train Epoch 4: 63/634 Loss: 0.169633
2023-01-01 08:51: Train Epoch 4: 67/634 Loss: 0.145164
2023-01-01 08:52: Train Epoch 4: 71/634 Loss: 0.155741
2023-01-01 08:52: Train Epoch 4: 75/634 Loss: 0.161561
2023-01-01 08:52: Train Epoch 4: 79/634 Loss: 0.182021
2023-01-01 08:53: Train Epoch 4: 83/634 Loss: 0.148776
2023-01-01 08:53: Train Epoch 4: 87/634 Loss: 0.155580
2023-01-01 08:53: Train Epoch 4: 91/634 Loss: 0.201086
2023-01-01 08:54: Train Epoch 4: 95/634 Loss: 0.209016
2023-01-01 08:54: Train Epoch 4: 99/634 Loss: 0.167122
2023-01-01 08:54: Train Epoch 4: 103/634 Loss: 0.167888
2023-01-01 08:55: Train Epoch 4: 107/634 Loss: 0.202507
2023-01-01 08:55: Train Epoch 4: 111/634 Loss: 0.201024
2023-01-01 08:55: Train Epoch 4: 115/634 Loss: 0.178901
2023-01-01 08:56: Train Epoch 4: 119/634 Loss: 0.155081
2023-01-01 08:56: Train Epoch 4: 123/634 Loss: 0.188391
2023-01-01 08:56: Train Epoch 4: 127/634 Loss: 0.177774
2023-01-01 08:57: Train Epoch 4: 131/634 Loss: 0.177407
2023-01-01 08:57: Train Epoch 4: 135/634 Loss: 0.161784
2023-01-01 08:57: Train Epoch 4: 139/634 Loss: 0.201470
2023-01-01 08:58: Train Epoch 4: 143/634 Loss: 0.178627
2023-01-01 08:58: Train Epoch 4: 147/634 Loss: 0.135565
2023-01-01 08:58: Train Epoch 4: 151/634 Loss: 0.177327
2023-01-01 08:59: Train Epoch 4: 155/634 Loss: 0.188486
2023-01-01 08:59: Train Epoch 4: 159/634 Loss: 0.152538
2023-01-01 08:59: Train Epoch 4: 163/634 Loss: 0.171404
2023-01-01 09:00: Train Epoch 4: 167/634 Loss: 0.172059
2023-01-01 09:00: Train Epoch 4: 171/634 Loss: 0.175625
2023-01-01 09:00: Train Epoch 4: 175/634 Loss: 0.176562
2023-01-01 09:01: Train Epoch 4: 179/634 Loss: 0.161664
2023-01-01 09:01: Train Epoch 4: 183/634 Loss: 0.176649
2023-01-01 09:02: Train Epoch 4: 187/634 Loss: 0.157280
2023-01-01 09:02: Train Epoch 4: 191/634 Loss: 0.159163
2023-01-01 09:02: Train Epoch 4: 195/634 Loss: 0.162149
2023-01-01 09:03: Train Epoch 4: 199/634 Loss: 0.171936
2023-01-01 09:03: Train Epoch 4: 203/634 Loss: 0.178360
2023-01-01 09:03: Train Epoch 4: 207/634 Loss: 0.166532
2023-01-01 09:04: Train Epoch 4: 211/634 Loss: 0.156035
2023-01-01 09:04: Train Epoch 4: 215/634 Loss: 0.161843
2023-01-01 09:04: Train Epoch 4: 219/634 Loss: 0.180950
2023-01-01 09:05: Train Epoch 4: 223/634 Loss: 0.195302
2023-01-01 09:05: Train Epoch 4: 227/634 Loss: 0.189452
2023-01-01 09:06: Train Epoch 4: 231/634 Loss: 0.202392
2023-01-01 09:06: Train Epoch 4: 235/634 Loss: 0.172771
2023-01-01 09:06: Train Epoch 4: 239/634 Loss: 0.160560
2023-01-01 09:07: Train Epoch 4: 243/634 Loss: 0.156320
2023-01-01 09:07: Train Epoch 4: 247/634 Loss: 0.175567
2023-01-01 09:07: Train Epoch 4: 251/634 Loss: 0.157575
2023-01-01 09:08: Train Epoch 4: 255/634 Loss: 0.193975
2023-01-01 09:08: Train Epoch 4: 259/634 Loss: 0.156175
2023-01-01 09:09: Train Epoch 4: 263/634 Loss: 0.173669
2023-01-01 09:09: Train Epoch 4: 267/634 Loss: 0.162634
2023-01-01 09:09: Train Epoch 4: 271/634 Loss: 0.152695
2023-01-01 09:10: Train Epoch 4: 275/634 Loss: 0.169436
2023-01-01 09:10: Train Epoch 4: 279/634 Loss: 0.161908
2023-01-01 09:10: Train Epoch 4: 283/634 Loss: 0.158930
2023-01-01 09:11: Train Epoch 4: 287/634 Loss: 0.178297
2023-01-01 09:11: Train Epoch 4: 291/634 Loss: 0.169231
2023-01-01 09:11: Train Epoch 4: 295/634 Loss: 0.197478
2023-01-01 09:12: Train Epoch 4: 299/634 Loss: 0.164096
2023-01-01 09:12: Train Epoch 4: 303/634 Loss: 0.176334
2023-01-01 09:13: Train Epoch 4: 307/634 Loss: 0.179495
2023-01-01 09:13: Train Epoch 4: 311/634 Loss: 0.165659
2023-01-01 09:13: Train Epoch 4: 315/634 Loss: 0.197069
2023-01-01 09:14: Train Epoch 4: 319/634 Loss: 0.171295
2023-01-01 09:14: Train Epoch 4: 323/634 Loss: 0.170631
2023-01-01 09:14: Train Epoch 4: 327/634 Loss: 0.164491
2023-01-01 09:15: Train Epoch 4: 331/634 Loss: 0.172707
2023-01-01 09:15: Train Epoch 4: 335/634 Loss: 0.168058
2023-01-01 09:15: Train Epoch 4: 339/634 Loss: 0.160837
2023-01-01 09:16: Train Epoch 4: 343/634 Loss: 0.214380
2023-01-01 09:16: Train Epoch 4: 347/634 Loss: 0.179570
2023-01-01 09:16: Train Epoch 4: 351/634 Loss: 0.156001
2023-01-01 09:17: Train Epoch 4: 355/634 Loss: 0.205789
2023-01-01 09:17: Train Epoch 4: 359/634 Loss: 0.212071
2023-01-01 09:18: Train Epoch 4: 363/634 Loss: 0.198964
2023-01-01 09:18: Train Epoch 4: 367/634 Loss: 0.188185
2023-01-01 09:18: Train Epoch 4: 371/634 Loss: 0.177229
2023-01-01 09:19: Train Epoch 4: 375/634 Loss: 0.154618
2023-01-01 09:19: Train Epoch 4: 379/634 Loss: 0.196273
2023-01-01 09:19: Train Epoch 4: 383/634 Loss: 0.152509
2023-01-01 09:20: Train Epoch 4: 387/634 Loss: 0.142348
2023-01-01 09:20: Train Epoch 4: 391/634 Loss: 0.171005
2023-01-01 09:20: Train Epoch 4: 395/634 Loss: 0.177457
2023-01-01 09:21: Train Epoch 4: 399/634 Loss: 0.167737
2023-01-01 09:21: Train Epoch 4: 403/634 Loss: 0.149728
2023-01-01 09:21: Train Epoch 4: 407/634 Loss: 0.236230
2023-01-01 09:22: Train Epoch 4: 411/634 Loss: 0.175319
2023-01-01 09:22: Train Epoch 4: 415/634 Loss: 0.209128
2023-01-01 09:23: Train Epoch 4: 419/634 Loss: 0.180303
2023-01-01 09:23: Train Epoch 4: 423/634 Loss: 0.185913
2023-01-01 09:23: Train Epoch 4: 427/634 Loss: 0.186089
2023-01-01 09:24: Train Epoch 4: 431/634 Loss: 0.206331
2023-01-01 09:24: Train Epoch 4: 435/634 Loss: 0.141452
2023-01-01 09:24: Train Epoch 4: 439/634 Loss: 0.188748
2023-01-01 09:25: Train Epoch 4: 443/634 Loss: 0.164376
2023-01-01 09:25: Train Epoch 4: 447/634 Loss: 0.196561
2023-01-01 09:25: Train Epoch 4: 451/634 Loss: 0.179008
2023-01-01 09:26: Train Epoch 4: 455/634 Loss: 0.167834
2023-01-01 09:26: Train Epoch 4: 459/634 Loss: 0.175118
2023-01-01 09:26: Train Epoch 4: 463/634 Loss: 0.176466
2023-01-01 09:27: Train Epoch 4: 467/634 Loss: 0.178807
2023-01-01 09:27: Train Epoch 4: 471/634 Loss: 0.185086
2023-01-01 09:27: Train Epoch 4: 475/634 Loss: 0.184356
2023-01-01 09:28: Train Epoch 4: 479/634 Loss: 0.175256
2023-01-01 09:28: Train Epoch 4: 483/634 Loss: 0.170555
2023-01-01 09:29: Train Epoch 4: 487/634 Loss: 0.200973
2023-01-01 09:29: Train Epoch 4: 491/634 Loss: 0.171746
2023-01-01 09:29: Train Epoch 4: 495/634 Loss: 0.203373
2023-01-01 09:30: Train Epoch 4: 499/634 Loss: 0.185327
2023-01-01 09:30: Train Epoch 4: 503/634 Loss: 0.173652
2023-01-01 09:30: Train Epoch 4: 507/634 Loss: 0.224564
2023-01-01 09:31: Train Epoch 4: 511/634 Loss: 0.191181
2023-01-01 09:31: Train Epoch 4: 515/634 Loss: 0.156004
2023-01-01 09:32: Train Epoch 4: 519/634 Loss: 0.165310
2023-01-01 09:32: Train Epoch 4: 523/634 Loss: 0.172019
2023-01-01 09:32: Train Epoch 4: 527/634 Loss: 0.158124
2023-01-01 09:33: Train Epoch 4: 531/634 Loss: 0.179989
2023-01-01 09:33: Train Epoch 4: 535/634 Loss: 0.164257
2023-01-01 09:33: Train Epoch 4: 539/634 Loss: 0.167816
2023-01-01 09:34: Train Epoch 4: 543/634 Loss: 0.178957
2023-01-01 09:34: Train Epoch 4: 547/634 Loss: 0.153821
2023-01-01 09:35: Train Epoch 4: 551/634 Loss: 0.143871
2023-01-01 09:35: Train Epoch 4: 555/634 Loss: 0.185769
2023-01-01 09:35: Train Epoch 4: 559/634 Loss: 0.150210
2023-01-01 09:36: Train Epoch 4: 563/634 Loss: 0.163053
2023-01-01 09:36: Train Epoch 4: 567/634 Loss: 0.144418
2023-01-01 09:36: Train Epoch 4: 571/634 Loss: 0.166209
2023-01-01 09:37: Train Epoch 4: 575/634 Loss: 0.129168
2023-01-01 09:37: Train Epoch 4: 579/634 Loss: 0.156078
2023-01-01 09:38: Train Epoch 4: 583/634 Loss: 0.152640
2023-01-01 09:38: Train Epoch 4: 587/634 Loss: 0.164158
2023-01-01 09:38: Train Epoch 4: 591/634 Loss: 0.170206
2023-01-01 09:39: Train Epoch 4: 595/634 Loss: 0.138015
2023-01-01 09:39: Train Epoch 4: 599/634 Loss: 0.156412
2023-01-01 09:39: Train Epoch 4: 603/634 Loss: 0.167817
2023-01-01 09:40: Train Epoch 4: 607/634 Loss: 0.148274
2023-01-01 09:40: Train Epoch 4: 611/634 Loss: 0.158847
2023-01-01 09:40: Train Epoch 4: 615/634 Loss: 0.158470
2023-01-01 09:41: Train Epoch 4: 619/634 Loss: 0.180904
2023-01-01 09:41: Train Epoch 4: 623/634 Loss: 0.155212
2023-01-01 09:41: Train Epoch 4: 627/634 Loss: 0.172396
2023-01-01 09:42: Train Epoch 4: 631/634 Loss: 0.131765
2023-01-01 09:42: Train Epoch 4: 633/634 Loss: 0.079137
2023-01-01 09:42: **********Train Epoch 4: averaged Loss: 0.173275 
2023-01-01 09:42: 
Epoch time elapsed: 3393.8830213546753

2023-01-01 09:43: 
 metrics validation: {'precision': 0.8214285714285714, 'recall': 0.6723076923076923, 'f1-score': 0.7394247038917089, 'support': 1300, 'AUC': 0.9189674556213017, 'AUCPR': 0.8442930371246051, 'TP': 874, 'FP': 190, 'TN': 2410, 'FN': 426} 

2023-01-01 09:43: **********Val Epoch 4: average Loss: 0.167796
2023-01-01 09:43: *********************************Current best model saved!
2023-01-01 09:45: 
 Testing metrics {'precision': 0.8551165146909828, 'recall': 0.6872964169381107, 'f1-score': 0.762076749435666, 'support': 1228, 'AUC': 0.9221765615550298, 'AUCPR': 0.8732191373473247, 'TP': 844, 'FP': 143, 'TN': 2313, 'FN': 384} 

2023-01-01 09:50: 
 Testing metrics {'precision': 0.9146124523506989, 'recall': 0.8166553210800999, 'f1-score': 0.8628626228722129, 'support': 4407, 'AUC': 0.9729413295441597, 'AUCPR': 0.9489349820992912, 'TP': 3599, 'FP': 336, 'TN': 8478, 'FN': 808} 

2023-01-01 09:50: Train Epoch 5: 3/634 Loss: 0.184844
2023-01-01 09:51: Train Epoch 5: 7/634 Loss: 0.178562
2023-01-01 09:51: Train Epoch 5: 11/634 Loss: 0.183378
2023-01-01 09:51: Train Epoch 5: 15/634 Loss: 0.165074
2023-01-01 09:52: Train Epoch 5: 19/634 Loss: 0.154356
2023-01-01 09:52: Train Epoch 5: 23/634 Loss: 0.184220
2023-01-01 09:52: Train Epoch 5: 27/634 Loss: 0.163485
2023-01-01 09:53: Train Epoch 5: 31/634 Loss: 0.169049
2023-01-01 09:53: Train Epoch 5: 35/634 Loss: 0.144620
2023-01-01 09:53: Train Epoch 5: 39/634 Loss: 0.182051
2023-01-01 09:54: Train Epoch 5: 43/634 Loss: 0.145704
2023-01-01 09:54: Train Epoch 5: 47/634 Loss: 0.164513
2023-01-01 09:55: Train Epoch 5: 51/634 Loss: 0.136995
2023-01-01 09:55: Train Epoch 5: 55/634 Loss: 0.167027
2023-01-01 09:55: Train Epoch 5: 59/634 Loss: 0.144422
2023-01-01 09:56: Train Epoch 5: 63/634 Loss: 0.171057
2023-01-01 09:56: Train Epoch 5: 67/634 Loss: 0.158494
2023-01-01 09:56: Train Epoch 5: 71/634 Loss: 0.166152
2023-01-01 09:57: Train Epoch 5: 75/634 Loss: 0.169535
2023-01-01 09:57: Train Epoch 5: 79/634 Loss: 0.169107
2023-01-01 09:57: Train Epoch 5: 83/634 Loss: 0.176888
2023-01-01 09:58: Train Epoch 5: 87/634 Loss: 0.192138
2023-01-01 09:58: Train Epoch 5: 91/634 Loss: 0.163657
2023-01-01 09:59: Train Epoch 5: 95/634 Loss: 0.179240
2023-01-01 09:59: Train Epoch 5: 99/634 Loss: 0.149609
2023-01-01 09:59: Train Epoch 5: 103/634 Loss: 0.164895
2023-01-01 10:00: Train Epoch 5: 107/634 Loss: 0.197398
2023-01-01 10:00: Train Epoch 5: 111/634 Loss: 0.152016
2023-01-01 10:00: Train Epoch 5: 115/634 Loss: 0.147693
2023-01-01 10:01: Train Epoch 5: 119/634 Loss: 0.163190
2023-01-01 10:01: Train Epoch 5: 123/634 Loss: 0.182633
2023-01-01 10:01: Train Epoch 5: 127/634 Loss: 0.160241
2023-01-01 10:02: Train Epoch 5: 131/634 Loss: 0.179171
2023-01-01 10:02: Train Epoch 5: 135/634 Loss: 0.153502
2023-01-01 10:03: Train Epoch 5: 139/634 Loss: 0.162324
2023-01-01 10:03: Train Epoch 5: 143/634 Loss: 0.157206
2023-01-01 10:03: Train Epoch 5: 147/634 Loss: 0.205879
2023-01-01 10:04: Train Epoch 5: 151/634 Loss: 0.157751
2023-01-01 10:04: Train Epoch 5: 155/634 Loss: 0.179899
2023-01-01 10:04: Train Epoch 5: 159/634 Loss: 0.152968
2023-01-01 10:05: Train Epoch 5: 163/634 Loss: 0.178741
2023-01-01 10:05: Train Epoch 5: 167/634 Loss: 0.152721
2023-01-01 10:05: Train Epoch 5: 171/634 Loss: 0.170542
2023-01-01 10:06: Train Epoch 5: 175/634 Loss: 0.151798
2023-01-01 10:06: Train Epoch 5: 179/634 Loss: 0.161446
2023-01-01 10:07: Train Epoch 5: 183/634 Loss: 0.181588
2023-01-01 10:07: Train Epoch 5: 187/634 Loss: 0.140376
2023-01-01 10:07: Train Epoch 5: 191/634 Loss: 0.182904
2023-01-01 10:08: Train Epoch 5: 195/634 Loss: 0.151845
2023-01-01 10:08: Train Epoch 5: 199/634 Loss: 0.184933
2023-01-01 10:08: Train Epoch 5: 203/634 Loss: 0.145133
2023-01-01 10:09: Train Epoch 5: 207/634 Loss: 0.155072
2023-01-01 10:09: Train Epoch 5: 211/634 Loss: 0.138831
2023-01-01 10:09: Train Epoch 5: 215/634 Loss: 0.154837
2023-01-01 10:10: Train Epoch 5: 219/634 Loss: 0.156098
2023-01-01 10:10: Train Epoch 5: 223/634 Loss: 0.177803
2023-01-01 10:11: Train Epoch 5: 227/634 Loss: 0.175054
2023-01-01 10:11: Train Epoch 5: 231/634 Loss: 0.184109
2023-01-01 10:11: Train Epoch 5: 235/634 Loss: 0.184464
2023-01-01 10:12: Train Epoch 5: 239/634 Loss: 0.148129
2023-01-01 10:12: Train Epoch 5: 243/634 Loss: 0.147277
2023-01-01 10:12: Train Epoch 5: 247/634 Loss: 0.156901
2023-01-01 10:13: Train Epoch 5: 251/634 Loss: 0.171755
2023-01-01 10:13: Train Epoch 5: 255/634 Loss: 0.165725
2023-01-01 10:13: Train Epoch 5: 259/634 Loss: 0.157795
2023-01-01 10:14: Train Epoch 5: 263/634 Loss: 0.152158
2023-01-01 10:14: Train Epoch 5: 267/634 Loss: 0.193188
2023-01-01 10:15: Train Epoch 5: 271/634 Loss: 0.161217
2023-01-01 10:15: Train Epoch 5: 275/634 Loss: 0.166169
2023-01-01 10:15: Train Epoch 5: 279/634 Loss: 0.161733
2023-01-01 10:16: Train Epoch 5: 283/634 Loss: 0.157287
2023-01-01 10:16: Train Epoch 5: 287/634 Loss: 0.135898
2023-01-01 10:16: Train Epoch 5: 291/634 Loss: 0.195417
2023-01-01 10:17: Train Epoch 5: 295/634 Loss: 0.175034
2023-01-01 10:17: Train Epoch 5: 299/634 Loss: 0.173023
2023-01-01 10:17: Train Epoch 5: 303/634 Loss: 0.165171
2023-01-01 10:18: Train Epoch 5: 307/634 Loss: 0.160407
2023-01-01 10:18: Train Epoch 5: 311/634 Loss: 0.177291
2023-01-01 10:18: Train Epoch 5: 315/634 Loss: 0.173076
2023-01-01 10:19: Train Epoch 5: 319/634 Loss: 0.159210
2023-01-01 10:19: Train Epoch 5: 323/634 Loss: 0.171312
2023-01-01 10:19: Train Epoch 5: 327/634 Loss: 0.204153
2023-01-01 10:20: Train Epoch 5: 331/634 Loss: 0.188458
2023-01-01 10:20: Train Epoch 5: 335/634 Loss: 0.167586
2023-01-01 10:20: Train Epoch 5: 339/634 Loss: 0.183770
2023-01-01 10:21: Train Epoch 5: 343/634 Loss: 0.202103
2023-01-01 10:21: Train Epoch 5: 347/634 Loss: 0.189301
2023-01-01 10:22: Train Epoch 5: 351/634 Loss: 0.233481
2023-01-01 10:22: Train Epoch 5: 355/634 Loss: 0.190866
2023-01-01 10:22: Train Epoch 5: 359/634 Loss: 0.171837
2023-01-01 10:23: Train Epoch 5: 363/634 Loss: 0.224609
2023-01-01 10:23: Train Epoch 5: 367/634 Loss: 0.180866
2023-01-01 10:23: Train Epoch 5: 371/634 Loss: 0.174179
2023-01-01 10:24: Train Epoch 5: 375/634 Loss: 0.139973
2023-01-01 10:24: Train Epoch 5: 379/634 Loss: 0.174915
2023-01-01 10:24: Train Epoch 5: 383/634 Loss: 0.167952
2023-01-01 10:25: Train Epoch 5: 387/634 Loss: 0.184967
2023-01-01 10:25: Train Epoch 5: 391/634 Loss: 0.163184
2023-01-01 10:26: Train Epoch 5: 395/634 Loss: 0.171070
2023-01-01 10:26: Train Epoch 5: 399/634 Loss: 0.186793
2023-01-01 10:26: Train Epoch 5: 403/634 Loss: 0.193302
2023-01-01 10:27: Train Epoch 5: 407/634 Loss: 0.160777
2023-01-01 10:27: Train Epoch 5: 411/634 Loss: 0.182051
2023-01-01 10:27: Train Epoch 5: 415/634 Loss: 0.184023
2023-01-01 10:28: Train Epoch 5: 419/634 Loss: 0.186348
2023-01-01 10:28: Train Epoch 5: 423/634 Loss: 0.163881
2023-01-01 10:28: Train Epoch 5: 427/634 Loss: 0.207597
2023-01-01 10:29: Train Epoch 5: 431/634 Loss: 0.150956
2023-01-01 10:29: Train Epoch 5: 435/634 Loss: 0.168432
2023-01-01 10:30: Train Epoch 5: 439/634 Loss: 0.178501
2023-01-01 10:30: Train Epoch 5: 443/634 Loss: 0.176690
2023-01-01 10:30: Train Epoch 5: 447/634 Loss: 0.177524
2023-01-01 10:31: Train Epoch 5: 451/634 Loss: 0.145169
2023-01-01 10:31: Train Epoch 5: 455/634 Loss: 0.168685
2023-01-01 10:31: Train Epoch 5: 459/634 Loss: 0.168019
2023-01-01 10:32: Train Epoch 5: 463/634 Loss: 0.156597
2023-01-01 10:32: Train Epoch 5: 467/634 Loss: 0.163283
2023-01-01 10:32: Train Epoch 5: 471/634 Loss: 0.159866
2023-01-01 10:33: Train Epoch 5: 475/634 Loss: 0.168568
2023-01-01 10:33: Train Epoch 5: 479/634 Loss: 0.153025
2023-01-01 10:33: Train Epoch 5: 483/634 Loss: 0.130513
2023-01-01 10:34: Train Epoch 5: 487/634 Loss: 0.194842
2023-01-01 10:34: Train Epoch 5: 491/634 Loss: 0.154846
2023-01-01 10:35: Train Epoch 5: 495/634 Loss: 0.173168
2023-01-01 10:35: Train Epoch 5: 499/634 Loss: 0.193252
2023-01-01 10:35: Train Epoch 5: 503/634 Loss: 0.167569
2023-01-01 10:36: Train Epoch 5: 507/634 Loss: 0.181213
2023-01-01 10:36: Train Epoch 5: 511/634 Loss: 0.186596
2023-01-01 10:36: Train Epoch 5: 515/634 Loss: 0.178285
2023-01-01 10:37: Train Epoch 5: 519/634 Loss: 0.175239
2023-01-01 10:37: Train Epoch 5: 523/634 Loss: 0.166505
2023-01-01 10:37: Train Epoch 5: 527/634 Loss: 0.177979
2023-01-01 10:38: Train Epoch 5: 531/634 Loss: 0.187847
2023-01-01 10:38: Train Epoch 5: 535/634 Loss: 0.160717
2023-01-01 10:39: Train Epoch 5: 539/634 Loss: 0.178298
2023-01-01 10:39: Train Epoch 5: 543/634 Loss: 0.166821
2023-01-01 10:39: Train Epoch 5: 547/634 Loss: 0.168643
2023-01-01 10:40: Train Epoch 5: 551/634 Loss: 0.164500
2023-01-01 10:40: Train Epoch 5: 555/634 Loss: 0.147789
2023-01-01 10:40: Train Epoch 5: 559/634 Loss: 0.176455
2023-01-01 10:41: Train Epoch 5: 563/634 Loss: 0.150621
2023-01-01 10:41: Train Epoch 5: 567/634 Loss: 0.188139
2023-01-01 10:42: Train Epoch 5: 571/634 Loss: 0.174347
2023-01-01 10:42: Train Epoch 5: 575/634 Loss: 0.177877
2023-01-01 10:42: Train Epoch 5: 579/634 Loss: 0.137708
2023-01-01 10:43: Train Epoch 5: 583/634 Loss: 0.172764
2023-01-01 10:43: Train Epoch 5: 587/634 Loss: 0.156490
2023-01-01 10:43: Train Epoch 5: 591/634 Loss: 0.168556
2023-01-01 10:44: Train Epoch 5: 595/634 Loss: 0.167536
2023-01-01 10:44: Train Epoch 5: 599/634 Loss: 0.161421
2023-01-01 10:44: Train Epoch 5: 603/634 Loss: 0.164122
2023-01-01 10:45: Train Epoch 5: 607/634 Loss: 0.158594
2023-01-01 10:45: Train Epoch 5: 611/634 Loss: 0.151054
2023-01-01 10:45: Train Epoch 5: 615/634 Loss: 0.174132
2023-01-01 10:46: Train Epoch 5: 619/634 Loss: 0.185748
2023-01-01 10:46: Train Epoch 5: 623/634 Loss: 0.158855
2023-01-01 10:47: Train Epoch 5: 627/634 Loss: 0.158286
2023-01-01 10:47: Train Epoch 5: 631/634 Loss: 0.189309
2023-01-01 10:47: Train Epoch 5: 633/634 Loss: 0.068998
2023-01-01 10:47: **********Train Epoch 5: averaged Loss: 0.168826 
2023-01-01 10:47: 
Epoch time elapsed: 3435.7088119983673

2023-01-01 10:49: 
 metrics validation: {'precision': 0.7757980697847068, 'recall': 0.8038461538461539, 'f1-score': 0.7895731016244807, 'support': 1300, 'AUC': 0.9253218934911244, 'AUCPR': 0.8571118670159099, 'TP': 1045, 'FP': 302, 'TN': 2298, 'FN': 255} 

2023-01-01 10:49: **********Val Epoch 5: average Loss: 0.155374
2023-01-01 10:49: *********************************Current best model saved!
2023-01-01 10:50: 
 Testing metrics {'precision': 0.8096856414613424, 'recall': 0.7760586319218241, 'f1-score': 0.7925155925155926, 'support': 1228, 'AUC': 0.9260224246411103, 'AUCPR': 0.8814196290905293, 'TP': 953, 'FP': 224, 'TN': 2232, 'FN': 275} 

2023-01-01 10:55: 
 Testing metrics {'precision': 0.8662952646239555, 'recall': 0.8468345813478557, 'f1-score': 0.8564543889845095, 'support': 4407, 'AUC': 0.9690275655790093, 'AUCPR': 0.9429331310281731, 'TP': 3732, 'FP': 576, 'TN': 8238, 'FN': 675} 

2023-01-01 10:56: Train Epoch 6: 3/634 Loss: 0.155348
2023-01-01 10:56: Train Epoch 6: 7/634 Loss: 0.136986
2023-01-01 10:56: Train Epoch 6: 11/634 Loss: 0.155503
2023-01-01 10:57: Train Epoch 6: 15/634 Loss: 0.157878
2023-01-01 10:57: Train Epoch 6: 19/634 Loss: 0.152822
2023-01-01 10:57: Train Epoch 6: 23/634 Loss: 0.204052
2023-01-01 10:58: Train Epoch 6: 27/634 Loss: 0.137556
2023-01-01 10:58: Train Epoch 6: 31/634 Loss: 0.151191
2023-01-01 10:59: Train Epoch 6: 35/634 Loss: 0.185486
2023-01-01 10:59: Train Epoch 6: 39/634 Loss: 0.167890
2023-01-01 10:59: Train Epoch 6: 43/634 Loss: 0.176565
2023-01-01 11:00: Train Epoch 6: 47/634 Loss: 0.182113
2023-01-01 11:00: Train Epoch 6: 51/634 Loss: 0.176475
2023-01-01 11:00: Train Epoch 6: 55/634 Loss: 0.166880
2023-01-01 11:01: Train Epoch 6: 59/634 Loss: 0.173472
2023-01-01 11:01: Train Epoch 6: 63/634 Loss: 0.156187
2023-01-01 11:02: Train Epoch 6: 67/634 Loss: 0.170317
2023-01-01 11:02: Train Epoch 6: 71/634 Loss: 0.142186
2023-01-01 11:02: Train Epoch 6: 75/634 Loss: 0.156886
2023-01-01 11:03: Train Epoch 6: 79/634 Loss: 0.158077
2023-01-01 11:03: Train Epoch 6: 83/634 Loss: 0.178332
2023-01-01 11:03: Train Epoch 6: 87/634 Loss: 0.156039
2023-01-01 11:04: Train Epoch 6: 91/634 Loss: 0.140706
2023-01-01 11:04: Train Epoch 6: 95/634 Loss: 0.182612
2023-01-01 11:05: Train Epoch 6: 99/634 Loss: 0.176564
2023-01-01 11:05: Train Epoch 6: 103/634 Loss: 0.165824
2023-01-01 11:05: Train Epoch 6: 107/634 Loss: 0.183412
2023-01-01 11:06: Train Epoch 6: 111/634 Loss: 0.206546
2023-01-01 11:06: Train Epoch 6: 115/634 Loss: 0.155727
2023-01-01 11:06: Train Epoch 6: 119/634 Loss: 0.162476
2023-01-01 11:07: Train Epoch 6: 123/634 Loss: 0.180781
2023-01-01 11:07: Train Epoch 6: 127/634 Loss: 0.139313
2023-01-01 11:07: Train Epoch 6: 131/634 Loss: 0.156313
2023-01-01 11:08: Train Epoch 6: 135/634 Loss: 0.140205
2023-01-01 11:08: Train Epoch 6: 139/634 Loss: 0.179098
2023-01-01 11:09: Train Epoch 6: 143/634 Loss: 0.144559
2023-01-01 11:09: Train Epoch 6: 147/634 Loss: 0.142292
2023-01-01 11:09: Train Epoch 6: 151/634 Loss: 0.161595
2023-01-01 11:10: Train Epoch 6: 155/634 Loss: 0.166043
2023-01-01 11:10: Train Epoch 6: 159/634 Loss: 0.145005
2023-01-01 11:10: Train Epoch 6: 163/634 Loss: 0.154230
2023-01-01 11:11: Train Epoch 6: 167/634 Loss: 0.153587
2023-01-01 11:11: Train Epoch 6: 171/634 Loss: 0.155661
2023-01-01 11:11: Train Epoch 6: 175/634 Loss: 0.183936
2023-01-01 11:12: Train Epoch 6: 179/634 Loss: 0.175561
2023-01-01 11:12: Train Epoch 6: 183/634 Loss: 0.169643
2023-01-01 11:12: Train Epoch 6: 187/634 Loss: 0.160453
2023-01-01 11:13: Train Epoch 6: 191/634 Loss: 0.164868
2023-01-01 11:13: Train Epoch 6: 195/634 Loss: 0.156301
2023-01-01 11:14: Train Epoch 6: 199/634 Loss: 0.161987
2023-01-01 11:14: Train Epoch 6: 203/634 Loss: 0.177703
2023-01-01 11:14: Train Epoch 6: 207/634 Loss: 0.170124
2023-01-01 11:15: Train Epoch 6: 211/634 Loss: 0.159464
2023-01-01 11:15: Train Epoch 6: 215/634 Loss: 0.159622
2023-01-01 11:15: Train Epoch 6: 219/634 Loss: 0.144207
2023-01-01 11:16: Train Epoch 6: 223/634 Loss: 0.129745
2023-01-01 11:16: Train Epoch 6: 227/634 Loss: 0.186933
2023-01-01 11:16: Train Epoch 6: 231/634 Loss: 0.175017
2023-01-01 11:17: Train Epoch 6: 235/634 Loss: 0.149980
2023-01-01 11:17: Train Epoch 6: 239/634 Loss: 0.174712
2023-01-01 11:17: Train Epoch 6: 243/634 Loss: 0.161721
2023-01-01 11:18: Train Epoch 6: 247/634 Loss: 0.188454
2023-01-01 11:18: Train Epoch 6: 251/634 Loss: 0.164247
2023-01-01 11:18: Train Epoch 6: 255/634 Loss: 0.188746
2023-01-01 11:19: Train Epoch 6: 259/634 Loss: 0.159778
2023-01-01 11:19: Train Epoch 6: 263/634 Loss: 0.154378
2023-01-01 11:19: Train Epoch 6: 267/634 Loss: 0.196268
2023-01-01 11:20: Train Epoch 6: 271/634 Loss: 0.167046
2023-01-01 11:20: Train Epoch 6: 275/634 Loss: 0.177326
2023-01-01 11:20: Train Epoch 6: 279/634 Loss: 0.184888
2023-01-01 11:21: Train Epoch 6: 283/634 Loss: 0.144850
2023-01-01 11:21: Train Epoch 6: 287/634 Loss: 0.170004
2023-01-01 11:21: Train Epoch 6: 291/634 Loss: 0.181334
2023-01-01 11:22: Train Epoch 6: 295/634 Loss: 0.178513
2023-01-01 11:22: Train Epoch 6: 299/634 Loss: 0.129910
2023-01-01 11:22: Train Epoch 6: 303/634 Loss: 0.145619
2023-01-01 11:23: Train Epoch 6: 307/634 Loss: 0.173333
2023-01-01 11:23: Train Epoch 6: 311/634 Loss: 0.166364
2023-01-01 11:23: Train Epoch 6: 315/634 Loss: 0.149311
2023-01-01 11:24: Train Epoch 6: 319/634 Loss: 0.143449
2023-01-01 11:24: Train Epoch 6: 323/634 Loss: 0.167747
2023-01-01 11:25: Train Epoch 6: 327/634 Loss: 0.152887
2023-01-01 11:25: Train Epoch 6: 331/634 Loss: 0.175873
2023-01-01 11:25: Train Epoch 6: 335/634 Loss: 0.146779
2023-01-01 11:26: Train Epoch 6: 339/634 Loss: 0.174324
2023-01-01 11:26: Train Epoch 6: 343/634 Loss: 0.153287
2023-01-01 11:26: Train Epoch 6: 347/634 Loss: 0.134350
2023-01-01 11:27: Train Epoch 6: 351/634 Loss: 0.183264
2023-01-01 11:27: Train Epoch 6: 355/634 Loss: 0.160672
2023-01-01 11:27: Train Epoch 6: 359/634 Loss: 0.157145
2023-01-01 11:28: Train Epoch 6: 363/634 Loss: 0.174234
2023-01-01 11:28: Train Epoch 6: 367/634 Loss: 0.159475
2023-01-01 11:28: Train Epoch 6: 371/634 Loss: 0.157972
2023-01-01 11:29: Train Epoch 6: 375/634 Loss: 0.168282
2023-01-01 11:29: Train Epoch 6: 379/634 Loss: 0.181176
2023-01-01 11:29: Train Epoch 6: 383/634 Loss: 0.148284
2023-01-01 11:30: Train Epoch 6: 387/634 Loss: 0.177099
2023-01-01 11:30: Train Epoch 6: 391/634 Loss: 0.149746
2023-01-01 11:30: Train Epoch 6: 395/634 Loss: 0.159171
2023-01-01 11:31: Train Epoch 6: 399/634 Loss: 0.135424
2023-01-01 11:31: Train Epoch 6: 403/634 Loss: 0.160460
2023-01-01 11:32: Train Epoch 6: 407/634 Loss: 0.184807
2023-01-01 11:32: Train Epoch 6: 411/634 Loss: 0.151001
2023-01-01 11:32: Train Epoch 6: 415/634 Loss: 0.157030
2023-01-01 11:33: Train Epoch 6: 419/634 Loss: 0.157524
2023-01-01 11:33: Train Epoch 6: 423/634 Loss: 0.150904
2023-01-01 11:33: Train Epoch 6: 427/634 Loss: 0.136576
2023-01-01 11:34: Train Epoch 6: 431/634 Loss: 0.141642
2023-01-01 11:34: Train Epoch 6: 435/634 Loss: 0.175194
2023-01-01 11:34: Train Epoch 6: 439/634 Loss: 0.154720
2023-01-01 11:35: Train Epoch 6: 443/634 Loss: 0.141776
2023-01-01 11:35: Train Epoch 6: 447/634 Loss: 0.113637
2023-01-01 11:35: Train Epoch 6: 451/634 Loss: 0.139691
2023-01-01 11:36: Train Epoch 6: 455/634 Loss: 0.143289
2023-01-01 11:36: Train Epoch 6: 459/634 Loss: 0.130629
2023-01-01 11:36: Train Epoch 6: 463/634 Loss: 0.168350
2023-01-01 11:37: Train Epoch 6: 467/634 Loss: 0.138459
2023-01-01 11:37: Train Epoch 6: 471/634 Loss: 0.180705
2023-01-01 11:37: Train Epoch 6: 475/634 Loss: 0.156990
2023-01-01 11:38: Train Epoch 6: 479/634 Loss: 0.140222
2023-01-01 11:38: Train Epoch 6: 483/634 Loss: 0.193854
2023-01-01 11:38: Train Epoch 6: 487/634 Loss: 0.156316
2023-01-01 11:39: Train Epoch 6: 491/634 Loss: 0.167672
2023-01-01 11:39: Train Epoch 6: 495/634 Loss: 0.146199
2023-01-01 11:40: Train Epoch 6: 499/634 Loss: 0.191274
2023-01-01 11:40: Train Epoch 6: 503/634 Loss: 0.144992
2023-01-01 11:40: Train Epoch 6: 507/634 Loss: 0.184200
2023-01-01 11:41: Train Epoch 6: 511/634 Loss: 0.183119
2023-01-01 11:41: Train Epoch 6: 515/634 Loss: 0.183357
2023-01-01 11:41: Train Epoch 6: 519/634 Loss: 0.163669
2023-01-01 11:42: Train Epoch 6: 523/634 Loss: 0.153408
2023-01-01 11:42: Train Epoch 6: 527/634 Loss: 0.152620
2023-01-01 11:42: Train Epoch 6: 531/634 Loss: 0.168192
2023-01-01 11:43: Train Epoch 6: 535/634 Loss: 0.132917
2023-01-01 11:43: Train Epoch 6: 539/634 Loss: 0.149028
2023-01-01 11:44: Train Epoch 6: 543/634 Loss: 0.151923
2023-01-01 11:44: Train Epoch 6: 547/634 Loss: 0.159157
2023-01-01 11:44: Train Epoch 6: 551/634 Loss: 0.169959
2023-01-01 11:45: Train Epoch 6: 555/634 Loss: 0.163068
2023-01-01 11:45: Train Epoch 6: 559/634 Loss: 0.163939
2023-01-01 11:45: Train Epoch 6: 563/634 Loss: 0.170785
2023-01-01 11:46: Train Epoch 6: 567/634 Loss: 0.156748
2023-01-01 11:46: Train Epoch 6: 571/634 Loss: 0.166923
2023-01-01 11:46: Train Epoch 6: 575/634 Loss: 0.148127
2023-01-01 11:47: Train Epoch 6: 579/634 Loss: 0.174675
2023-01-01 11:47: Train Epoch 6: 583/634 Loss: 0.169601
2023-01-01 11:48: Train Epoch 6: 587/634 Loss: 0.154471
2023-01-01 11:48: Train Epoch 6: 591/634 Loss: 0.151546
2023-01-01 11:48: Train Epoch 6: 595/634 Loss: 0.155195
2023-01-01 11:49: Train Epoch 6: 599/634 Loss: 0.146141
2023-01-01 11:49: Train Epoch 6: 603/634 Loss: 0.179890
2023-01-01 11:49: Train Epoch 6: 607/634 Loss: 0.169315
2023-01-01 11:50: Train Epoch 6: 611/634 Loss: 0.170897
2023-01-01 11:50: Train Epoch 6: 615/634 Loss: 0.158920
2023-01-01 11:50: Train Epoch 6: 619/634 Loss: 0.158624
2023-01-01 11:51: Train Epoch 6: 623/634 Loss: 0.132201
2023-01-01 11:51: Train Epoch 6: 627/634 Loss: 0.155413
2023-01-01 11:51: Train Epoch 6: 631/634 Loss: 0.135763
2023-01-01 11:51: Train Epoch 6: 633/634 Loss: 0.058539
2023-01-01 11:51: **********Train Epoch 6: averaged Loss: 0.160692 
2023-01-01 11:51: 
Epoch time elapsed: 3377.7520711421967

2023-01-01 11:53: 
 metrics validation: {'precision': 0.764026402640264, 'recall': 0.7123076923076923, 'f1-score': 0.7372611464968154, 'support': 1300, 'AUC': 0.9039405325443787, 'AUCPR': 0.8285127010696234, 'TP': 926, 'FP': 286, 'TN': 2314, 'FN': 374} 

2023-01-01 11:53: **********Val Epoch 6: average Loss: 0.179707
2023-01-01 11:54: 
 Testing metrics {'precision': 0.8096856414613424, 'recall': 0.7760586319218241, 'f1-score': 0.7925155925155926, 'support': 1228, 'AUC': 0.9260224246411103, 'AUCPR': 0.8814196290905293, 'TP': 953, 'FP': 224, 'TN': 2232, 'FN': 275} 

2023-01-01 12:00: 
 Testing metrics {'precision': 0.8662952646239555, 'recall': 0.8468345813478557, 'f1-score': 0.8564543889845095, 'support': 4407, 'AUC': 0.9690275655790093, 'AUCPR': 0.9429331310281731, 'TP': 3732, 'FP': 576, 'TN': 8238, 'FN': 675} 

2023-01-01 12:00: Train Epoch 7: 3/634 Loss: 0.180090
2023-01-01 12:01: Train Epoch 7: 7/634 Loss: 0.175532
2023-01-01 12:01: Train Epoch 7: 11/634 Loss: 0.156817
2023-01-01 12:01: Train Epoch 7: 15/634 Loss: 0.162147
2023-01-01 12:02: Train Epoch 7: 19/634 Loss: 0.191427
2023-01-01 12:02: Train Epoch 7: 23/634 Loss: 0.158839
2023-01-01 12:02: Train Epoch 7: 27/634 Loss: 0.185629
2023-01-01 12:03: Train Epoch 7: 31/634 Loss: 0.168936
2023-01-01 12:03: Train Epoch 7: 35/634 Loss: 0.144019
2023-01-01 12:03: Train Epoch 7: 39/634 Loss: 0.171800
2023-01-01 12:04: Train Epoch 7: 43/634 Loss: 0.131644
2023-01-01 12:04: Train Epoch 7: 47/634 Loss: 0.189629
2023-01-01 12:05: Train Epoch 7: 51/634 Loss: 0.155677
2023-01-01 12:05: Train Epoch 7: 55/634 Loss: 0.166410
2023-01-01 12:05: Train Epoch 7: 59/634 Loss: 0.194913
2023-01-01 12:06: Train Epoch 7: 63/634 Loss: 0.157643
2023-01-01 12:06: Train Epoch 7: 67/634 Loss: 0.141789
2023-01-01 12:06: Train Epoch 7: 71/634 Loss: 0.171925
2023-01-01 12:07: Train Epoch 7: 75/634 Loss: 0.144473
2023-01-01 12:07: Train Epoch 7: 79/634 Loss: 0.165416
2023-01-01 12:07: Train Epoch 7: 83/634 Loss: 0.165574
2023-01-01 12:08: Train Epoch 7: 87/634 Loss: 0.158781
2023-01-01 12:08: Train Epoch 7: 91/634 Loss: 0.149785
2023-01-01 12:08: Train Epoch 7: 95/634 Loss: 0.148778
2023-01-01 12:09: Train Epoch 7: 99/634 Loss: 0.147637
2023-01-01 12:09: Train Epoch 7: 103/634 Loss: 0.179623
2023-01-01 12:10: Train Epoch 7: 107/634 Loss: 0.153521
2023-01-01 12:10: Train Epoch 7: 111/634 Loss: 0.130532
2023-01-01 12:10: Train Epoch 7: 115/634 Loss: 0.175518
2023-01-01 12:11: Train Epoch 7: 119/634 Loss: 0.120874
2023-01-01 12:11: Train Epoch 7: 123/634 Loss: 0.186018
2023-01-01 12:11: Train Epoch 7: 127/634 Loss: 0.160200
2023-01-01 12:12: Train Epoch 7: 131/634 Loss: 0.160100
2023-01-01 12:12: Train Epoch 7: 135/634 Loss: 0.157569
2023-01-01 12:12: Train Epoch 7: 139/634 Loss: 0.160938
2023-01-01 12:13: Train Epoch 7: 143/634 Loss: 0.175740
2023-01-01 12:13: Train Epoch 7: 147/634 Loss: 0.148496
2023-01-01 12:14: Train Epoch 7: 151/634 Loss: 0.145795
2023-01-01 12:14: Train Epoch 7: 155/634 Loss: 0.167183
2023-01-01 12:14: Train Epoch 7: 159/634 Loss: 0.174050
2023-01-01 12:15: Train Epoch 7: 163/634 Loss: 0.152401
2023-01-01 12:15: Train Epoch 7: 167/634 Loss: 0.172122
2023-01-01 12:15: Train Epoch 7: 171/634 Loss: 0.130222
2023-01-01 12:16: Train Epoch 7: 175/634 Loss: 0.149928
2023-01-01 12:16: Train Epoch 7: 179/634 Loss: 0.158098
2023-01-01 12:16: Train Epoch 7: 183/634 Loss: 0.160404
2023-01-01 12:17: Train Epoch 7: 187/634 Loss: 0.152683
2023-01-01 12:17: Train Epoch 7: 191/634 Loss: 0.183594
2023-01-01 12:18: Train Epoch 7: 195/634 Loss: 0.142101
2023-01-01 12:18: Train Epoch 7: 199/634 Loss: 0.158045
2023-01-01 12:18: Train Epoch 7: 203/634 Loss: 0.148330
2023-01-01 12:19: Train Epoch 7: 207/634 Loss: 0.167983
2023-01-01 12:19: Train Epoch 7: 211/634 Loss: 0.142030
2023-01-01 12:19: Train Epoch 7: 215/634 Loss: 0.148106
2023-01-01 12:20: Train Epoch 7: 219/634 Loss: 0.162113
2023-01-01 12:20: Train Epoch 7: 223/634 Loss: 0.189180
2023-01-01 12:20: Train Epoch 7: 227/634 Loss: 0.148257
2023-01-01 12:21: Train Epoch 7: 231/634 Loss: 0.132490
2023-01-01 12:21: Train Epoch 7: 235/634 Loss: 0.153105
2023-01-01 12:21: Train Epoch 7: 239/634 Loss: 0.174185
2023-01-01 12:22: Train Epoch 7: 243/634 Loss: 0.164752
2023-01-01 12:22: Train Epoch 7: 247/634 Loss: 0.157740
2023-01-01 12:23: Train Epoch 7: 251/634 Loss: 0.201674
2023-01-01 12:23: Train Epoch 7: 255/634 Loss: 0.153598
2023-01-01 12:23: Train Epoch 7: 259/634 Loss: 0.183937
2023-01-01 12:24: Train Epoch 7: 263/634 Loss: 0.135216
2023-01-01 12:24: Train Epoch 7: 267/634 Loss: 0.206359
2023-01-01 12:24: Train Epoch 7: 271/634 Loss: 0.152225
2023-01-01 12:25: Train Epoch 7: 275/634 Loss: 0.200590
2023-01-01 12:25: Train Epoch 7: 279/634 Loss: 0.187398
2023-01-01 12:26: Train Epoch 7: 283/634 Loss: 0.154366
2023-01-01 12:26: Train Epoch 7: 287/634 Loss: 0.152512
2023-01-01 12:26: Train Epoch 7: 291/634 Loss: 0.170795
2023-01-01 12:27: Train Epoch 7: 295/634 Loss: 0.158618
2023-01-01 12:27: Train Epoch 7: 299/634 Loss: 0.186515
2023-01-01 12:27: Train Epoch 7: 303/634 Loss: 0.177464
2023-01-01 12:28: Train Epoch 7: 307/634 Loss: 0.161877
2023-01-01 12:28: Train Epoch 7: 311/634 Loss: 0.169851
2023-01-01 12:29: Train Epoch 7: 315/634 Loss: 0.161995
2023-01-01 12:29: Train Epoch 7: 319/634 Loss: 0.176166
2023-01-01 12:29: Train Epoch 7: 323/634 Loss: 0.156698
2023-01-01 12:30: Train Epoch 7: 327/634 Loss: 0.140370
2023-01-01 12:30: Train Epoch 7: 331/634 Loss: 0.135587
2023-01-01 12:30: Train Epoch 7: 335/634 Loss: 0.175574
2023-01-01 12:31: Train Epoch 7: 339/634 Loss: 0.163230
2023-01-01 12:31: Train Epoch 7: 343/634 Loss: 0.186559
2023-01-01 12:32: Train Epoch 7: 347/634 Loss: 0.165546
2023-01-01 12:32: Train Epoch 7: 351/634 Loss: 0.164463
2023-01-01 12:32: Train Epoch 7: 355/634 Loss: 0.184010
2023-01-01 12:33: Train Epoch 7: 359/634 Loss: 0.177477
2023-01-01 12:33: Train Epoch 7: 363/634 Loss: 0.167948
2023-01-01 12:33: Train Epoch 7: 367/634 Loss: 0.177158
2023-01-01 12:34: Train Epoch 7: 371/634 Loss: 0.156209
2023-01-01 12:34: Train Epoch 7: 375/634 Loss: 0.159576
2023-01-01 12:34: Train Epoch 7: 379/634 Loss: 0.194840
2023-01-01 12:35: Train Epoch 7: 383/634 Loss: 0.160196
2023-01-01 12:35: Train Epoch 7: 387/634 Loss: 0.224619
2023-01-01 12:35: Train Epoch 7: 391/634 Loss: 0.170413
2023-01-01 12:36: Train Epoch 7: 395/634 Loss: 0.163814
2023-01-01 12:36: Train Epoch 7: 399/634 Loss: 0.175132
2023-01-01 12:37: Train Epoch 7: 403/634 Loss: 0.187318
2023-01-01 12:37: Train Epoch 7: 407/634 Loss: 0.154274
2023-01-01 12:37: Train Epoch 7: 411/634 Loss: 0.166372
2023-01-01 12:38: Train Epoch 7: 415/634 Loss: 0.204633
2023-01-01 12:38: Train Epoch 7: 419/634 Loss: 0.154067
2023-01-01 12:38: Train Epoch 7: 423/634 Loss: 0.142395
2023-01-01 12:39: Train Epoch 7: 427/634 Loss: 0.203778
2023-01-01 12:39: Train Epoch 7: 431/634 Loss: 0.197164
2023-01-01 12:39: Train Epoch 7: 435/634 Loss: 0.163251
2023-01-01 12:40: Train Epoch 7: 439/634 Loss: 0.191437
2023-01-01 12:40: Train Epoch 7: 443/634 Loss: 0.132510
2023-01-01 12:40: Train Epoch 7: 447/634 Loss: 0.166644
2023-01-01 12:41: Train Epoch 7: 451/634 Loss: 0.172400
2023-01-01 12:41: Train Epoch 7: 455/634 Loss: 0.173739
2023-01-01 12:42: Train Epoch 7: 459/634 Loss: 0.144178
2023-01-01 12:42: Train Epoch 7: 463/634 Loss: 0.148020
2023-01-01 12:42: Train Epoch 7: 467/634 Loss: 0.176899
2023-01-01 12:43: Train Epoch 7: 471/634 Loss: 0.159603
2023-01-01 12:43: Train Epoch 7: 475/634 Loss: 0.155322
2023-01-01 12:43: Train Epoch 7: 479/634 Loss: 0.169311
2023-01-01 12:44: Train Epoch 7: 483/634 Loss: 0.133072
2023-01-01 12:44: Train Epoch 7: 487/634 Loss: 0.162300
2023-01-01 12:44: Train Epoch 7: 491/634 Loss: 0.137466
2023-01-01 12:45: Train Epoch 7: 495/634 Loss: 0.163180
2023-01-01 12:45: Train Epoch 7: 499/634 Loss: 0.140973
2023-01-01 12:46: Train Epoch 7: 503/634 Loss: 0.160920
2023-01-01 12:46: Train Epoch 7: 507/634 Loss: 0.161619
2023-01-01 12:46: Train Epoch 7: 511/634 Loss: 0.172322
2023-01-01 12:47: Train Epoch 7: 515/634 Loss: 0.127039
2023-01-01 12:47: Train Epoch 7: 519/634 Loss: 0.136393
2023-01-01 12:47: Train Epoch 7: 523/634 Loss: 0.130775
2023-01-01 12:48: Train Epoch 7: 527/634 Loss: 0.151583
2023-01-01 12:48: Train Epoch 7: 531/634 Loss: 0.140256
2023-01-01 12:48: Train Epoch 7: 535/634 Loss: 0.181581
2023-01-01 12:49: Train Epoch 7: 539/634 Loss: 0.163773
2023-01-01 12:49: Train Epoch 7: 543/634 Loss: 0.139005
2023-01-01 12:50: Train Epoch 7: 547/634 Loss: 0.156439
2023-01-01 12:50: Train Epoch 7: 551/634 Loss: 0.153319
2023-01-01 12:50: Train Epoch 7: 555/634 Loss: 0.158691
2023-01-01 12:51: Train Epoch 7: 559/634 Loss: 0.176613
2023-01-01 12:51: Train Epoch 7: 563/634 Loss: 0.191148
2023-01-01 12:51: Train Epoch 7: 567/634 Loss: 0.142801
2023-01-01 12:52: Train Epoch 7: 571/634 Loss: 0.144292
2023-01-01 12:52: Train Epoch 7: 575/634 Loss: 0.154774
2023-01-01 12:53: Train Epoch 7: 579/634 Loss: 0.177032
2023-01-01 12:53: Train Epoch 7: 583/634 Loss: 0.148637
2023-01-01 12:53: Train Epoch 7: 587/634 Loss: 0.139583
2023-01-01 12:54: Train Epoch 7: 591/634 Loss: 0.140864
2023-01-01 12:54: Train Epoch 7: 595/634 Loss: 0.169981
2023-01-01 12:54: Train Epoch 7: 599/634 Loss: 0.141217
2023-01-01 12:55: Train Epoch 7: 603/634 Loss: 0.152856
2023-01-01 12:55: Train Epoch 7: 607/634 Loss: 0.167191
2023-01-01 12:55: Train Epoch 7: 611/634 Loss: 0.137867
2023-01-01 12:56: Train Epoch 7: 615/634 Loss: 0.146853
2023-01-01 12:56: Train Epoch 7: 619/634 Loss: 0.150281
2023-01-01 12:56: Train Epoch 7: 623/634 Loss: 0.154661
2023-01-01 12:57: Train Epoch 7: 627/634 Loss: 0.176447
2023-01-01 12:57: Train Epoch 7: 631/634 Loss: 0.158057
2023-01-01 12:57: Train Epoch 7: 633/634 Loss: 0.060602
2023-01-01 12:57: **********Train Epoch 7: averaged Loss: 0.161545 
2023-01-01 12:57: 
Epoch time elapsed: 3449.868748188019

2023-01-01 12:59: 
 metrics validation: {'precision': 0.872057318321392, 'recall': 0.6553846153846153, 'f1-score': 0.7483530961791832, 'support': 1300, 'AUC': 0.9315381656804733, 'AUCPR': 0.8710217758680376, 'TP': 852, 'FP': 125, 'TN': 2475, 'FN': 448} 

2023-01-01 12:59: **********Val Epoch 7: average Loss: 0.159079
2023-01-01 13:00: 
 Testing metrics {'precision': 0.8096856414613424, 'recall': 0.7760586319218241, 'f1-score': 0.7925155925155926, 'support': 1228, 'AUC': 0.9260224246411103, 'AUCPR': 0.8814196290905293, 'TP': 953, 'FP': 224, 'TN': 2232, 'FN': 275} 

2023-01-01 13:05: 
 Testing metrics {'precision': 0.8662952646239555, 'recall': 0.8468345813478557, 'f1-score': 0.8564543889845095, 'support': 4407, 'AUC': 0.9690275655790093, 'AUCPR': 0.9429331310281731, 'TP': 3732, 'FP': 576, 'TN': 8238, 'FN': 675} 

2023-01-01 13:05: Train Epoch 8: 3/634 Loss: 0.181826
2023-01-01 13:06: Train Epoch 8: 7/634 Loss: 0.150892
2023-01-01 13:06: Train Epoch 8: 11/634 Loss: 0.179407
2023-01-01 13:07: Train Epoch 8: 15/634 Loss: 0.172478
2023-01-01 13:07: Train Epoch 8: 19/634 Loss: 0.152375
2023-01-01 13:07: Train Epoch 8: 23/634 Loss: 0.192482
2023-01-01 13:08: Train Epoch 8: 27/634 Loss: 0.161639
2023-01-01 13:08: Train Epoch 8: 31/634 Loss: 0.179787
2023-01-01 13:08: Train Epoch 8: 35/634 Loss: 0.203614
2023-01-01 13:09: Train Epoch 8: 39/634 Loss: 0.140778
2023-01-01 13:09: Train Epoch 8: 43/634 Loss: 0.163688
2023-01-01 13:09: Train Epoch 8: 47/634 Loss: 0.144502
2023-01-01 13:10: Train Epoch 8: 51/634 Loss: 0.189245
2023-01-01 13:10: Train Epoch 8: 55/634 Loss: 0.132769
2023-01-01 13:10: Train Epoch 8: 59/634 Loss: 0.143443
2023-01-01 13:11: Train Epoch 8: 63/634 Loss: 0.168744
2023-01-01 13:11: Train Epoch 8: 67/634 Loss: 0.139941
2023-01-01 13:12: Train Epoch 8: 71/634 Loss: 0.152657
2023-01-01 13:12: Train Epoch 8: 75/634 Loss: 0.158965
2023-01-01 13:12: Train Epoch 8: 79/634 Loss: 0.165302
2023-01-01 13:13: Train Epoch 8: 83/634 Loss: 0.161152
2023-01-01 13:13: Train Epoch 8: 87/634 Loss: 0.157402
2023-01-01 13:13: Train Epoch 8: 91/634 Loss: 0.209859
2023-01-01 13:14: Train Epoch 8: 95/634 Loss: 0.191580
2023-01-01 13:14: Train Epoch 8: 99/634 Loss: 0.177728
2023-01-01 13:14: Train Epoch 8: 103/634 Loss: 0.193525
2023-01-01 13:15: Train Epoch 8: 107/634 Loss: 0.147065
2023-01-01 13:15: Train Epoch 8: 111/634 Loss: 0.192412
2023-01-01 13:15: Train Epoch 8: 115/634 Loss: 0.200822
2023-01-01 13:16: Train Epoch 8: 119/634 Loss: 0.146355
2023-01-01 13:16: Train Epoch 8: 123/634 Loss: 0.155323
2023-01-01 13:17: Train Epoch 8: 127/634 Loss: 0.208712
2023-01-01 13:17: Train Epoch 8: 131/634 Loss: 0.175864
2023-01-01 13:17: Train Epoch 8: 135/634 Loss: 0.166046
2023-01-01 13:17: Train Epoch 8: 139/634 Loss: 0.221096
2023-01-01 13:18: Train Epoch 8: 143/634 Loss: 0.197027
2023-01-01 13:18: Train Epoch 8: 147/634 Loss: 0.153375
2023-01-01 13:19: Train Epoch 8: 151/634 Loss: 0.193091
2023-01-01 13:19: Train Epoch 8: 155/634 Loss: 0.218468
2023-01-01 13:19: Train Epoch 8: 159/634 Loss: 0.191940
2023-01-01 13:20: Train Epoch 8: 163/634 Loss: 0.187059
2023-01-01 13:20: Train Epoch 8: 167/634 Loss: 0.286734
2023-01-01 13:20: Train Epoch 8: 171/634 Loss: 0.151633
2023-01-01 13:21: Train Epoch 8: 175/634 Loss: 0.148873
2023-01-01 13:21: Train Epoch 8: 179/634 Loss: 0.172550
2023-01-01 13:22: Train Epoch 8: 183/634 Loss: 0.236427
2023-01-01 13:22: Train Epoch 8: 187/634 Loss: 0.183256
2023-01-01 13:22: Train Epoch 8: 191/634 Loss: 0.183704
2023-01-01 13:23: Train Epoch 8: 195/634 Loss: 0.197938
2023-01-01 13:23: Train Epoch 8: 199/634 Loss: 0.203022
2023-01-01 13:23: Train Epoch 8: 203/634 Loss: 0.157670
2023-01-01 13:24: Train Epoch 8: 207/634 Loss: 0.243021
2023-01-01 13:24: Train Epoch 8: 211/634 Loss: 0.253871
2023-01-01 13:24: Train Epoch 8: 215/634 Loss: 0.184442
2023-01-01 13:25: Train Epoch 8: 219/634 Loss: 0.219032
2023-01-01 13:25: Train Epoch 8: 223/634 Loss: 0.233629
2023-01-01 13:26: Train Epoch 8: 227/634 Loss: 0.168938
2023-01-01 13:26: Train Epoch 8: 231/634 Loss: 0.169986
2023-01-01 13:26: Train Epoch 8: 235/634 Loss: 0.175103
2023-01-01 13:27: Train Epoch 8: 239/634 Loss: 0.184116
2023-01-01 13:27: Train Epoch 8: 243/634 Loss: 0.173787
2023-01-01 13:27: Train Epoch 8: 247/634 Loss: 0.188286
2023-01-01 13:28: Train Epoch 8: 251/634 Loss: 0.176218
2023-01-01 13:28: Train Epoch 8: 255/634 Loss: 0.185583
2023-01-01 13:28: Train Epoch 8: 259/634 Loss: 0.180656
2023-01-01 13:29: Train Epoch 8: 263/634 Loss: 0.178103
2023-01-01 13:29: Train Epoch 8: 267/634 Loss: 0.134679
2023-01-01 13:30: Train Epoch 8: 271/634 Loss: 0.144832
2023-01-01 13:30: Train Epoch 8: 275/634 Loss: 0.167294
2023-01-01 13:30: Train Epoch 8: 279/634 Loss: 0.203755
2023-01-01 13:31: Train Epoch 8: 283/634 Loss: 0.124359
2023-01-01 13:31: Train Epoch 8: 287/634 Loss: 0.181296
2023-01-01 13:31: Train Epoch 8: 291/634 Loss: 0.195683
2023-01-01 13:32: Train Epoch 8: 295/634 Loss: 0.181574
2023-01-01 13:32: Train Epoch 8: 299/634 Loss: 0.144042
2023-01-01 13:32: Train Epoch 8: 303/634 Loss: 0.155282
2023-01-01 13:33: Train Epoch 8: 307/634 Loss: 0.176167
2023-01-01 13:33: Train Epoch 8: 311/634 Loss: 0.163081
2023-01-01 13:34: Train Epoch 8: 315/634 Loss: 0.164852
2023-01-01 13:34: Train Epoch 8: 319/634 Loss: 0.183578
2023-01-01 13:34: Train Epoch 8: 323/634 Loss: 0.162995
2023-01-01 13:35: Train Epoch 8: 327/634 Loss: 0.156433
2023-01-01 13:35: Train Epoch 8: 331/634 Loss: 0.195905
2023-01-01 13:35: Train Epoch 8: 335/634 Loss: 0.154101
2023-01-01 13:36: Train Epoch 8: 339/634 Loss: 0.168526
2023-01-01 13:36: Train Epoch 8: 343/634 Loss: 0.146234
2023-01-01 13:36: Train Epoch 8: 347/634 Loss: 0.155308
2023-01-01 13:37: Train Epoch 8: 351/634 Loss: 0.160643
2023-01-01 13:37: Train Epoch 8: 355/634 Loss: 0.165812
2023-01-01 13:37: Train Epoch 8: 359/634 Loss: 0.163167
2023-01-01 13:38: Train Epoch 8: 363/634 Loss: 0.178721
2023-01-01 13:38: Train Epoch 8: 367/634 Loss: 0.182098
2023-01-01 13:39: Train Epoch 8: 371/634 Loss: 0.165588
2023-01-01 13:39: Train Epoch 8: 375/634 Loss: 0.151165
2023-01-01 13:39: Train Epoch 8: 379/634 Loss: 0.149608
2023-01-01 13:40: Train Epoch 8: 383/634 Loss: 0.152211
2023-01-01 13:40: Train Epoch 8: 387/634 Loss: 0.157363
2023-01-01 13:40: Train Epoch 8: 391/634 Loss: 0.160413
2023-01-01 13:41: Train Epoch 8: 395/634 Loss: 0.154618
2023-01-01 13:41: Train Epoch 8: 399/634 Loss: 0.161589
2023-01-01 13:41: Train Epoch 8: 403/634 Loss: 0.182514
2023-01-01 13:42: Train Epoch 8: 407/634 Loss: 0.144568
2023-01-01 13:42: Train Epoch 8: 411/634 Loss: 0.176622
2023-01-01 13:43: Train Epoch 8: 415/634 Loss: 0.144280
2023-01-01 13:43: Train Epoch 8: 419/634 Loss: 0.155999
2023-01-01 13:43: Train Epoch 8: 423/634 Loss: 0.207651
2023-01-01 13:44: Train Epoch 8: 427/634 Loss: 0.170768
2023-01-01 13:44: Train Epoch 8: 431/634 Loss: 0.161314
2023-01-01 13:44: Train Epoch 8: 435/634 Loss: 0.175075
2023-01-01 13:45: Train Epoch 8: 439/634 Loss: 0.163715
2023-01-01 13:45: Train Epoch 8: 443/634 Loss: 0.171916
2023-01-01 13:45: Train Epoch 8: 447/634 Loss: 0.190323
2023-01-01 13:46: Train Epoch 8: 451/634 Loss: 0.184268
2023-01-01 13:46: Train Epoch 8: 455/634 Loss: 0.171882
2023-01-01 13:46: Train Epoch 8: 459/634 Loss: 0.155127
2023-01-01 13:47: Train Epoch 8: 463/634 Loss: 0.195365
2023-01-01 13:47: Train Epoch 8: 467/634 Loss: 0.168780
2023-01-01 13:48: Train Epoch 8: 471/634 Loss: 0.138854
2023-01-01 13:48: Train Epoch 8: 475/634 Loss: 0.201272
2023-01-01 13:48: Train Epoch 8: 479/634 Loss: 0.174336
2023-01-01 13:49: Train Epoch 8: 483/634 Loss: 0.177226
2023-01-01 13:49: Train Epoch 8: 487/634 Loss: 0.184764
2023-01-01 13:49: Train Epoch 8: 491/634 Loss: 0.143620
2023-01-01 13:50: Train Epoch 8: 495/634 Loss: 0.152295
2023-01-01 13:50: Train Epoch 8: 499/634 Loss: 0.196796
2023-01-01 13:50: Train Epoch 8: 503/634 Loss: 0.188651
2023-01-01 13:51: Train Epoch 8: 507/634 Loss: 0.140649
2023-01-01 13:51: Train Epoch 8: 511/634 Loss: 0.164830
2023-01-01 13:51: Train Epoch 8: 515/634 Loss: 0.158027
2023-01-01 13:52: Train Epoch 8: 519/634 Loss: 0.173868
2023-01-01 13:52: Train Epoch 8: 523/634 Loss: 0.153492
2023-01-01 13:52: Train Epoch 8: 527/634 Loss: 0.161333
2023-01-01 13:53: Train Epoch 8: 531/634 Loss: 0.124824
2023-01-01 13:53: Train Epoch 8: 535/634 Loss: 0.150886
2023-01-01 13:53: Train Epoch 8: 539/634 Loss: 0.161881
2023-01-01 13:54: Train Epoch 8: 543/634 Loss: 0.140545
2023-01-01 13:54: Train Epoch 8: 547/634 Loss: 0.192847
2023-01-01 13:54: Train Epoch 8: 551/634 Loss: 0.186539
2023-01-01 13:55: Train Epoch 8: 555/634 Loss: 0.133877
2023-01-01 13:55: Train Epoch 8: 559/634 Loss: 0.146826
2023-01-01 13:55: Train Epoch 8: 563/634 Loss: 0.153315
2023-01-01 13:56: Train Epoch 8: 567/634 Loss: 0.185984
2023-01-01 13:56: Train Epoch 8: 571/634 Loss: 0.192962
2023-01-01 13:57: Train Epoch 8: 575/634 Loss: 0.174719
2023-01-01 13:57: Train Epoch 8: 579/634 Loss: 0.145564
2023-01-01 13:57: Train Epoch 8: 583/634 Loss: 0.166307
2023-01-01 13:58: Train Epoch 8: 587/634 Loss: 0.168574
2023-01-01 13:58: Train Epoch 8: 591/634 Loss: 0.165758
2023-01-01 13:58: Train Epoch 8: 595/634 Loss: 0.204097
2023-01-01 13:59: Train Epoch 8: 599/634 Loss: 0.173413
2023-01-01 13:59: Train Epoch 8: 603/634 Loss: 0.165165
2023-01-01 13:59: Train Epoch 8: 607/634 Loss: 0.147426
2023-01-01 14:00: Train Epoch 8: 611/634 Loss: 0.131678
2023-01-01 14:00: Train Epoch 8: 615/634 Loss: 0.173560
2023-01-01 14:00: Train Epoch 8: 619/634 Loss: 0.140513
2023-01-01 14:01: Train Epoch 8: 623/634 Loss: 0.154093
2023-01-01 14:01: Train Epoch 8: 627/634 Loss: 0.161836
2023-01-01 14:01: Train Epoch 8: 631/634 Loss: 0.158526
2023-01-01 14:02: Train Epoch 8: 633/634 Loss: 0.083446
2023-01-01 14:02: **********Train Epoch 8: averaged Loss: 0.171291 
2023-01-01 14:02: 
Epoch time elapsed: 3385.40727186203

2023-01-01 14:03: 
 metrics validation: {'precision': 0.7571533382245048, 'recall': 0.7938461538461539, 'f1-score': 0.7750657153586181, 'support': 1300, 'AUC': 0.921057396449704, 'AUCPR': 0.8465357262471631, 'TP': 1032, 'FP': 331, 'TN': 2269, 'FN': 268} 

2023-01-01 14:03: **********Val Epoch 8: average Loss: 0.163332
2023-01-01 14:04: 
 Testing metrics {'precision': 0.8096856414613424, 'recall': 0.7760586319218241, 'f1-score': 0.7925155925155926, 'support': 1228, 'AUC': 0.9260224246411103, 'AUCPR': 0.8814196290905293, 'TP': 953, 'FP': 224, 'TN': 2232, 'FN': 275} 

2023-01-01 14:10: 
 Testing metrics {'precision': 0.8662952646239555, 'recall': 0.8468345813478557, 'f1-score': 0.8564543889845095, 'support': 4407, 'AUC': 0.9690275655790093, 'AUCPR': 0.9429331310281731, 'TP': 3732, 'FP': 576, 'TN': 8238, 'FN': 675} 

2023-01-01 14:10: Train Epoch 9: 3/634 Loss: 0.184370
2023-01-01 14:10: Train Epoch 9: 7/634 Loss: 0.183644
2023-01-01 14:11: Train Epoch 9: 11/634 Loss: 0.145555
2023-01-01 14:11: Train Epoch 9: 15/634 Loss: 0.152672
2023-01-01 14:11: Train Epoch 9: 19/634 Loss: 0.161747
2023-01-01 14:12: Train Epoch 9: 23/634 Loss: 0.133917
2023-01-01 14:12: Train Epoch 9: 27/634 Loss: 0.134781
2023-01-01 14:12: Train Epoch 9: 31/634 Loss: 0.168953
2023-01-01 14:13: Train Epoch 9: 35/634 Loss: 0.164284
2023-01-01 14:13: Train Epoch 9: 39/634 Loss: 0.151025
2023-01-01 14:13: Train Epoch 9: 43/634 Loss: 0.173406
2023-01-01 14:14: Train Epoch 9: 47/634 Loss: 0.162233
2023-01-01 14:14: Train Epoch 9: 51/634 Loss: 0.168719
2023-01-01 14:14: Train Epoch 9: 55/634 Loss: 0.213958
2023-01-01 14:15: Train Epoch 9: 59/634 Loss: 0.192370
2023-01-01 14:15: Train Epoch 9: 63/634 Loss: 0.145935
2023-01-01 14:16: Train Epoch 9: 67/634 Loss: 0.201303
2023-01-01 14:16: Train Epoch 9: 71/634 Loss: 0.215724
2023-01-01 14:16: Train Epoch 9: 75/634 Loss: 0.148122
2023-01-01 14:17: Train Epoch 9: 79/634 Loss: 0.170774
2023-01-01 14:17: Train Epoch 9: 83/634 Loss: 0.220257
2023-01-01 14:17: Train Epoch 9: 87/634 Loss: 0.161732
2023-01-01 14:18: Train Epoch 9: 91/634 Loss: 0.173258
2023-01-01 14:18: Train Epoch 9: 95/634 Loss: 0.211917
2023-01-01 14:18: Train Epoch 9: 99/634 Loss: 0.172835
2023-01-01 14:19: Train Epoch 9: 103/634 Loss: 0.180405
2023-01-01 14:19: Train Epoch 9: 107/634 Loss: 0.179439
2023-01-01 14:20: Train Epoch 9: 111/634 Loss: 0.175572
2023-01-01 14:20: Train Epoch 9: 115/634 Loss: 0.147497
2023-01-01 14:20: Train Epoch 9: 119/634 Loss: 0.214344
2023-01-01 14:21: Train Epoch 9: 123/634 Loss: 0.210061
2023-01-01 14:21: Train Epoch 9: 127/634 Loss: 0.164783
2023-01-01 14:21: Train Epoch 9: 131/634 Loss: 0.156768
2023-01-01 14:22: Train Epoch 9: 135/634 Loss: 0.181017
2023-01-01 14:22: Train Epoch 9: 139/634 Loss: 0.166672
2023-01-01 14:22: Train Epoch 9: 143/634 Loss: 0.173030
2023-01-01 14:23: Train Epoch 9: 147/634 Loss: 0.149371
2023-01-01 14:23: Train Epoch 9: 151/634 Loss: 0.144785
2023-01-01 14:24: Train Epoch 9: 155/634 Loss: 0.163000
2023-01-01 14:24: Train Epoch 9: 159/634 Loss: 0.170121
2023-01-01 14:24: Train Epoch 9: 163/634 Loss: 0.133183
2023-01-01 14:25: Train Epoch 9: 167/634 Loss: 0.189155
2023-01-01 14:25: Train Epoch 9: 171/634 Loss: 0.154947
2023-01-01 14:25: Train Epoch 9: 175/634 Loss: 0.191034
2023-01-01 14:26: Train Epoch 9: 179/634 Loss: 0.168260
2023-01-01 14:26: Train Epoch 9: 183/634 Loss: 0.171718
2023-01-01 14:27: Train Epoch 9: 187/634 Loss: 0.145932
2023-01-01 14:27: Train Epoch 9: 191/634 Loss: 0.149169
2023-01-01 14:27: Train Epoch 9: 195/634 Loss: 0.149102
2023-01-01 14:28: Train Epoch 9: 199/634 Loss: 0.165705
2023-01-01 14:28: Train Epoch 9: 203/634 Loss: 0.157999
2023-01-01 14:29: Train Epoch 9: 207/634 Loss: 0.172639
2023-01-01 14:29: Train Epoch 9: 211/634 Loss: 0.151625
2023-01-01 14:29: Train Epoch 9: 215/634 Loss: 0.142759
2023-01-01 14:30: Train Epoch 9: 219/634 Loss: 0.159644
2023-01-01 14:30: Train Epoch 9: 223/634 Loss: 0.141015
2023-01-01 14:30: Train Epoch 9: 227/634 Loss: 0.158791
2023-01-01 14:31: Train Epoch 9: 231/634 Loss: 0.154768
2023-01-01 14:31: Train Epoch 9: 235/634 Loss: 0.171748
2023-01-01 14:31: Train Epoch 9: 239/634 Loss: 0.185740
2023-01-01 14:32: Train Epoch 9: 243/634 Loss: 0.158035
2023-01-01 14:32: Train Epoch 9: 247/634 Loss: 0.160020
2023-01-01 14:33: Train Epoch 9: 251/634 Loss: 0.169000
2023-01-01 14:33: Train Epoch 9: 255/634 Loss: 0.163952
2023-01-01 14:33: Train Epoch 9: 259/634 Loss: 0.169081
2023-01-01 14:34: Train Epoch 9: 263/634 Loss: 0.142247
2023-01-01 14:34: Train Epoch 9: 267/634 Loss: 0.153156
