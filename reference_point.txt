/home/joel.chacon/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)
  return torch._C._cuda_getDeviceCount() > 0
2023-01-08 13:43: log dir: /home/joel.chacon/tmp/ZIGZAG_from_files/experiments/2020/2023010813432491171534736
2023-01-08 13:43: Experiment log path in: /home/joel.chacon/tmp/ZIGZAG_from_files/experiments/2020/2023010813432491171534736
2023-01-08 13:43: Argument: Namespace(ZPI_dir='/home/joel.chacon/tmp/ZIGZAG_from_files/data/datasets_grl', batch_size=256, clc='vec', cuda=True, dataset='2020', dataset_root='/home/joel.chacon/tmp/selectingSampling/data/datasets_grl1/datasets_grl/', debug=False, default_graph=True, device='cpu', dynamic_features=['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh'], early_stop=True, early_stop_patience=8, embed_dim=64, epochs=30, grad_norm=False, horizon=1, input_dim=25, lag=10, link_len=2, log_dir='/home/joel.chacon/tmp/ZIGZAG_from_files/experiments/2020/2023010813432491171534736', log_step=1, loss_func='nllloss', lr_decay=True, lr_decay_rate=0.1, lr_decay_step='30', lr_init=0.0001, maxDimHoles=[1], max_grad_norm=5, minbatch_size=64, mode='train', model='fire_GCN', nan_fill=-1.0, num_layers=1, num_nodes=625, num_workers=12, output_dim=2, patch_height=25, patch_width=25, persistent_workers=True, pin_memory=True, plot=False, positive_weight=0.5, prefetch_factor=2, real_value=True, rnn_units=48, scaleParameter=[0.1], seed=10000, sizeBorder=[6], static_features=['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density'], teacher_forcing=False, weight_decay=0.0, window_len=10)
2023-01-08 13:43: Argument ZPI_dir: '/home/joel.chacon/tmp/ZIGZAG_from_files/data/datasets_grl'
2023-01-08 13:43: Argument batch_size: 256
2023-01-08 13:43: Argument clc: 'vec'
2023-01-08 13:43: Argument cuda: True
2023-01-08 13:43: Argument dataset: '2020'
2023-01-08 13:43: Argument dataset_root: '/home/joel.chacon/tmp/selectingSampling/data/datasets_grl1/datasets_grl/'
2023-01-08 13:43: Argument debug: False
2023-01-08 13:43: Argument default_graph: True
2023-01-08 13:43: Argument device: 'cpu'
2023-01-08 13:43: Argument dynamic_features: ['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh']
2023-01-08 13:43: Argument early_stop: True
2023-01-08 13:43: Argument early_stop_patience: 8
2023-01-08 13:43: Argument embed_dim: 64
2023-01-08 13:43: Argument epochs: 30
2023-01-08 13:43: Argument grad_norm: False
2023-01-08 13:43: Argument horizon: 1
2023-01-08 13:43: Argument input_dim: 25
2023-01-08 13:43: Argument lag: 10
2023-01-08 13:43: Argument link_len: 2
2023-01-08 13:43: Argument log_dir: '/home/joel.chacon/tmp/ZIGZAG_from_files/experiments/2020/2023010813432491171534736'
2023-01-08 13:43: Argument log_step: 1
2023-01-08 13:43: Argument loss_func: 'nllloss'
2023-01-08 13:43: Argument lr_decay: True
2023-01-08 13:43: Argument lr_decay_rate: 0.1
2023-01-08 13:43: Argument lr_decay_step: '30'
2023-01-08 13:43: Argument lr_init: 0.0001
2023-01-08 13:43: Argument maxDimHoles: [1]
2023-01-08 13:43: Argument max_grad_norm: 5
2023-01-08 13:43: Argument minbatch_size: 64
2023-01-08 13:43: Argument mode: 'train'
2023-01-08 13:43: Argument model: 'fire_GCN'
2023-01-08 13:43: Argument nan_fill: -1.0
2023-01-08 13:43: Argument num_layers: 1
2023-01-08 13:43: Argument num_nodes: 625
2023-01-08 13:43: Argument num_workers: 12
2023-01-08 13:43: Argument output_dim: 2
2023-01-08 13:43: Argument patch_height: 25
2023-01-08 13:43: Argument patch_width: 25
2023-01-08 13:43: Argument persistent_workers: True
2023-01-08 13:43: Argument pin_memory: True
2023-01-08 13:43: Argument plot: False
2023-01-08 13:43: Argument positive_weight: 0.5
2023-01-08 13:43: Argument prefetch_factor: 2
2023-01-08 13:43: Argument real_value: True
2023-01-08 13:43: Argument rnn_units: 48
2023-01-08 13:43: Argument scaleParameter: [0.1]
2023-01-08 13:43: Argument seed: 10000
2023-01-08 13:43: Argument sizeBorder: [6]
2023-01-08 13:43: Argument static_features: ['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density']
2023-01-08 13:43: Argument teacher_forcing: False
2023-01-08 13:43: Argument weight_decay: 0.0
2023-01-08 13:43: Argument window_len: 10
++++++++++++++
2020_fire_GCN.conf
++++++++++++++
*****************Model Parameter*****************
node_embeddings torch.Size([625, 64]) True
ln1.weight torch.Size([25]) True
ln1.bias torch.Size([25]) True
encoder.cell_list.0.gate.weights_pool torch.Size([64, 2, 73, 48]) True
encoder.cell_list.0.gate.weights_window torch.Size([64, 1, 48]) True
encoder.cell_list.0.gate.bias_pool torch.Size([64, 96]) True
encoder.cell_list.0.gate.T torch.Size([10]) True
encoder.cell_list.0.gate.ln1.weight torch.Size([48]) True
encoder.cell_list.0.gate.ln1.bias torch.Size([48]) True
encoder.cell_list.0.gate.ln2.weight torch.Size([48]) True
encoder.cell_list.0.gate.ln2.bias torch.Size([48]) True
encoder.cell_list.0.gate.cnn.features.0.weight torch.Size([24, 2, 3, 3]) True
encoder.cell_list.0.gate.cnn.features.0.bias torch.Size([24]) True
encoder.cell_list.0.gate.cnn.features.3.weight torch.Size([48, 24, 3, 3]) True
encoder.cell_list.0.gate.cnn.features.3.bias torch.Size([48]) True
encoder.cell_list.0.update.weights_pool torch.Size([64, 2, 73, 24]) True
encoder.cell_list.0.update.weights_window torch.Size([64, 1, 24]) True
encoder.cell_list.0.update.bias_pool torch.Size([64, 48]) True
encoder.cell_list.0.update.T torch.Size([10]) True
encoder.cell_list.0.update.ln1.weight torch.Size([24]) True
encoder.cell_list.0.update.ln1.bias torch.Size([24]) True
encoder.cell_list.0.update.ln2.weight torch.Size([24]) True
encoder.cell_list.0.update.ln2.bias torch.Size([24]) True
encoder.cell_list.0.update.cnn.features.0.weight torch.Size([12, 2, 3, 3]) True
encoder.cell_list.0.update.cnn.features.0.bias torch.Size([12]) True
encoder.cell_list.0.update.cnn.features.3.weight torch.Size([24, 12, 3, 3]) True
encoder.cell_list.0.update.cnn.features.3.bias torch.Size([24]) True
fc1.weight torch.Size([2, 30000]) True
fc1.bias torch.Size([2]) True
Total params num: 800668
*****************Finish Parameter****************
Positives: 13518 / Negatives: 27036
Dataset length 40554
Positives: 1300 / Negatives: 2600
Dataset length 3900
Positives: 1228 / Negatives: 2456
Dataset length 3684
Positives: 4407 / Negatives: 8814
Dataset length 13221
Applying learning rate decay.
Creat Log File in:  /home/joel.chacon/tmp/ZIGZAG_from_files/experiments/2020/2023010813432491171534736/run.log
2023-01-08 13:43: Train Epoch 1: 3/634 Loss: 0.439427
2023-01-08 13:44: Train Epoch 1: 7/634 Loss: 0.598704
2023-01-08 13:44: Train Epoch 1: 11/634 Loss: 0.299619
2023-01-08 13:44: Train Epoch 1: 15/634 Loss: 0.392315
2023-01-08 13:45: Train Epoch 1: 19/634 Loss: 0.399953
2023-01-08 13:45: Train Epoch 1: 23/634 Loss: 0.270581
2023-01-08 13:45: Train Epoch 1: 27/634 Loss: 0.295153
2023-01-08 13:45: Train Epoch 1: 31/634 Loss: 0.279928
2023-01-08 13:46: Train Epoch 1: 35/634 Loss: 0.281467
2023-01-08 13:46: Train Epoch 1: 39/634 Loss: 0.274928
2023-01-08 13:46: Train Epoch 1: 43/634 Loss: 0.246606
2023-01-08 13:47: Train Epoch 1: 47/634 Loss: 0.254289
2023-01-08 13:47: Train Epoch 1: 51/634 Loss: 0.252876
2023-01-08 13:47: Train Epoch 1: 55/634 Loss: 0.233610
2023-01-08 13:48: Train Epoch 1: 59/634 Loss: 0.265958
2023-01-08 13:48: Train Epoch 1: 63/634 Loss: 0.259732
2023-01-08 13:48: Train Epoch 1: 67/634 Loss: 0.245164
2023-01-08 13:49: Train Epoch 1: 71/634 Loss: 0.227949
2023-01-08 13:49: Train Epoch 1: 75/634 Loss: 0.235581
2023-01-08 13:49: Train Epoch 1: 79/634 Loss: 0.237827
2023-01-08 13:49: Train Epoch 1: 83/634 Loss: 0.253220
2023-01-08 13:50: Train Epoch 1: 87/634 Loss: 0.238955
2023-01-08 13:50: Train Epoch 1: 91/634 Loss: 0.219952
2023-01-08 13:50: Train Epoch 1: 95/634 Loss: 0.234001
2023-01-08 13:51: Train Epoch 1: 99/634 Loss: 0.250862
2023-01-08 13:51: Train Epoch 1: 103/634 Loss: 0.226370
2023-01-08 13:51: Train Epoch 1: 107/634 Loss: 0.243627
2023-01-08 13:52: Train Epoch 1: 111/634 Loss: 0.240357
2023-01-08 13:52: Train Epoch 1: 115/634 Loss: 0.225374
2023-01-08 13:52: Train Epoch 1: 119/634 Loss: 0.215117
2023-01-08 13:52: Train Epoch 1: 123/634 Loss: 0.235643
2023-01-08 13:53: Train Epoch 1: 127/634 Loss: 0.217122
2023-01-08 13:53: Train Epoch 1: 131/634 Loss: 0.210860
2023-01-08 13:53: Train Epoch 1: 135/634 Loss: 0.236261
2023-01-08 13:54: Train Epoch 1: 139/634 Loss: 0.206242
2023-01-08 13:54: Train Epoch 1: 143/634 Loss: 0.227499
2023-01-08 13:54: Train Epoch 1: 147/634 Loss: 0.254977
2023-01-08 13:54: Train Epoch 1: 151/634 Loss: 0.223821
2023-01-08 13:55: Train Epoch 1: 155/634 Loss: 0.214203
2023-01-08 13:55: Train Epoch 1: 159/634 Loss: 0.233942
2023-01-08 13:55: Train Epoch 1: 163/634 Loss: 0.239393
2023-01-08 13:56: Train Epoch 1: 167/634 Loss: 0.250415
2023-01-08 13:56: Train Epoch 1: 171/634 Loss: 0.233036
2023-01-08 13:56: Train Epoch 1: 175/634 Loss: 0.247554
2023-01-08 13:56: Train Epoch 1: 179/634 Loss: 0.260259
2023-01-08 13:57: Train Epoch 1: 183/634 Loss: 0.243675
2023-01-08 13:57: Train Epoch 1: 187/634 Loss: 0.215027
2023-01-08 13:57: Train Epoch 1: 191/634 Loss: 0.235980
2023-01-08 13:58: Train Epoch 1: 195/634 Loss: 0.251265
2023-01-08 13:58: Train Epoch 1: 199/634 Loss: 0.212660
2023-01-08 13:58: Train Epoch 1: 203/634 Loss: 0.204272
2023-01-08 13:59: Train Epoch 1: 207/634 Loss: 0.200688
2023-01-08 13:59: Train Epoch 1: 211/634 Loss: 0.240676
2023-01-08 13:59: Train Epoch 1: 215/634 Loss: 0.246448
2023-01-08 13:59: Train Epoch 1: 219/634 Loss: 0.237947
2023-01-08 14:00: Train Epoch 1: 223/634 Loss: 0.251032
2023-01-08 14:00: Train Epoch 1: 227/634 Loss: 0.219630
2023-01-08 14:00: Train Epoch 1: 231/634 Loss: 0.221869
2023-01-08 14:01: Train Epoch 1: 235/634 Loss: 0.210691
2023-01-08 14:01: Train Epoch 1: 239/634 Loss: 0.213509
2023-01-08 14:01: Train Epoch 1: 243/634 Loss: 0.235932
2023-01-08 14:02: Train Epoch 1: 247/634 Loss: 0.227258
2023-01-08 14:02: Train Epoch 1: 251/634 Loss: 0.226338
2023-01-08 14:02: Train Epoch 1: 255/634 Loss: 0.235688
2023-01-08 14:03: Train Epoch 1: 259/634 Loss: 0.221660
2023-01-08 14:03: Train Epoch 1: 263/634 Loss: 0.232420
2023-01-08 14:03: Train Epoch 1: 267/634 Loss: 0.220098
2023-01-08 14:04: Train Epoch 1: 271/634 Loss: 0.230796
2023-01-08 14:04: Train Epoch 1: 275/634 Loss: 0.268276
2023-01-08 14:04: Train Epoch 1: 279/634 Loss: 0.212516
2023-01-08 14:04: Train Epoch 1: 283/634 Loss: 0.213414
2023-01-08 14:05: Train Epoch 1: 287/634 Loss: 0.232751
2023-01-08 14:05: Train Epoch 1: 291/634 Loss: 0.198807
2023-01-08 14:05: Train Epoch 1: 295/634 Loss: 0.222105
2023-01-08 14:06: Train Epoch 1: 299/634 Loss: 0.248429
2023-01-08 14:06: Train Epoch 1: 303/634 Loss: 0.256415
2023-01-08 14:06: Train Epoch 1: 307/634 Loss: 0.211813
2023-01-08 14:07: Train Epoch 1: 311/634 Loss: 0.214223
2023-01-08 14:07: Train Epoch 1: 315/634 Loss: 0.224599
2023-01-08 14:07: Train Epoch 1: 319/634 Loss: 0.222033
2023-01-08 14:08: Train Epoch 1: 323/634 Loss: 0.222926
2023-01-08 14:08: Train Epoch 1: 327/634 Loss: 0.226248
2023-01-08 14:08: Train Epoch 1: 331/634 Loss: 0.233118
2023-01-08 14:08: Train Epoch 1: 335/634 Loss: 0.231496
2023-01-08 14:09: Train Epoch 1: 339/634 Loss: 0.222762
2023-01-08 14:09: Train Epoch 1: 343/634 Loss: 0.230509
2023-01-08 14:09: Train Epoch 1: 347/634 Loss: 0.209399
2023-01-08 14:10: Train Epoch 1: 351/634 Loss: 0.232111
2023-01-08 14:10: Train Epoch 1: 355/634 Loss: 0.213338
2023-01-08 14:10: Train Epoch 1: 359/634 Loss: 0.214432
2023-01-08 14:11: Train Epoch 1: 363/634 Loss: 0.216329
2023-01-08 14:11: Train Epoch 1: 367/634 Loss: 0.222018
2023-01-08 14:11: Train Epoch 1: 371/634 Loss: 0.214469
2023-01-08 14:11: Train Epoch 1: 375/634 Loss: 0.216633
2023-01-08 14:12: Train Epoch 1: 379/634 Loss: 0.209419
2023-01-08 14:12: Train Epoch 1: 383/634 Loss: 0.244245
2023-01-08 14:12: Train Epoch 1: 387/634 Loss: 0.230216
2023-01-08 14:13: Train Epoch 1: 391/634 Loss: 0.240820
2023-01-08 14:13: Train Epoch 1: 395/634 Loss: 0.229627
2023-01-08 14:13: Train Epoch 1: 399/634 Loss: 0.224559
2023-01-08 14:14: Train Epoch 1: 403/634 Loss: 0.225126
2023-01-08 14:14: Train Epoch 1: 407/634 Loss: 0.208694
2023-01-08 14:14: Train Epoch 1: 411/634 Loss: 0.224366
2023-01-08 14:15: Train Epoch 1: 415/634 Loss: 0.206078
2023-01-08 14:15: Train Epoch 1: 419/634 Loss: 0.197252
2023-01-08 14:15: Train Epoch 1: 423/634 Loss: 0.219497
2023-01-08 14:15: Train Epoch 1: 427/634 Loss: 0.205186
2023-01-08 14:16: Train Epoch 1: 431/634 Loss: 0.227839
2023-01-08 14:16: Train Epoch 1: 435/634 Loss: 0.200483
2023-01-08 14:16: Train Epoch 1: 439/634 Loss: 0.219657
2023-01-08 14:17: Train Epoch 1: 443/634 Loss: 0.210499
2023-01-08 14:17: Train Epoch 1: 447/634 Loss: 0.237590
2023-01-08 14:17: Train Epoch 1: 451/634 Loss: 0.200385
2023-01-08 14:18: Train Epoch 1: 455/634 Loss: 0.232327
2023-01-08 14:18: Train Epoch 1: 459/634 Loss: 0.210398
2023-01-08 14:18: Train Epoch 1: 463/634 Loss: 0.217983
2023-01-08 14:19: Train Epoch 1: 467/634 Loss: 0.228791
2023-01-08 14:19: Train Epoch 1: 471/634 Loss: 0.182304
2023-01-08 14:19: Train Epoch 1: 475/634 Loss: 0.206037
2023-01-08 14:20: Train Epoch 1: 479/634 Loss: 0.234975
2023-01-08 14:20: Train Epoch 1: 483/634 Loss: 0.226335
2023-01-08 14:20: Train Epoch 1: 487/634 Loss: 0.226650
2023-01-08 14:20: Train Epoch 1: 491/634 Loss: 0.226018
2023-01-08 14:21: Train Epoch 1: 495/634 Loss: 0.205786
2023-01-08 14:21: Train Epoch 1: 499/634 Loss: 0.202872
2023-01-08 14:21: Train Epoch 1: 503/634 Loss: 0.201284
2023-01-08 14:22: Train Epoch 1: 507/634 Loss: 0.221946
2023-01-08 14:22: Train Epoch 1: 511/634 Loss: 0.226678
2023-01-08 14:22: Train Epoch 1: 515/634 Loss: 0.237139
2023-01-08 14:23: Train Epoch 1: 519/634 Loss: 0.210815
2023-01-08 14:23: Train Epoch 1: 523/634 Loss: 0.197758
2023-01-08 14:23: Train Epoch 1: 527/634 Loss: 0.201677
2023-01-08 14:24: Train Epoch 1: 531/634 Loss: 0.218386
2023-01-08 14:24: Train Epoch 1: 535/634 Loss: 0.241471
2023-01-08 14:24: Train Epoch 1: 539/634 Loss: 0.191061
2023-01-08 14:25: Train Epoch 1: 543/634 Loss: 0.213442
2023-01-08 14:25: Train Epoch 1: 547/634 Loss: 0.218064
2023-01-08 14:25: Train Epoch 1: 551/634 Loss: 0.192021
2023-01-08 14:25: Train Epoch 1: 555/634 Loss: 0.272837
2023-01-08 14:26: Train Epoch 1: 559/634 Loss: 0.206417
2023-01-08 14:26: Train Epoch 1: 563/634 Loss: 0.275471
2023-01-08 14:26: Train Epoch 1: 567/634 Loss: 0.202799
2023-01-08 14:27: Train Epoch 1: 571/634 Loss: 0.235446
2023-01-08 14:27: Train Epoch 1: 575/634 Loss: 0.201973
2023-01-08 14:27: Train Epoch 1: 579/634 Loss: 0.204206
2023-01-08 14:28: Train Epoch 1: 583/634 Loss: 0.223978
2023-01-08 14:28: Train Epoch 1: 587/634 Loss: 0.257895
2023-01-08 14:28: Train Epoch 1: 591/634 Loss: 0.205047
2023-01-08 14:29: Train Epoch 1: 595/634 Loss: 0.233635
2023-01-08 14:29: Train Epoch 1: 599/634 Loss: 0.209541
2023-01-08 14:29: Train Epoch 1: 603/634 Loss: 0.230602
2023-01-08 14:29: Train Epoch 1: 607/634 Loss: 0.210489
2023-01-08 14:30: Train Epoch 1: 611/634 Loss: 0.215636
2023-01-08 14:30: Train Epoch 1: 615/634 Loss: 0.211667
2023-01-08 14:30: Train Epoch 1: 619/634 Loss: 0.219293
2023-01-08 14:31: Train Epoch 1: 623/634 Loss: 0.204954
2023-01-08 14:31: Train Epoch 1: 627/634 Loss: 0.177823
2023-01-08 14:31: Train Epoch 1: 631/634 Loss: 0.224688
2023-01-08 14:31: Train Epoch 1: 633/634 Loss: 0.090039
2023-01-08 14:31: **********Train Epoch 1: averaged Loss: 0.232363 
2023-01-08 14:31: 
Epoch time elapsed: 2911.4857892990112

2023-01-08 14:33: 
 metrics validation: {'precision': 0.7255985267034991, 'recall': 0.6061538461538462, 'f1-score': 0.6605196982397318, 'support': 1300, 'AUC': 0.8268347633136095, 'AUCPR': 0.7202717274836334, 'TP': 788, 'FP': 298, 'TN': 2302, 'FN': 512} 

2023-01-08 14:33: **********Val Epoch 1: average Loss: 0.226133
2023-01-08 14:33: *********************************Current best model saved!
2023-01-08 14:34: 
 Testing metrics {'precision': 0.7807835820895522, 'recall': 0.6815960912052117, 'f1-score': 0.7278260869565217, 'support': 1228, 'AUC': 0.8649720421436832, 'AUCPR': 0.7726974214870426, 'TP': 837, 'FP': 235, 'TN': 2221, 'FN': 391} 

2023-01-08 14:39: 
 Testing metrics {'precision': 0.8985645933014355, 'recall': 0.852280462899932, 'f1-score': 0.8748107604518458, 'support': 4407, 'AUC': 0.9641615395273595, 'AUCPR': 0.9351780842747726, 'TP': 3756, 'FP': 424, 'TN': 8390, 'FN': 651} 

2023-01-08 14:39: Train Epoch 2: 3/634 Loss: 0.216669
2023-01-08 14:40: Train Epoch 2: 7/634 Loss: 0.172889
2023-01-08 14:40: Train Epoch 2: 11/634 Loss: 0.189681
2023-01-08 14:40: Train Epoch 2: 15/634 Loss: 0.215963
2023-01-08 14:41: Train Epoch 2: 19/634 Loss: 0.233241
2023-01-08 14:41: Train Epoch 2: 23/634 Loss: 0.209874
2023-01-08 14:41: Train Epoch 2: 27/634 Loss: 0.211212
2023-01-08 14:42: Train Epoch 2: 31/634 Loss: 0.188513
2023-01-08 14:42: Train Epoch 2: 35/634 Loss: 0.189397
2023-01-08 14:42: Train Epoch 2: 39/634 Loss: 0.220184
2023-01-08 14:43: Train Epoch 2: 43/634 Loss: 0.210577
2023-01-08 14:43: Train Epoch 2: 47/634 Loss: 0.197254
2023-01-08 14:43: Train Epoch 2: 51/634 Loss: 0.227367
2023-01-08 14:43: Train Epoch 2: 55/634 Loss: 0.199797
2023-01-08 14:44: Train Epoch 2: 59/634 Loss: 0.216680
2023-01-08 14:44: Train Epoch 2: 63/634 Loss: 0.197415
2023-01-08 14:44: Train Epoch 2: 67/634 Loss: 0.192860
2023-01-08 14:45: Train Epoch 2: 71/634 Loss: 0.208401
2023-01-08 14:45: Train Epoch 2: 75/634 Loss: 0.223344
2023-01-08 14:45: Train Epoch 2: 79/634 Loss: 0.239688
2023-01-08 14:46: Train Epoch 2: 83/634 Loss: 0.218441
2023-01-08 14:46: Train Epoch 2: 87/634 Loss: 0.240568
2023-01-08 14:46: Train Epoch 2: 91/634 Loss: 0.216489
2023-01-08 14:47: Train Epoch 2: 95/634 Loss: 0.180108
2023-01-08 14:47: Train Epoch 2: 99/634 Loss: 0.195176
2023-01-08 14:47: Train Epoch 2: 103/634 Loss: 0.207320
2023-01-08 14:48: Train Epoch 2: 107/634 Loss: 0.212479
2023-01-08 14:48: Train Epoch 2: 111/634 Loss: 0.199006
2023-01-08 14:48: Train Epoch 2: 115/634 Loss: 0.198638
2023-01-08 14:48: Train Epoch 2: 119/634 Loss: 0.231475
2023-01-08 14:49: Train Epoch 2: 123/634 Loss: 0.212016
2023-01-08 14:49: Train Epoch 2: 127/634 Loss: 0.197904
2023-01-08 14:49: Train Epoch 2: 131/634 Loss: 0.196182
2023-01-08 14:50: Train Epoch 2: 135/634 Loss: 0.211101
2023-01-08 14:50: Train Epoch 2: 139/634 Loss: 0.228354
2023-01-08 14:50: Train Epoch 2: 143/634 Loss: 0.211301
2023-01-08 14:51: Train Epoch 2: 147/634 Loss: 0.230492
2023-01-08 14:51: Train Epoch 2: 151/634 Loss: 0.174002
2023-01-08 14:51: Train Epoch 2: 155/634 Loss: 0.197134
2023-01-08 14:52: Train Epoch 2: 159/634 Loss: 0.211952
2023-01-08 14:52: Train Epoch 2: 163/634 Loss: 0.206624
2023-01-08 14:52: Train Epoch 2: 167/634 Loss: 0.217155
2023-01-08 14:53: Train Epoch 2: 171/634 Loss: 0.236072
2023-01-08 14:53: Train Epoch 2: 175/634 Loss: 0.239330
2023-01-08 14:53: Train Epoch 2: 179/634 Loss: 0.241644
2023-01-08 14:54: Train Epoch 2: 183/634 Loss: 0.214960
2023-01-08 14:54: Train Epoch 2: 187/634 Loss: 0.237459
2023-01-08 14:54: Train Epoch 2: 191/634 Loss: 0.219785
2023-01-08 14:54: Train Epoch 2: 195/634 Loss: 0.259945
2023-01-08 14:55: Train Epoch 2: 199/634 Loss: 0.212053
2023-01-08 14:55: Train Epoch 2: 203/634 Loss: 0.224447
2023-01-08 14:55: Train Epoch 2: 207/634 Loss: 0.220617
2023-01-08 14:56: Train Epoch 2: 211/634 Loss: 0.219948
2023-01-08 14:56: Train Epoch 2: 215/634 Loss: 0.219593
2023-01-08 14:56: Train Epoch 2: 219/634 Loss: 0.212242
2023-01-08 14:57: Train Epoch 2: 223/634 Loss: 0.233956
2023-01-08 14:57: Train Epoch 2: 227/634 Loss: 0.209288
2023-01-08 14:57: Train Epoch 2: 231/634 Loss: 0.204500
2023-01-08 14:57: Train Epoch 2: 235/634 Loss: 0.226948
2023-01-08 14:58: Train Epoch 2: 239/634 Loss: 0.228730
2023-01-08 14:58: Train Epoch 2: 243/634 Loss: 0.211811
2023-01-08 14:58: Train Epoch 2: 247/634 Loss: 0.203893
2023-01-08 14:59: Train Epoch 2: 251/634 Loss: 0.210368
2023-01-08 14:59: Train Epoch 2: 255/634 Loss: 0.216265
2023-01-08 14:59: Train Epoch 2: 259/634 Loss: 0.217128
2023-01-08 15:00: Train Epoch 2: 263/634 Loss: 0.195412
2023-01-08 15:00: Train Epoch 2: 267/634 Loss: 0.211244
2023-01-08 15:00: Train Epoch 2: 271/634 Loss: 0.217111
2023-01-08 15:01: Train Epoch 2: 275/634 Loss: 0.211075
2023-01-08 15:01: Train Epoch 2: 279/634 Loss: 0.201436
2023-01-08 15:01: Train Epoch 2: 283/634 Loss: 0.232367
2023-01-08 15:01: Train Epoch 2: 287/634 Loss: 0.218279
2023-01-08 15:02: Train Epoch 2: 291/634 Loss: 0.243008
2023-01-08 15:02: Train Epoch 2: 295/634 Loss: 0.222261
2023-01-08 15:02: Train Epoch 2: 299/634 Loss: 0.213792
2023-01-08 15:03: Train Epoch 2: 303/634 Loss: 0.214607
2023-01-08 15:03: Train Epoch 2: 307/634 Loss: 0.205119
2023-01-08 15:03: Train Epoch 2: 311/634 Loss: 0.222063
2023-01-08 15:04: Train Epoch 2: 315/634 Loss: 0.198815
2023-01-08 15:04: Train Epoch 2: 319/634 Loss: 0.225596
2023-01-08 15:04: Train Epoch 2: 323/634 Loss: 0.225401
2023-01-08 15:05: Train Epoch 2: 327/634 Loss: 0.223259
2023-01-08 15:05: Train Epoch 2: 331/634 Loss: 0.190476
2023-01-08 15:05: Train Epoch 2: 335/634 Loss: 0.218749
2023-01-08 15:05: Train Epoch 2: 339/634 Loss: 0.209542
2023-01-08 15:06: Train Epoch 2: 343/634 Loss: 0.210579
2023-01-08 15:06: Train Epoch 2: 347/634 Loss: 0.205564
2023-01-08 15:06: Train Epoch 2: 351/634 Loss: 0.205175
2023-01-08 15:07: Train Epoch 2: 355/634 Loss: 0.207378
2023-01-08 15:07: Train Epoch 2: 359/634 Loss: 0.246907
2023-01-08 15:07: Train Epoch 2: 363/634 Loss: 0.201170
2023-01-08 15:08: Train Epoch 2: 367/634 Loss: 0.194786
2023-01-08 15:08: Train Epoch 2: 371/634 Loss: 0.190490
2023-01-08 15:08: Train Epoch 2: 375/634 Loss: 0.197378
2023-01-08 15:09: Train Epoch 2: 379/634 Loss: 0.213494
2023-01-08 15:09: Train Epoch 2: 383/634 Loss: 0.214504
2023-01-08 15:09: Train Epoch 2: 387/634 Loss: 0.193424
2023-01-08 15:10: Train Epoch 2: 391/634 Loss: 0.228134
2023-01-08 15:10: Train Epoch 2: 395/634 Loss: 0.202926
2023-01-08 15:10: Train Epoch 2: 399/634 Loss: 0.227517
2023-01-08 15:11: Train Epoch 2: 403/634 Loss: 0.208602
2023-01-08 15:11: Train Epoch 2: 407/634 Loss: 0.208069
2023-01-08 15:11: Train Epoch 2: 411/634 Loss: 0.191235
2023-01-08 15:11: Train Epoch 2: 415/634 Loss: 0.190447
2023-01-08 15:12: Train Epoch 2: 419/634 Loss: 0.207538
2023-01-08 15:12: Train Epoch 2: 423/634 Loss: 0.222281
2023-01-08 15:12: Train Epoch 2: 427/634 Loss: 0.181479
2023-01-08 15:13: Train Epoch 2: 431/634 Loss: 0.198934
2023-01-08 15:13: Train Epoch 2: 435/634 Loss: 0.221409
2023-01-08 15:13: Train Epoch 2: 439/634 Loss: 0.237329
2023-01-08 15:14: Train Epoch 2: 443/634 Loss: 0.198553
2023-01-08 15:14: Train Epoch 2: 447/634 Loss: 0.206891
2023-01-08 15:14: Train Epoch 2: 451/634 Loss: 0.209799
2023-01-08 15:15: Train Epoch 2: 455/634 Loss: 0.238585
2023-01-08 15:15: Train Epoch 2: 459/634 Loss: 0.223316
2023-01-08 15:15: Train Epoch 2: 463/634 Loss: 0.213401
2023-01-08 15:15: Train Epoch 2: 467/634 Loss: 0.194775
2023-01-08 15:16: Train Epoch 2: 471/634 Loss: 0.211971
