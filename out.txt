2023-01-05 20:16: log dir: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2023010520162439825840981
2023-01-05 20:16: Experiment log path in: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2023010520162439825840981
2023-01-05 20:16: Argument: Namespace(batch_size=256, clc='vec', cuda=True, dataset='2020', dataset_root='/home/joel.chacon/tmp/datasets_grl/', debug=False, default_graph=True, device='cpu', dynamic_features=['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh'], early_stop=True, early_stop_patience=8, embed_dim=64, epochs=30, grad_norm=False, horizon=1, input_dim=25, lag=10, link_len=2, log_dir='/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2023010520162439825840981', log_step=1, loss_func='nllloss', lr_decay=True, lr_decay_rate=0.1, lr_decay_step='20', lr_init=0.0001, max_grad_norm=5, minbatch_size=64, mode='train', model='fire_GCN', nan_fill=-1.0, num_layers=1, num_nodes=625, num_workers=12, output_dim=2, patch_height=25, patch_width=25, persistent_workers=True, pin_memory=True, plot=False, positive_weight=0.5, prefetch_factor=2, real_value=True, rnn_units=64, seed=10000, static_features=['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density'], teacher_forcing=False, weight_decay=0.0, window_len=10)
2023-01-05 20:16: Argument batch_size: 256
2023-01-05 20:16: Argument clc: 'vec'
2023-01-05 20:16: Argument cuda: True
2023-01-05 20:16: Argument dataset: '2020'
2023-01-05 20:16: Argument dataset_root: '/home/joel.chacon/tmp/datasets_grl/'
2023-01-05 20:16: Argument debug: False
2023-01-05 20:16: Argument default_graph: True
2023-01-05 20:16: Argument device: 'cpu'
2023-01-05 20:16: Argument dynamic_features: ['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh']
2023-01-05 20:16: Argument early_stop: True
2023-01-05 20:16: Argument early_stop_patience: 8
2023-01-05 20:16: Argument embed_dim: 64
2023-01-05 20:16: Argument epochs: 30
2023-01-05 20:16: Argument grad_norm: False
2023-01-05 20:16: Argument horizon: 1
2023-01-05 20:16: Argument input_dim: 25
2023-01-05 20:16: Argument lag: 10
2023-01-05 20:16: Argument link_len: 2
2023-01-05 20:16: Argument log_dir: '/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2023010520162439825840981'
2023-01-05 20:16: Argument log_step: 1
2023-01-05 20:16: Argument loss_func: 'nllloss'
2023-01-05 20:16: Argument lr_decay: True
2023-01-05 20:16: Argument lr_decay_rate: 0.1
2023-01-05 20:16: Argument lr_decay_step: '20'
2023-01-05 20:16: Argument lr_init: 0.0001
2023-01-05 20:16: Argument max_grad_norm: 5
2023-01-05 20:16: Argument minbatch_size: 64
2023-01-05 20:16: Argument mode: 'train'
2023-01-05 20:16: Argument model: 'fire_GCN'
2023-01-05 20:16: Argument nan_fill: -1.0
2023-01-05 20:16: Argument num_layers: 1
2023-01-05 20:16: Argument num_nodes: 625
2023-01-05 20:16: Argument num_workers: 12
2023-01-05 20:16: Argument output_dim: 2
2023-01-05 20:16: Argument patch_height: 25
2023-01-05 20:16: Argument patch_width: 25
2023-01-05 20:16: Argument persistent_workers: True
2023-01-05 20:16: Argument pin_memory: True
2023-01-05 20:16: Argument plot: False
2023-01-05 20:16: Argument positive_weight: 0.5
2023-01-05 20:16: Argument prefetch_factor: 2
2023-01-05 20:16: Argument real_value: True
2023-01-05 20:16: Argument rnn_units: 64
2023-01-05 20:16: Argument seed: 10000
2023-01-05 20:16: Argument static_features: ['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density']
2023-01-05 20:16: Argument teacher_forcing: False
2023-01-05 20:16: Argument weight_decay: 0.0
2023-01-05 20:16: Argument window_len: 10
++++++++++++++
2020_fire_GCN.conf
++++++++++++++
*****************Model Parameter*****************
node_embeddings torch.Size([625, 64]) True
ln1.weight torch.Size([25]) True
ln1.bias torch.Size([25]) True
encoder.cell_list.0.gate.weights_pool torch.Size([64, 2, 89, 64]) True
encoder.cell_list.0.gate.weights_window torch.Size([64, 1, 64]) True
encoder.cell_list.0.gate.bias_pool torch.Size([64, 128]) True
encoder.cell_list.0.gate.T torch.Size([10]) True
encoder.cell_list.0.update.weights_pool torch.Size([64, 2, 89, 32]) True
encoder.cell_list.0.update.weights_window torch.Size([64, 1, 32]) True
encoder.cell_list.0.update.bias_pool torch.Size([64, 64]) True
encoder.cell_list.0.update.T torch.Size([10]) True
fc1.weight torch.Size([2, 40000]) True
fc1.bias torch.Size([2]) True
Total params num: 1232136
*****************Finish Parameter****************
Positives: 1228 / Negatives: 2456
Dataset length 3684
Positives: 1228 / Negatives: 2456
Dataset length 3684
Positives: 1228 / Negatives: 2456
Dataset length 3684
Positives: 1228 / Negatives: 2456
Dataset length 3684
Applying learning rate decay.
Creat Log File in:  /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2023010520162439825840981/run.log
2023-01-05 20:16: Train Epoch 1: 3/58 Loss: 0.362633
2023-01-05 20:16: Train Epoch 1: 7/58 Loss: 0.520570
2023-01-05 20:17: Train Epoch 1: 11/58 Loss: 0.351228
2023-01-05 20:17: Train Epoch 1: 15/58 Loss: 0.376915
2023-01-05 20:17: Train Epoch 1: 19/58 Loss: 0.357718
2023-01-05 20:17: Train Epoch 1: 23/58 Loss: 0.289518
2023-01-05 20:18: Train Epoch 1: 27/58 Loss: 0.268786
2023-01-05 20:18: Train Epoch 1: 31/58 Loss: 0.327956
2023-01-05 20:18: Train Epoch 1: 35/58 Loss: 0.314009
2023-01-05 20:18: Train Epoch 1: 39/58 Loss: 0.294923
2023-01-05 20:19: Train Epoch 1: 43/58 Loss: 0.246945
2023-01-05 20:19: Train Epoch 1: 47/58 Loss: 0.277035
2023-01-05 20:19: Train Epoch 1: 51/58 Loss: 0.286853
2023-01-05 20:19: Train Epoch 1: 55/58 Loss: 0.272165
2023-01-05 20:19: Train Epoch 1: 57/58 Loss: 0.100589
2023-01-05 20:19: **********Train Epoch 1: averaged Loss: 0.309856 
2023-01-05 20:19: 
Epoch time elapsed: 205.3741729259491

2023-01-05 20:20: 
 metrics validation: {'precision': 0.7723919915700738, 'recall': 0.5969055374592834, 'f1-score': 0.673403766651355, 'support': 1228, 'AUC': 0.8579381478848582, 'AUCPR': 0.7747993607934797, 'TP': 733, 'FP': 216, 'TN': 2240, 'FN': 495} 

2023-01-05 20:20: **********Val Epoch 1: average Loss: 0.215758
2023-01-05 20:20: *********************************Current best model saved!
2023-01-05 20:21: 
 Testing metrics {'precision': 0.7748414376321353, 'recall': 0.5969055374592834, 'f1-score': 0.6743330266789329, 'support': 1228, 'AUC': 0.8595018249530499, 'AUCPR': 0.7756558585202823, 'TP': 733, 'FP': 213, 'TN': 2243, 'FN': 495} 

2023-01-05 20:23: 
 Testing metrics {'precision': 0.7831196581196581, 'recall': 0.5969055374592834, 'f1-score': 0.677449168207024, 'support': 1228, 'AUC': 0.8619889203068466, 'AUCPR': 0.7797338734054126, 'TP': 733, 'FP': 203, 'TN': 2253, 'FN': 495} 

2023-01-05 20:23: Train Epoch 2: 3/58 Loss: 0.297215
2023-01-05 20:23: Train Epoch 2: 7/58 Loss: 0.258700
2023-01-05 20:23: Train Epoch 2: 11/58 Loss: 0.252180
2023-01-05 20:24: Train Epoch 2: 15/58 Loss: 0.252361
2023-01-05 20:24: Train Epoch 2: 19/58 Loss: 0.235230
2023-01-05 20:24: Train Epoch 2: 23/58 Loss: 0.252059
2023-01-05 20:24: Train Epoch 2: 27/58 Loss: 0.276763
2023-01-05 20:25: Train Epoch 2: 31/58 Loss: 0.238503
2023-01-05 20:25: Train Epoch 2: 35/58 Loss: 0.258129
2023-01-05 20:25: Train Epoch 2: 39/58 Loss: 0.228721
2023-01-05 20:25: Train Epoch 2: 43/58 Loss: 0.243330
2023-01-05 20:26: Train Epoch 2: 47/58 Loss: 0.283368
2023-01-05 20:26: Train Epoch 2: 51/58 Loss: 0.261964
2023-01-05 20:26: Train Epoch 2: 55/58 Loss: 0.222841
2023-01-05 20:26: Train Epoch 2: 57/58 Loss: 0.077538
2023-01-05 20:26: **********Train Epoch 2: averaged Loss: 0.242593 
2023-01-05 20:26: 
Epoch time elapsed: 228.6943759918213

2023-01-05 20:27: 
 metrics validation: {'precision': 0.793939393939394, 'recall': 0.746742671009772, 'f1-score': 0.7696181284095679, 'support': 1228, 'AUC': 0.8776223753037168, 'AUCPR': 0.8055682925195309, 'TP': 917, 'FP': 238, 'TN': 2218, 'FN': 311} 

2023-01-05 20:27: **********Val Epoch 2: average Loss: 0.202662
2023-01-05 20:27: *********************************Current best model saved!
2023-01-05 20:28: 
 Testing metrics {'precision': 0.7960069444444444, 'recall': 0.746742671009772, 'f1-score': 0.7705882352941177, 'support': 1228, 'AUC': 0.8780351780920753, 'AUCPR': 0.8031479082206996, 'TP': 917, 'FP': 235, 'TN': 2221, 'FN': 311} 

2023-01-05 20:29: 
 Testing metrics {'precision': 0.7973913043478261, 'recall': 0.746742671009772, 'f1-score': 0.7712363330529856, 'support': 1228, 'AUC': 0.8775786082610956, 'AUCPR': 0.8008302986300123, 'TP': 917, 'FP': 233, 'TN': 2223, 'FN': 311} 

2023-01-05 20:30: Train Epoch 3: 3/58 Loss: 0.233006
2023-01-05 20:30: Train Epoch 3: 7/58 Loss: 0.257771
2023-01-05 20:30: Train Epoch 3: 11/58 Loss: 0.245948
2023-01-05 20:30: Train Epoch 3: 15/58 Loss: 0.247057
2023-01-05 20:31: Train Epoch 3: 19/58 Loss: 0.255212
2023-01-05 20:31: Train Epoch 3: 23/58 Loss: 0.236054
2023-01-05 20:31: Train Epoch 3: 27/58 Loss: 0.251040
2023-01-05 20:31: Train Epoch 3: 31/58 Loss: 0.251035
2023-01-05 20:32: Train Epoch 3: 35/58 Loss: 0.224873
2023-01-05 20:32: Train Epoch 3: 39/58 Loss: 0.264826
2023-01-05 20:32: Train Epoch 3: 43/58 Loss: 0.238928
2023-01-05 20:32: Train Epoch 3: 47/58 Loss: 0.235248
2023-01-05 20:33: Train Epoch 3: 51/58 Loss: 0.256497
2023-01-05 20:33: Train Epoch 3: 55/58 Loss: 0.233455
2023-01-05 20:33: Train Epoch 3: 57/58 Loss: 0.101281
2023-01-05 20:33: **********Train Epoch 3: averaged Loss: 0.235482 
2023-01-05 20:33: 
Epoch time elapsed: 226.12189936637878

2023-01-05 20:34: 
 metrics validation: {'precision': 0.7375, 'recall': 0.8167752442996743, 'f1-score': 0.7751159196290571, 'support': 1228, 'AUC': 0.887691447654617, 'AUCPR': 0.8152722544887111, 'TP': 1003, 'FP': 357, 'TN': 2099, 'FN': 225} 

2023-01-05 20:34: **********Val Epoch 3: average Loss: 0.196455
2023-01-05 20:34: *********************************Current best model saved!
2023-01-05 20:35: 
 Testing metrics {'precision': 0.75018698578908, 'recall': 0.8167752442996743, 'f1-score': 0.7820662768031189, 'support': 1228, 'AUC': 0.8882706978323378, 'AUCPR': 0.8135392314723024, 'TP': 1003, 'FP': 334, 'TN': 2122, 'FN': 225} 

2023-01-05 20:36: 
 Testing metrics {'precision': 0.7479492915734527, 'recall': 0.8167752442996743, 'f1-score': 0.7808485792137018, 'support': 1228, 'AUC': 0.8876808374624665, 'AUCPR': 0.8102831904912227, 'TP': 1003, 'FP': 338, 'TN': 2118, 'FN': 225} 

2023-01-05 20:36: Train Epoch 4: 3/58 Loss: 0.243714
2023-01-05 20:37: Train Epoch 4: 7/58 Loss: 0.257723
2023-01-05 20:37: Train Epoch 4: 11/58 Loss: 0.258904
2023-01-05 20:37: Train Epoch 4: 15/58 Loss: 0.254025
2023-01-05 20:37: Train Epoch 4: 19/58 Loss: 0.235966
2023-01-05 20:38: Train Epoch 4: 23/58 Loss: 0.226402
2023-01-05 20:38: Train Epoch 4: 27/58 Loss: 0.238653
2023-01-05 20:38: Train Epoch 4: 31/58 Loss: 0.214887
2023-01-05 20:39: Train Epoch 4: 35/58 Loss: 0.250915
2023-01-05 20:39: Train Epoch 4: 39/58 Loss: 0.233892
2023-01-05 20:39: Train Epoch 4: 43/58 Loss: 0.228956
2023-01-05 20:39: Train Epoch 4: 47/58 Loss: 0.228914
2023-01-05 20:40: Train Epoch 4: 51/58 Loss: 0.252731
2023-01-05 20:40: Train Epoch 4: 55/58 Loss: 0.236613
2023-01-05 20:40: Train Epoch 4: 57/58 Loss: 0.081233
2023-01-05 20:40: **********Train Epoch 4: averaged Loss: 0.229569 
2023-01-05 20:40: 
Epoch time elapsed: 229.04341983795166

2023-01-05 20:41: 
 metrics validation: {'precision': 0.8701492537313433, 'recall': 0.4747557003257329, 'f1-score': 0.6143308746048473, 'support': 1228, 'AUC': 0.8988593380301119, 'AUCPR': 0.8239900636284514, 'TP': 583, 'FP': 87, 'TN': 2369, 'FN': 645} 

2023-01-05 20:41: **********Val Epoch 4: average Loss: 0.197693
2023-01-05 20:42: 
 Testing metrics {'precision': 0.75018698578908, 'recall': 0.8167752442996743, 'f1-score': 0.7820662768031189, 'support': 1228, 'AUC': 0.8882706978323378, 'AUCPR': 0.8135392314723024, 'TP': 1003, 'FP': 334, 'TN': 2122, 'FN': 225} 

2023-01-05 20:43: 
 Testing metrics {'precision': 0.7479492915734527, 'recall': 0.8167752442996743, 'f1-score': 0.7808485792137018, 'support': 1228, 'AUC': 0.8876808374624665, 'AUCPR': 0.8102831904912227, 'TP': 1003, 'FP': 338, 'TN': 2118, 'FN': 225} 

2023-01-05 20:43: Train Epoch 5: 3/58 Loss: 0.219769
2023-01-05 20:43: Train Epoch 5: 7/58 Loss: 0.247094
2023-01-05 20:44: Train Epoch 5: 11/58 Loss: 0.206390
2023-01-05 20:44: Train Epoch 5: 15/58 Loss: 0.238000
2023-01-05 20:44: Train Epoch 5: 19/58 Loss: 0.235598
2023-01-05 20:44: Train Epoch 5: 23/58 Loss: 0.253493
2023-01-05 20:45: Train Epoch 5: 27/58 Loss: 0.255643
2023-01-05 20:45: Train Epoch 5: 31/58 Loss: 0.257832
2023-01-05 20:45: Train Epoch 5: 35/58 Loss: 0.242820
2023-01-05 20:45: Train Epoch 5: 39/58 Loss: 0.230961
2023-01-05 20:46: Train Epoch 5: 43/58 Loss: 0.250751
2023-01-05 20:46: Train Epoch 5: 47/58 Loss: 0.254369
2023-01-05 20:46: Train Epoch 5: 51/58 Loss: 0.234057
2023-01-05 20:47: Train Epoch 5: 55/58 Loss: 0.258542
2023-01-05 20:47: Train Epoch 5: 57/58 Loss: 0.082261
2023-01-05 20:47: **********Train Epoch 5: averaged Loss: 0.231172 
2023-01-05 20:47: 
Epoch time elapsed: 223.92473006248474

2023-01-05 20:48: 
 metrics validation: {'precision': 0.7317425885755604, 'recall': 0.8241042345276873, 'f1-score': 0.7751819226350058, 'support': 1228, 'AUC': 0.8983023029422064, 'AUCPR': 0.8248278776188939, 'TP': 1012, 'FP': 371, 'TN': 2085, 'FN': 216} 

2023-01-05 20:48: **********Val Epoch 5: average Loss: 0.192771
2023-01-05 20:48: *********************************Current best model saved!
2023-01-05 20:49: 
 Testing metrics {'precision': 0.7413919413919414, 'recall': 0.8241042345276873, 'f1-score': 0.7805630543771694, 'support': 1228, 'AUC': 0.8994598085921337, 'AUCPR': 0.8250056886210332, 'TP': 1012, 'FP': 353, 'TN': 2103, 'FN': 216} 

2023-01-05 20:50: 
 Testing metrics {'precision': 0.7424798239178283, 'recall': 0.8241042345276873, 'f1-score': 0.7811655731377846, 'support': 1228, 'AUC': 0.8979505087587136, 'AUCPR': 0.8193335577797898, 'TP': 1012, 'FP': 351, 'TN': 2105, 'FN': 216} 

2023-01-05 20:50: Train Epoch 6: 3/58 Loss: 0.238522
2023-01-05 20:50: Train Epoch 6: 7/58 Loss: 0.221100
2023-01-05 20:50: Train Epoch 6: 11/58 Loss: 0.218230
2023-01-05 20:51: Train Epoch 6: 15/58 Loss: 0.248558
2023-01-05 20:51: Train Epoch 6: 19/58 Loss: 0.241303
2023-01-05 20:51: Train Epoch 6: 23/58 Loss: 0.215923
2023-01-05 20:51: Train Epoch 6: 27/58 Loss: 0.218831
2023-01-05 20:52: Train Epoch 6: 31/58 Loss: 0.231805
2023-01-05 20:52: Train Epoch 6: 35/58 Loss: 0.246362
2023-01-05 20:52: Train Epoch 6: 39/58 Loss: 0.234740
2023-01-05 20:52: Train Epoch 6: 43/58 Loss: 0.238198
2023-01-05 20:53: Train Epoch 6: 47/58 Loss: 0.271534
2023-01-05 20:53: Train Epoch 6: 51/58 Loss: 0.232388
2023-01-05 20:53: Train Epoch 6: 55/58 Loss: 0.239332
2023-01-05 20:53: Train Epoch 6: 57/58 Loss: 0.085565
2023-01-05 20:53: **********Train Epoch 6: averaged Loss: 0.225493 
2023-01-05 20:53: 
Epoch time elapsed: 225.52668261528015

2023-01-05 20:54: 
 metrics validation: {'precision': 0.8691232528589581, 'recall': 0.5570032573289903, 'f1-score': 0.678908188585608, 'support': 1228, 'AUC': 0.9103004408534838, 'AUCPR': 0.8342917504561647, 'TP': 684, 'FP': 103, 'TN': 2353, 'FN': 544} 

2023-01-05 20:54: **********Val Epoch 6: average Loss: 0.188210
2023-01-05 20:54: *********************************Current best model saved!
2023-01-05 20:55: 
 Testing metrics {'precision': 0.868020304568528, 'recall': 0.5570032573289903, 'f1-score': 0.6785714285714286, 'support': 1228, 'AUC': 0.9119688935691626, 'AUCPR': 0.8357683067478734, 'TP': 684, 'FP': 104, 'TN': 2352, 'FN': 544} 

2023-01-05 20:56: 
 Testing metrics {'precision': 0.8507462686567164, 'recall': 0.5570032573289903, 'f1-score': 0.6732283464566929, 'support': 1228, 'AUC': 0.9095036817366762, 'AUCPR': 0.8280183926154222, 'TP': 684, 'FP': 120, 'TN': 2336, 'FN': 544} 

2023-01-05 20:57: Train Epoch 7: 3/58 Loss: 0.235482
2023-01-05 20:57: Train Epoch 7: 7/58 Loss: 0.201295
2023-01-05 20:57: Train Epoch 7: 11/58 Loss: 0.258739
2023-01-05 20:57: Train Epoch 7: 15/58 Loss: 0.257549
2023-01-05 20:58: Train Epoch 7: 19/58 Loss: 0.276431
2023-01-05 20:58: Train Epoch 7: 23/58 Loss: 0.224642
2023-01-05 20:58: Train Epoch 7: 27/58 Loss: 0.256015
2023-01-05 20:58: Train Epoch 7: 31/58 Loss: 0.248559
2023-01-05 20:59: Train Epoch 7: 35/58 Loss: 0.227471
2023-01-05 20:59: Train Epoch 7: 39/58 Loss: 0.229776
2023-01-05 20:59: Train Epoch 7: 43/58 Loss: 0.232350
2023-01-05 20:59: Train Epoch 7: 47/58 Loss: 0.258731
2023-01-05 21:00: Train Epoch 7: 51/58 Loss: 0.212765
2023-01-05 21:00: Train Epoch 7: 55/58 Loss: 0.220242
2023-01-05 21:00: Train Epoch 7: 57/58 Loss: 0.079817
2023-01-05 21:00: **********Train Epoch 7: averaged Loss: 0.227991 
2023-01-05 21:00: 
Epoch time elapsed: 224.0544776916504

2023-01-05 21:01: 
 metrics validation: {'precision': 0.8625541125541125, 'recall': 0.6490228013029316, 'f1-score': 0.7407063197026021, 'support': 1228, 'AUC': 0.9174205429235324, 'AUCPR': 0.8443700818562326, 'TP': 797, 'FP': 127, 'TN': 2329, 'FN': 431} 

2023-01-05 21:01: **********Val Epoch 7: average Loss: 0.177725
2023-01-05 21:01: *********************************Current best model saved!
2023-01-05 21:02: 
 Testing metrics {'precision': 0.8597626752966558, 'recall': 0.6490228013029316, 'f1-score': 0.739675174013921, 'support': 1228, 'AUC': 0.9184533788156906, 'AUCPR': 0.843250857403047, 'TP': 797, 'FP': 130, 'TN': 2326, 'FN': 431} 

2023-01-05 21:03: 
 Testing metrics {'precision': 0.8533190578158458, 'recall': 0.6490228013029316, 'f1-score': 0.7372802960222017, 'support': 1228, 'AUC': 0.9168449399993632, 'AUCPR': 0.8382170505205425, 'TP': 797, 'FP': 137, 'TN': 2319, 'FN': 431} 

2023-01-05 21:03: Train Epoch 8: 3/58 Loss: 0.212960
2023-01-05 21:04: Train Epoch 8: 7/58 Loss: 0.215752
2023-01-05 21:04: Train Epoch 8: 11/58 Loss: 0.221997
2023-01-05 21:04: Train Epoch 8: 15/58 Loss: 0.224666
2023-01-05 21:04: Train Epoch 8: 19/58 Loss: 0.248232
2023-01-05 21:05: Train Epoch 8: 23/58 Loss: 0.232359
2023-01-05 21:05: Train Epoch 8: 27/58 Loss: 0.241975
2023-01-05 21:05: Train Epoch 8: 31/58 Loss: 0.210697
2023-01-05 21:05: Train Epoch 8: 35/58 Loss: 0.208588
2023-01-05 21:06: Train Epoch 8: 39/58 Loss: 0.207885
2023-01-05 21:06: Train Epoch 8: 43/58 Loss: 0.225093
2023-01-05 21:06: Train Epoch 8: 47/58 Loss: 0.230211
2023-01-05 21:07: Train Epoch 8: 51/58 Loss: 0.215074
2023-01-05 21:07: Train Epoch 8: 55/58 Loss: 0.234987
2023-01-05 21:07: Train Epoch 8: 57/58 Loss: 0.090014
2023-01-05 21:07: **********Train Epoch 8: averaged Loss: 0.214699 
2023-01-05 21:07: 
Epoch time elapsed: 226.96421122550964

2023-01-05 21:08: 
 metrics validation: {'precision': 0.84375, 'recall': 0.7255700325732899, 'f1-score': 0.7802101576182137, 'support': 1228, 'AUC': 0.9242677641142082, 'AUCPR': 0.8546121132100073, 'TP': 891, 'FP': 165, 'TN': 2291, 'FN': 337} 

2023-01-05 21:08: **********Val Epoch 8: average Loss: 0.168360
2023-01-05 21:08: *********************************Current best model saved!
2023-01-05 21:09: 
 Testing metrics {'precision': 0.8510028653295129, 'recall': 0.7255700325732899, 'f1-score': 0.7832967032967032, 'support': 1228, 'AUC': 0.9264319117444216, 'AUCPR': 0.8572820602601926, 'TP': 891, 'FP': 156, 'TN': 2300, 'FN': 337} 

2023-01-05 21:10: 
 Testing metrics {'precision': 0.8374060150375939, 'recall': 0.7255700325732899, 'f1-score': 0.7774869109947644, 'support': 1228, 'AUC': 0.9237200129444345, 'AUCPR': 0.8488312139130805, 'TP': 891, 'FP': 173, 'TN': 2283, 'FN': 337} 

2023-01-05 21:10: Train Epoch 9: 3/58 Loss: 0.222369
2023-01-05 21:10: Train Epoch 9: 7/58 Loss: 0.222126
2023-01-05 21:11: Train Epoch 9: 11/58 Loss: 0.203384
2023-01-05 21:11: Train Epoch 9: 15/58 Loss: 0.223838
2023-01-05 21:11: Train Epoch 9: 19/58 Loss: 0.234510
2023-01-05 21:11: Train Epoch 9: 23/58 Loss: 0.198633
2023-01-05 21:12: Train Epoch 9: 27/58 Loss: 0.245093
2023-01-05 21:12: Train Epoch 9: 31/58 Loss: 0.222163
2023-01-05 21:12: Train Epoch 9: 35/58 Loss: 0.209494
2023-01-05 21:12: Train Epoch 9: 39/58 Loss: 0.221115
2023-01-05 21:13: Train Epoch 9: 43/58 Loss: 0.241272
2023-01-05 21:13: Train Epoch 9: 47/58 Loss: 0.227174
2023-01-05 21:13: Train Epoch 9: 51/58 Loss: 0.225103
2023-01-05 21:13: Train Epoch 9: 55/58 Loss: 0.207489
2023-01-05 21:13: Train Epoch 9: 57/58 Loss: 0.079813
2023-01-05 21:13: **********Train Epoch 9: averaged Loss: 0.212238 
2023-01-05 21:13: 
Epoch time elapsed: 204.5257866382599

2023-01-05 21:14: 
 metrics validation: {'precision': 0.8589494163424124, 'recall': 0.7190553745928339, 'f1-score': 0.7828014184397163, 'support': 1228, 'AUC': 0.9340294724612462, 'AUCPR': 0.868171891223312, 'TP': 883, 'FP': 145, 'TN': 2311, 'FN': 345} 

2023-01-05 21:14: **********Val Epoch 9: average Loss: 0.161305
2023-01-05 21:14: *********************************Current best model saved!
2023-01-05 21:16: 
 Testing metrics {'precision': 0.8639921722113503, 'recall': 0.7190553745928339, 'f1-score': 0.7848888888888889, 'support': 1228, 'AUC': 0.9362277716474444, 'AUCPR': 0.869275885071997, 'TP': 883, 'FP': 139, 'TN': 2317, 'FN': 345} 

2023-01-05 21:17: 
 Testing metrics {'precision': 0.8531400966183574, 'recall': 0.7190553745928339, 'f1-score': 0.7803800265134777, 'support': 1228, 'AUC': 0.933502610107269, 'AUCPR': 0.8616159067340905, 'TP': 883, 'FP': 152, 'TN': 2304, 'FN': 345} 

2023-01-05 21:17: Train Epoch 10: 3/58 Loss: 0.227560
2023-01-05 21:17: Train Epoch 10: 7/58 Loss: 0.239354
2023-01-05 21:18: Train Epoch 10: 11/58 Loss: 0.193368
2023-01-05 21:18: Train Epoch 10: 15/58 Loss: 0.203900
2023-01-05 21:18: Train Epoch 10: 19/58 Loss: 0.210558
2023-01-05 21:18: Train Epoch 10: 23/58 Loss: 0.198507
2023-01-05 21:19: Train Epoch 10: 27/58 Loss: 0.193002
2023-01-05 21:19: Train Epoch 10: 31/58 Loss: 0.220004
2023-01-05 21:19: Train Epoch 10: 35/58 Loss: 0.238562
2023-01-05 21:19: Train Epoch 10: 39/58 Loss: 0.215865
2023-01-05 21:20: Train Epoch 10: 43/58 Loss: 0.180392
2023-01-05 21:20: Train Epoch 10: 47/58 Loss: 0.213430
2023-01-05 21:20: Train Epoch 10: 51/58 Loss: 0.237745
2023-01-05 21:21: Train Epoch 10: 55/58 Loss: 0.207604
2023-01-05 21:21: Train Epoch 10: 57/58 Loss: 0.072764
2023-01-05 21:21: **********Train Epoch 10: averaged Loss: 0.203508 
2023-01-05 21:21: 
Epoch time elapsed: 236.24070715904236

2023-01-05 21:22: 
 metrics validation: {'precision': 0.7065081351689612, 'recall': 0.9193811074918566, 'f1-score': 0.7990092002830856, 'support': 1228, 'AUC': 0.9416575374805037, 'AUCPR': 0.8870889185363819, 'TP': 1129, 'FP': 469, 'TN': 1987, 'FN': 99} 

2023-01-05 21:22: **********Val Epoch 10: average Loss: 0.178370
2023-01-05 21:23: 
 Testing metrics {'precision': 0.8639921722113503, 'recall': 0.7190553745928339, 'f1-score': 0.7848888888888889, 'support': 1228, 'AUC': 0.9362277716474444, 'AUCPR': 0.869275885071997, 'TP': 883, 'FP': 139, 'TN': 2317, 'FN': 345} 

2023-01-05 21:24: 
 Testing metrics {'precision': 0.8531400966183574, 'recall': 0.7190553745928339, 'f1-score': 0.7803800265134777, 'support': 1228, 'AUC': 0.933502610107269, 'AUCPR': 0.8616159067340905, 'TP': 883, 'FP': 152, 'TN': 2304, 'FN': 345} 

2023-01-05 21:24: Train Epoch 11: 3/58 Loss: 0.227449
2023-01-05 21:25: Train Epoch 11: 7/58 Loss: 0.209156
2023-01-05 21:25: Train Epoch 11: 11/58 Loss: 0.218147
2023-01-05 21:25: Train Epoch 11: 15/58 Loss: 0.237041
2023-01-05 21:25: Train Epoch 11: 19/58 Loss: 0.203406
2023-01-05 21:26: Train Epoch 11: 23/58 Loss: 0.182566
2023-01-05 21:26: Train Epoch 11: 27/58 Loss: 0.219999
2023-01-05 21:26: Train Epoch 11: 31/58 Loss: 0.191053
2023-01-05 21:27: Train Epoch 11: 35/58 Loss: 0.196544
2023-01-05 21:27: Train Epoch 11: 39/58 Loss: 0.218219
2023-01-05 21:27: Train Epoch 11: 43/58 Loss: 0.215995
2023-01-05 21:27: Train Epoch 11: 47/58 Loss: 0.228322
2023-01-05 21:28: Train Epoch 11: 51/58 Loss: 0.187403
2023-01-05 21:28: Train Epoch 11: 55/58 Loss: 0.205179
2023-01-05 21:28: Train Epoch 11: 57/58 Loss: 0.098420
2023-01-05 21:28: **********Train Epoch 11: averaged Loss: 0.202593 
2023-01-05 21:28: 
Epoch time elapsed: 240.33733081817627

2023-01-05 21:29: 
 metrics validation: {'precision': 0.8768046198267565, 'recall': 0.74185667752443, 'f1-score': 0.8037053374503749, 'support': 1228, 'AUC': 0.9397354348587253, 'AUCPR': 0.8826094827725258, 'TP': 911, 'FP': 128, 'TN': 2328, 'FN': 317} 

2023-01-05 21:29: **********Val Epoch 11: average Loss: 0.152196
2023-01-05 21:29: *********************************Current best model saved!
2023-01-05 21:30: 
 Testing metrics {'precision': 0.8759615384615385, 'recall': 0.74185667752443, 'f1-score': 0.8033509700176367, 'support': 1228, 'AUC': 0.9418007750745365, 'AUCPR': 0.8825495436982299, 'TP': 911, 'FP': 129, 'TN': 2327, 'FN': 317} 

2023-01-05 21:32: 
 Testing metrics {'precision': 0.8659695817490495, 'recall': 0.74185667752443, 'f1-score': 0.7991228070175439, 'support': 1228, 'AUC': 0.939481121815616, 'AUCPR': 0.8769447645421854, 'TP': 911, 'FP': 141, 'TN': 2315, 'FN': 317} 

2023-01-05 21:32: Train Epoch 12: 3/58 Loss: 0.217700
2023-01-05 21:32: Train Epoch 12: 7/58 Loss: 0.255972
2023-01-05 21:32: Train Epoch 12: 11/58 Loss: 0.185923
2023-01-05 21:33: Train Epoch 12: 15/58 Loss: 0.223810
2023-01-05 21:33: Train Epoch 12: 19/58 Loss: 0.184437
2023-01-05 21:33: Train Epoch 12: 23/58 Loss: 0.215538
2023-01-05 21:34: Train Epoch 12: 27/58 Loss: 0.205772
2023-01-05 21:34: Train Epoch 12: 31/58 Loss: 0.181288
2023-01-05 21:34: Train Epoch 12: 35/58 Loss: 0.174581
2023-01-05 21:34: Train Epoch 12: 39/58 Loss: 0.246173
2023-01-05 21:35: Train Epoch 12: 43/58 Loss: 0.185733
2023-01-05 21:35: Train Epoch 12: 47/58 Loss: 0.209207
2023-01-05 21:35: Train Epoch 12: 51/58 Loss: 0.215149
2023-01-05 21:36: Train Epoch 12: 55/58 Loss: 0.200476
2023-01-05 21:36: Train Epoch 12: 57/58 Loss: 0.069412
2023-01-05 21:36: **********Train Epoch 12: averaged Loss: 0.198078 
2023-01-05 21:36: 
Epoch time elapsed: 242.74422907829285

2023-01-05 21:37: 
 metrics validation: {'precision': 0.7853025936599424, 'recall': 0.8876221498371335, 'f1-score': 0.8333333333333333, 'support': 1228, 'AUC': 0.9477428142473662, 'AUCPR': 0.901277659144876, 'TP': 1090, 'FP': 298, 'TN': 2158, 'FN': 138} 

2023-01-05 21:37: **********Val Epoch 12: average Loss: 0.149571
2023-01-05 21:37: *********************************Current best model saved!
2023-01-05 21:38: 
 Testing metrics {'precision': 0.7904278462654097, 'recall': 0.8876221498371335, 'f1-score': 0.8362102032988108, 'support': 1228, 'AUC': 0.9496158447304481, 'AUCPR': 0.897678636720933, 'TP': 1090, 'FP': 289, 'TN': 2167, 'FN': 138} 

2023-01-05 21:39: 
 Testing metrics {'precision': 0.7847372210223182, 'recall': 0.8876221498371335, 'f1-score': 0.8330149025601834, 'support': 1228, 'AUC': 0.9470455256819703, 'AUCPR': 0.8952589946838447, 'TP': 1090, 'FP': 299, 'TN': 2157, 'FN': 138} 

2023-01-05 21:39: Train Epoch 13: 3/58 Loss: 0.211273
2023-01-05 21:40: Train Epoch 13: 7/58 Loss: 0.176513
2023-01-05 21:40: Train Epoch 13: 11/58 Loss: 0.194641
2023-01-05 21:40: Train Epoch 13: 15/58 Loss: 0.205025
2023-01-05 21:41: Train Epoch 13: 19/58 Loss: 0.207785
2023-01-05 21:41: Train Epoch 13: 23/58 Loss: 0.187639
2023-01-05 21:41: Train Epoch 13: 27/58 Loss: 0.170460
2023-01-05 21:41: Train Epoch 13: 31/58 Loss: 0.180790
2023-01-05 21:42: Train Epoch 13: 35/58 Loss: 0.205438
2023-01-05 21:42: Train Epoch 13: 39/58 Loss: 0.186318
2023-01-05 21:42: Train Epoch 13: 43/58 Loss: 0.196213
2023-01-05 21:43: Train Epoch 13: 47/58 Loss: 0.174621
2023-01-05 21:43: Train Epoch 13: 51/58 Loss: 0.204699
2023-01-05 21:43: Train Epoch 13: 55/58 Loss: 0.175454
2023-01-05 21:43: Train Epoch 13: 57/58 Loss: 0.075841
2023-01-05 21:43: **********Train Epoch 13: averaged Loss: 0.183514 
2023-01-05 21:43: 
Epoch time elapsed: 250.69963026046753

2023-01-05 21:44: 
 metrics validation: {'precision': 0.81973293768546, 'recall': 0.8998371335504886, 'f1-score': 0.857919254658385, 'support': 1228, 'AUC': 0.9571318395951152, 'AUCPR': 0.9191287529274881, 'TP': 1105, 'FP': 243, 'TN': 2213, 'FN': 123} 

2023-01-05 21:44: **********Val Epoch 13: average Loss: 0.139719
2023-01-05 21:44: *********************************Current best model saved!
2023-01-05 21:46: 
 Testing metrics {'precision': 0.8227848101265823, 'recall': 0.8998371335504886, 'f1-score': 0.8595877090626216, 'support': 1228, 'AUC': 0.9591328555210136, 'AUCPR': 0.9147768513396906, 'TP': 1105, 'FP': 238, 'TN': 2218, 'FN': 123} 

2023-01-05 21:47: 
 Testing metrics {'precision': 0.8154981549815498, 'recall': 0.8998371335504886, 'f1-score': 0.8555942702284165, 'support': 1228, 'AUC': 0.9558708845717195, 'AUCPR': 0.9110970861555441, 'TP': 1105, 'FP': 250, 'TN': 2206, 'FN': 123} 

2023-01-05 21:47: Train Epoch 14: 3/58 Loss: 0.166997
2023-01-05 21:47: Train Epoch 14: 7/58 Loss: 0.179790
2023-01-05 21:48: Train Epoch 14: 11/58 Loss: 0.181378
2023-01-05 21:48: Train Epoch 14: 15/58 Loss: 0.164728
2023-01-05 21:48: Train Epoch 14: 19/58 Loss: 0.188137
2023-01-05 21:48: Train Epoch 14: 23/58 Loss: 0.179583
2023-01-05 21:49: Train Epoch 14: 27/58 Loss: 0.184363
2023-01-05 21:49: Train Epoch 14: 31/58 Loss: 0.218033
2023-01-05 21:49: Train Epoch 14: 35/58 Loss: 0.192989
2023-01-05 21:50: Train Epoch 14: 39/58 Loss: 0.195069
2023-01-05 21:50: Train Epoch 14: 43/58 Loss: 0.201826
2023-01-05 21:50: Train Epoch 14: 47/58 Loss: 0.161318
2023-01-05 21:50: Train Epoch 14: 51/58 Loss: 0.201602
2023-01-05 21:51: Train Epoch 14: 55/58 Loss: 0.198289
2023-01-05 21:51: Train Epoch 14: 57/58 Loss: 0.089973
2023-01-05 21:51: **********Train Epoch 14: averaged Loss: 0.180272 
2023-01-05 21:51: 
Epoch time elapsed: 247.1056981086731

2023-01-05 21:52: 
 metrics validation: {'precision': 0.8124108416547788, 'recall': 0.9275244299674267, 'f1-score': 0.8661596958174904, 'support': 1228, 'AUC': 0.9656468503644601, 'AUCPR': 0.9338843514951263, 'TP': 1139, 'FP': 263, 'TN': 2193, 'FN': 89} 

2023-01-05 21:52: **********Val Epoch 14: average Loss: 0.137447
2023-01-05 21:52: *********************************Current best model saved!
2023-01-05 21:53: 
 Testing metrics {'precision': 0.8188353702372394, 'recall': 0.9275244299674267, 'f1-score': 0.8697976326842306, 'support': 1228, 'AUC': 0.9672430211461129, 'AUCPR': 0.9286137642003405, 'TP': 1139, 'FP': 252, 'TN': 2204, 'FN': 89} 

2023-01-05 21:54: 
 Testing metrics {'precision': 0.8049469964664311, 'recall': 0.9275244299674267, 'f1-score': 0.8618993567915247, 'support': 1228, 'AUC': 0.9640619529119673, 'AUCPR': 0.9254759012450998, 'TP': 1139, 'FP': 276, 'TN': 2180, 'FN': 89} 

2023-01-05 21:55: Train Epoch 15: 3/58 Loss: 0.169272
2023-01-05 21:55: Train Epoch 15: 7/58 Loss: 0.185153
2023-01-05 21:55: Train Epoch 15: 11/58 Loss: 0.169427
2023-01-05 21:55: Train Epoch 15: 15/58 Loss: 0.173988
2023-01-05 21:56: Train Epoch 15: 19/58 Loss: 0.205524
2023-01-05 21:56: Train Epoch 15: 23/58 Loss: 0.166254
2023-01-05 21:56: Train Epoch 15: 27/58 Loss: 0.179617
2023-01-05 21:56: Train Epoch 15: 31/58 Loss: 0.201014
2023-01-05 21:57: Train Epoch 15: 35/58 Loss: 0.185006
2023-01-05 21:57: Train Epoch 15: 39/58 Loss: 0.167384
2023-01-05 21:57: Train Epoch 15: 43/58 Loss: 0.179778
2023-01-05 21:58: Train Epoch 15: 47/58 Loss: 0.177087
2023-01-05 21:58: Train Epoch 15: 51/58 Loss: 0.179570
2023-01-05 21:58: Train Epoch 15: 55/58 Loss: 0.167640
2023-01-05 21:58: Train Epoch 15: 57/58 Loss: 0.070186
2023-01-05 21:58: **********Train Epoch 15: averaged Loss: 0.171793 
2023-01-05 21:58: 
Epoch time elapsed: 232.40193009376526

2023-01-05 21:59: 
 metrics validation: {'precision': 0.9545945945945946, 'recall': 0.7190553745928339, 'f1-score': 0.8202508128193219, 'support': 1228, 'AUC': 0.9709330470349817, 'AUCPR': 0.9461382822335476, 'TP': 883, 'FP': 42, 'TN': 2414, 'FN': 345} 

2023-01-05 21:59: **********Val Epoch 15: average Loss: 0.133369
2023-01-05 21:59: *********************************Current best model saved!
2023-01-05 22:01: 
 Testing metrics {'precision': 0.9629225736095965, 'recall': 0.7190553745928339, 'f1-score': 0.8233100233100233, 'support': 1228, 'AUC': 0.9721250358093986, 'AUCPR': 0.9390113558596115, 'TP': 883, 'FP': 34, 'TN': 2422, 'FN': 345} 

2023-01-05 22:02: 
 Testing metrics {'precision': 0.9474248927038627, 'recall': 0.7190553745928339, 'f1-score': 0.8175925925925926, 'support': 1228, 'AUC': 0.9692632680452843, 'AUCPR': 0.9379376948583565, 'TP': 883, 'FP': 49, 'TN': 2407, 'FN': 345} 

2023-01-05 22:02: Train Epoch 16: 3/58 Loss: 0.173470
2023-01-05 22:02: Train Epoch 16: 7/58 Loss: 0.179262
2023-01-05 22:02: Train Epoch 16: 11/58 Loss: 0.175985
2023-01-05 22:03: Train Epoch 16: 15/58 Loss: 0.176131
2023-01-05 22:03: Train Epoch 16: 19/58 Loss: 0.154842
2023-01-05 22:03: Train Epoch 16: 23/58 Loss: 0.184887
2023-01-05 22:04: Train Epoch 16: 27/58 Loss: 0.180375
2023-01-05 22:04: Train Epoch 16: 31/58 Loss: 0.158668
2023-01-05 22:04: Train Epoch 16: 35/58 Loss: 0.174517
2023-01-05 22:04: Train Epoch 16: 39/58 Loss: 0.182257
2023-01-05 22:05: Train Epoch 16: 43/58 Loss: 0.184030
2023-01-05 22:05: Train Epoch 16: 47/58 Loss: 0.184292
2023-01-05 22:05: Train Epoch 16: 51/58 Loss: 0.175205
2023-01-05 22:05: Train Epoch 16: 55/58 Loss: 0.178462
2023-01-05 22:06: Train Epoch 16: 57/58 Loss: 0.055029
2023-01-05 22:06: **********Train Epoch 16: averaged Loss: 0.167827 
2023-01-05 22:06: 
Epoch time elapsed: 232.49413013458252

2023-01-05 22:07: 
 metrics validation: {'precision': 0.8335777126099707, 'recall': 0.9258957654723127, 'f1-score': 0.8773148148148149, 'support': 1228, 'AUC': 0.9732168908953942, 'AUCPR': 0.9514074193881962, 'TP': 1137, 'FP': 227, 'TN': 2229, 'FN': 91} 

2023-01-05 22:07: **********Val Epoch 16: average Loss: 0.118819
2023-01-05 22:07: *********************************Current best model saved!
2023-01-05 22:08: 
 Testing metrics {'precision': 0.8372606774668631, 'recall': 0.9258957654723127, 'f1-score': 0.8793503480278423, 'support': 1228, 'AUC': 0.9738336083141466, 'AUCPR': 0.9432012528455915, 'TP': 1137, 'FP': 221, 'TN': 2235, 'FN': 91} 

2023-01-05 22:09: 
 Testing metrics {'precision': 0.8263081395348837, 'recall': 0.9258957654723127, 'f1-score': 0.8732718894009216, 'support': 1228, 'AUC': 0.9716048048255153, 'AUCPR': 0.9442941512435401, 'TP': 1137, 'FP': 239, 'TN': 2217, 'FN': 91} 

2023-01-05 22:09: Train Epoch 17: 3/58 Loss: 0.162075
2023-01-05 22:10: Train Epoch 17: 7/58 Loss: 0.184576
2023-01-05 22:10: Train Epoch 17: 11/58 Loss: 0.162762
2023-01-05 22:10: Train Epoch 17: 15/58 Loss: 0.149402
2023-01-05 22:10: Train Epoch 17: 19/58 Loss: 0.193675
2023-01-05 22:11: Train Epoch 17: 23/58 Loss: 0.168070
2023-01-05 22:11: Train Epoch 17: 27/58 Loss: 0.172683
2023-01-05 22:11: Train Epoch 17: 31/58 Loss: 0.198837
2023-01-05 22:11: Train Epoch 17: 35/58 Loss: 0.199076
2023-01-05 22:12: Train Epoch 17: 39/58 Loss: 0.179440
2023-01-05 22:12: Train Epoch 17: 43/58 Loss: 0.188430
2023-01-05 22:12: Train Epoch 17: 47/58 Loss: 0.174152
2023-01-05 22:13: Train Epoch 17: 51/58 Loss: 0.160168
2023-01-05 22:13: Train Epoch 17: 55/58 Loss: 0.154669
2023-01-05 22:13: Train Epoch 17: 57/58 Loss: 0.060184
2023-01-05 22:13: **********Train Epoch 17: averaged Loss: 0.167213 
2023-01-05 22:13: 
Epoch time elapsed: 235.89463210105896

2023-01-05 22:14: 
 metrics validation: {'precision': 0.9421338155515371, 'recall': 0.8485342019543974, 'f1-score': 0.8928877463581834, 'support': 1228, 'AUC': 0.9777676686224788, 'AUCPR': 0.9596089641700364, 'TP': 1042, 'FP': 64, 'TN': 2392, 'FN': 186} 

2023-01-05 22:14: **********Val Epoch 17: average Loss: 0.110001
2023-01-05 22:14: *********************************Current best model saved!
2023-01-05 22:15: 
 Testing metrics {'precision': 0.9464123524069028, 'recall': 0.8485342019543974, 'f1-score': 0.8948046371833405, 'support': 1228, 'AUC': 0.9780398863648422, 'AUCPR': 0.9503732659431137, 'TP': 1042, 'FP': 59, 'TN': 2397, 'FN': 186} 

2023-01-05 22:16: 
 Testing metrics {'precision': 0.9328558639212176, 'recall': 0.8485342019543974, 'f1-score': 0.8886993603411515, 'support': 1228, 'AUC': 0.9757659895595708, 'AUCPR': 0.9506761152917748, 'TP': 1042, 'FP': 75, 'TN': 2381, 'FN': 186} 

2023-01-05 22:17: Train Epoch 18: 3/58 Loss: 0.154530
2023-01-05 22:17: Train Epoch 18: 7/58 Loss: 0.173135
2023-01-05 22:17: Train Epoch 18: 11/58 Loss: 0.170666
2023-01-05 22:17: Train Epoch 18: 15/58 Loss: 0.147615
2023-01-05 22:18: Train Epoch 18: 19/58 Loss: 0.159199
2023-01-05 22:18: Train Epoch 18: 23/58 Loss: 0.155847
2023-01-05 22:18: Train Epoch 18: 27/58 Loss: 0.150077
2023-01-05 22:18: Train Epoch 18: 31/58 Loss: 0.157219
2023-01-05 22:19: Train Epoch 18: 35/58 Loss: 0.160623
2023-01-05 22:19: Train Epoch 18: 39/58 Loss: 0.184614
2023-01-05 22:19: Train Epoch 18: 43/58 Loss: 0.138177
2023-01-05 22:19: Train Epoch 18: 47/58 Loss: 0.168211
2023-01-05 22:19: Train Epoch 18: 51/58 Loss: 0.166357
2023-01-05 22:20: Train Epoch 18: 55/58 Loss: 0.148251
2023-01-05 22:20: Train Epoch 18: 57/58 Loss: 0.040356
2023-01-05 22:20: **********Train Epoch 18: averaged Loss: 0.151658 
2023-01-05 22:20: 
Epoch time elapsed: 203.78277826309204

2023-01-05 22:21: 
 metrics validation: {'precision': 0.866565579984837, 'recall': 0.9307817589576547, 'f1-score': 0.8975265017667845, 'support': 1228, 'AUC': 0.9786735137773347, 'AUCPR': 0.9607196923576092, 'TP': 1143, 'FP': 176, 'TN': 2280, 'FN': 85} 

2023-01-05 22:21: **********Val Epoch 18: average Loss: 0.102725
2023-01-05 22:21: *********************************Current best model saved!
2023-01-05 22:22: 
 Testing metrics {'precision': 0.8692015209125475, 'recall': 0.9307817589576547, 'f1-score': 0.8989382618953992, 'support': 1228, 'AUC': 0.9787683423696804, 'AUCPR': 0.9524099275435425, 'TP': 1143, 'FP': 172, 'TN': 2284, 'FN': 85} 

2023-01-05 22:23: 
 Testing metrics {'precision': 0.8574643660915229, 'recall': 0.9307817589576547, 'f1-score': 0.8926200702850449, 'support': 1228, 'AUC': 0.9770673959405406, 'AUCPR': 0.9537028220305969, 'TP': 1143, 'FP': 190, 'TN': 2266, 'FN': 85} 

2023-01-05 22:23: Train Epoch 19: 3/58 Loss: 0.174737
2023-01-05 22:23: Train Epoch 19: 7/58 Loss: 0.151037
2023-01-05 22:24: Train Epoch 19: 11/58 Loss: 0.149378
2023-01-05 22:24: Train Epoch 19: 15/58 Loss: 0.153516
2023-01-05 22:24: Train Epoch 19: 19/58 Loss: 0.173105
2023-01-05 22:24: Train Epoch 19: 23/58 Loss: 0.142049
2023-01-05 22:24: Train Epoch 19: 27/58 Loss: 0.149938
2023-01-05 22:25: Train Epoch 19: 31/58 Loss: 0.177211
2023-01-05 22:25: Train Epoch 19: 35/58 Loss: 0.169763
2023-01-05 22:25: Train Epoch 19: 39/58 Loss: 0.178528
2023-01-05 22:25: Train Epoch 19: 43/58 Loss: 0.167173
2023-01-05 22:26: Train Epoch 19: 47/58 Loss: 0.153252
2023-01-05 22:26: Train Epoch 19: 51/58 Loss: 0.143529
2023-01-05 22:26: Train Epoch 19: 55/58 Loss: 0.147659
2023-01-05 22:26: Train Epoch 19: 57/58 Loss: 0.055566
2023-01-05 22:26: **********Train Epoch 19: averaged Loss: 0.152429 
2023-01-05 22:26: 
Epoch time elapsed: 206.96676898002625

2023-01-05 22:27: 
 metrics validation: {'precision': 0.9232715008431703, 'recall': 0.8916938110749185, 'f1-score': 0.9072079536039768, 'support': 1228, 'AUC': 0.9814348162845228, 'AUCPR': 0.9668604505506866, 'TP': 1095, 'FP': 91, 'TN': 2365, 'FN': 133} 

2023-01-05 22:27: **********Val Epoch 19: average Loss: 0.094932
2023-01-05 22:27: *********************************Current best model saved!
2023-01-05 22:28: 
 Testing metrics {'precision': 0.9335038363171355, 'recall': 0.8916938110749185, 'f1-score': 0.9121199500208247, 'support': 1228, 'AUC': 0.9813645237615254, 'AUCPR': 0.9593089748000792, 'TP': 1095, 'FP': 78, 'TN': 2378, 'FN': 133} 

2023-01-05 22:29: 
 Testing metrics {'precision': 0.9178541492036881, 'recall': 0.8916938110749185, 'f1-score': 0.9045848822800495, 'support': 1228, 'AUC': 0.979780289446042, 'AUCPR': 0.9597488481933454, 'TP': 1095, 'FP': 98, 'TN': 2358, 'FN': 133} 

2023-01-05 22:30: Train Epoch 20: 3/58 Loss: 0.140450
2023-01-05 22:30: Train Epoch 20: 7/58 Loss: 0.143098
2023-01-05 22:30: Train Epoch 20: 11/58 Loss: 0.152306
2023-01-05 22:30: Train Epoch 20: 15/58 Loss: 0.151997
2023-01-05 22:31: Train Epoch 20: 19/58 Loss: 0.147956
