2022-12-20 08:11: log dir: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/20221220081102
2022-12-20 08:11: Experiment log path in: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/20221220081102
2022-12-20 08:11: Argument: Namespace(batch_size=256, clc='vec', cuda=True, dataset='2020', dataset_root='/home/joel.chacon/tmp/datasets_grl/', debug=False, default_graph=True, device='cpu', dynamic_features=['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh'], early_stop=True, early_stop_patience=8, embed_dim=25, epochs=30, gamma=1.0, grad_norm=False, horizon=1, input_dim=25, lag=10, link_len=2, log_dir='/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/20221220081102', log_step=1, loss_func='nllloss', lr_decay=True, lr_decay_rate=0.1, lr_decay_step='10, 15, 20, 25', lr_init=0.0001, mae_thresh=None, mape_thresh=0.0, max_grad_norm=5, mode='train', model='fire_GCN', nan_fill=0.5, num_layers=1, num_nodes=625, num_workers=20, output_dim=2, patch_height=25, patch_width=25, persistent_workers=True, pin_memory=True, plot=False, positive_weight=0.5, prefetch_factor=2, real_value=True, rnn_units=16, seed=1992, static_features=['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density'], teacher_forcing=False, test_ratio=0.2, val_ratio=0.2, weight_decay=0.01, window_len=10)
2022-12-20 08:11: Argument batch_size: 256
2022-12-20 08:11: Argument clc: 'vec'
2022-12-20 08:11: Argument cuda: True
2022-12-20 08:11: Argument dataset: '2020'
2022-12-20 08:11: Argument dataset_root: '/home/joel.chacon/tmp/datasets_grl/'
2022-12-20 08:11: Argument debug: False
2022-12-20 08:11: Argument default_graph: True
2022-12-20 08:11: Argument device: 'cpu'
2022-12-20 08:11: Argument dynamic_features: ['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh']
2022-12-20 08:11: Argument early_stop: True
2022-12-20 08:11: Argument early_stop_patience: 8
2022-12-20 08:11: Argument embed_dim: 25
2022-12-20 08:11: Argument epochs: 30
2022-12-20 08:11: Argument gamma: 1.0
2022-12-20 08:11: Argument grad_norm: False
2022-12-20 08:11: Argument horizon: 1
2022-12-20 08:11: Argument input_dim: 25
2022-12-20 08:11: Argument lag: 10
2022-12-20 08:11: Argument link_len: 2
2022-12-20 08:11: Argument log_dir: '/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/20221220081102'
2022-12-20 08:11: Argument log_step: 1
2022-12-20 08:11: Argument loss_func: 'nllloss'
2022-12-20 08:11: Argument lr_decay: True
2022-12-20 08:11: Argument lr_decay_rate: 0.1
2022-12-20 08:11: Argument lr_decay_step: '10, 15, 20, 25'
2022-12-20 08:11: Argument lr_init: 0.0001
2022-12-20 08:11: Argument mae_thresh: None
2022-12-20 08:11: Argument mape_thresh: 0.0
2022-12-20 08:11: Argument max_grad_norm: 5
2022-12-20 08:11: Argument mode: 'train'
2022-12-20 08:11: Argument model: 'fire_GCN'
2022-12-20 08:11: Argument nan_fill: 0.5
2022-12-20 08:11: Argument num_layers: 1
2022-12-20 08:11: Argument num_nodes: 625
2022-12-20 08:11: Argument num_workers: 20
2022-12-20 08:11: Argument output_dim: 2
2022-12-20 08:11: Argument patch_height: 25
2022-12-20 08:11: Argument patch_width: 25
2022-12-20 08:11: Argument persistent_workers: True
2022-12-20 08:11: Argument pin_memory: True
2022-12-20 08:11: Argument plot: False
2022-12-20 08:11: Argument positive_weight: 0.5
2022-12-20 08:11: Argument prefetch_factor: 2
2022-12-20 08:11: Argument real_value: True
2022-12-20 08:11: Argument rnn_units: 16
2022-12-20 08:11: Argument seed: 1992
2022-12-20 08:11: Argument static_features: ['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density']
2022-12-20 08:11: Argument teacher_forcing: False
2022-12-20 08:11: Argument test_ratio: 0.2
2022-12-20 08:11: Argument val_ratio: 0.2
2022-12-20 08:11: Argument weight_decay: 0.01
2022-12-20 08:11: Argument window_len: 10
++++++++++++++
2020_fire_GCN.conf
++++++++++++++
*****************Model Parameter*****************
node_embeddings torch.Size([625, 25]) True
ln1.weight torch.Size([25]) True
ln1.bias torch.Size([25]) True
encoder.cell_list.0.gate.weights_pool torch.Size([25, 2, 41, 16]) True
encoder.cell_list.0.gate.weights_window torch.Size([25, 1, 16]) True
encoder.cell_list.0.gate.bias_pool torch.Size([25, 32]) True
encoder.cell_list.0.gate.T torch.Size([10]) True
encoder.cell_list.0.gate.cnn.features.0.weight torch.Size([8, 1, 3, 3]) True
encoder.cell_list.0.gate.cnn.features.0.bias torch.Size([8]) True
encoder.cell_list.0.gate.cnn.features.3.weight torch.Size([16, 8, 3, 3]) True
encoder.cell_list.0.gate.cnn.features.3.bias torch.Size([16]) True
encoder.cell_list.0.update.weights_pool torch.Size([25, 2, 41, 8]) True
encoder.cell_list.0.update.weights_window torch.Size([25, 1, 8]) True
encoder.cell_list.0.update.bias_pool torch.Size([25, 16]) True
encoder.cell_list.0.update.T torch.Size([10]) True
encoder.cell_list.0.update.cnn.features.0.weight torch.Size([8, 1, 3, 3]) True
encoder.cell_list.0.update.cnn.features.0.bias torch.Size([8]) True
encoder.cell_list.0.update.cnn.features.3.weight torch.Size([8, 8, 3, 3]) True
encoder.cell_list.0.update.cnn.features.3.bias torch.Size([8]) True
end_conv.weight torch.Size([2, 1, 625, 16]) True
end_conv.bias torch.Size([2]) True
Total params num: 88609
*****************Finish Parameter****************
Positives: 13518 / Negatives: 27036
Dataset length 40554
Positives: 1300 / Negatives: 2600
Dataset length 3900
Positives: 1228 / Negatives: 2456
Dataset length 3684
Applying learning rate decay.
Creat Log File in:  /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/20221220081102/run.log
2022-12-20 08:13: Train Epoch 1: 0/159 Loss: 0.663744
2022-12-20 08:14: Train Epoch 1: 1/159 Loss: 0.614879
2022-12-20 08:15: Train Epoch 1: 2/159 Loss: 0.668037
2022-12-20 08:17: Train Epoch 1: 3/159 Loss: 0.658474
2022-12-20 08:18: Train Epoch 1: 4/159 Loss: 0.690527
2022-12-20 08:19: Train Epoch 1: 5/159 Loss: 0.652120
2022-12-20 08:21: Train Epoch 1: 6/159 Loss: 0.629149
2022-12-20 08:22: Train Epoch 1: 7/159 Loss: 0.656437
2022-12-20 08:24: Train Epoch 1: 8/159 Loss: 0.669818
2022-12-20 08:25: Train Epoch 1: 9/159 Loss: 0.651643
2022-12-20 08:26: Train Epoch 1: 10/159 Loss: 0.645196
2022-12-20 08:28: Train Epoch 1: 11/159 Loss: 0.646771
2022-12-20 08:29: Train Epoch 1: 12/159 Loss: 0.622844
2022-12-20 08:30: Train Epoch 1: 13/159 Loss: 0.623114
2022-12-20 08:32: Train Epoch 1: 14/159 Loss: 0.580414
2022-12-20 08:33: Train Epoch 1: 15/159 Loss: 0.658768
2022-12-20 08:34: Train Epoch 1: 16/159 Loss: 0.568009
2022-12-20 08:36: Train Epoch 1: 17/159 Loss: 0.684223
2022-12-20 08:37: Train Epoch 1: 18/159 Loss: 0.646558
2022-12-20 08:38: Train Epoch 1: 19/159 Loss: 0.595988
2022-12-20 08:40: Train Epoch 1: 20/159 Loss: 0.650471
2022-12-20 08:41: Train Epoch 1: 21/159 Loss: 0.647624
2022-12-20 08:43: Train Epoch 1: 22/159 Loss: 0.644190
2022-12-20 08:44: Train Epoch 1: 23/159 Loss: 0.643561
2022-12-20 08:46: Train Epoch 1: 24/159 Loss: 0.653307
2022-12-20 08:47: Train Epoch 1: 25/159 Loss: 0.622167
2022-12-20 08:48: Train Epoch 1: 26/159 Loss: 0.639424
2022-12-20 08:50: Train Epoch 1: 27/159 Loss: 0.622287
2022-12-20 08:51: Train Epoch 1: 28/159 Loss: 0.633612
2022-12-20 08:53: Train Epoch 1: 29/159 Loss: 0.693526
2022-12-20 08:54: Train Epoch 1: 30/159 Loss: 0.632126
2022-12-20 08:55: Train Epoch 1: 31/159 Loss: 0.632393
2022-12-20 08:57: Train Epoch 1: 32/159 Loss: 0.680312
2022-12-20 08:58: Train Epoch 1: 33/159 Loss: 0.627199
2022-12-20 09:00: Train Epoch 1: 34/159 Loss: 0.633885
2022-12-20 09:01: Train Epoch 1: 35/159 Loss: 0.631985
2022-12-20 09:02: Train Epoch 1: 36/159 Loss: 0.622790
2022-12-20 09:04: Train Epoch 1: 37/159 Loss: 0.638608
2022-12-20 09:05: Train Epoch 1: 38/159 Loss: 0.647783
2022-12-20 09:06: Train Epoch 1: 39/159 Loss: 0.601815
2022-12-20 09:08: Train Epoch 1: 40/159 Loss: 0.663132
2022-12-20 09:09: Train Epoch 1: 41/159 Loss: 0.657534
2022-12-20 09:11: Train Epoch 1: 42/159 Loss: 0.638473
2022-12-20 09:12: Train Epoch 1: 43/159 Loss: 0.622719
2022-12-20 09:13: Train Epoch 1: 44/159 Loss: 0.602686
2022-12-20 09:15: Train Epoch 1: 45/159 Loss: 0.649567
2022-12-20 09:16: Train Epoch 1: 46/159 Loss: 0.632426
2022-12-20 09:18: Train Epoch 1: 47/159 Loss: 0.624635
2022-12-20 09:19: Train Epoch 1: 48/159 Loss: 0.637381
2022-12-20 09:21: Train Epoch 1: 49/159 Loss: 0.610328
2022-12-20 09:22: Train Epoch 1: 50/159 Loss: 0.608090
2022-12-20 09:24: Train Epoch 1: 51/159 Loss: 0.617657
2022-12-20 09:25: Train Epoch 1: 52/159 Loss: 0.641597
2022-12-20 09:26: Train Epoch 1: 53/159 Loss: 0.595114
2022-12-20 09:28: Train Epoch 1: 54/159 Loss: 0.570957
2022-12-20 09:29: Train Epoch 1: 55/159 Loss: 0.617362
2022-12-20 09:31: Train Epoch 1: 56/159 Loss: 0.587611
2022-12-20 09:32: Train Epoch 1: 57/159 Loss: 0.637233
2022-12-20 09:34: Train Epoch 1: 58/159 Loss: 0.592923
2022-12-20 09:35: Train Epoch 1: 59/159 Loss: 0.643507
2022-12-20 09:37: Train Epoch 1: 60/159 Loss: 0.603024
2022-12-20 09:38: Train Epoch 1: 61/159 Loss: 0.583206
2022-12-20 09:39: Train Epoch 1: 62/159 Loss: 0.620270
2022-12-20 09:41: Train Epoch 1: 63/159 Loss: 0.620776
2022-12-20 09:42: Train Epoch 1: 64/159 Loss: 0.587635
2022-12-20 09:44: Train Epoch 1: 65/159 Loss: 0.577778
2022-12-20 09:45: Train Epoch 1: 66/159 Loss: 0.597520
2022-12-20 09:47: Train Epoch 1: 67/159 Loss: 0.588343
2022-12-20 09:48: Train Epoch 1: 68/159 Loss: 0.600545
2022-12-20 09:49: Train Epoch 1: 69/159 Loss: 0.615120
2022-12-20 09:51: Train Epoch 1: 70/159 Loss: 0.615932
2022-12-20 09:52: Train Epoch 1: 71/159 Loss: 0.628567
2022-12-20 09:54: Train Epoch 1: 72/159 Loss: 0.610774
2022-12-20 09:55: Train Epoch 1: 73/159 Loss: 0.593162
2022-12-20 09:56: Train Epoch 1: 74/159 Loss: 0.614446
2022-12-20 09:58: Train Epoch 1: 75/159 Loss: 0.612509
2022-12-20 09:59: Train Epoch 1: 76/159 Loss: 0.607940
2022-12-20 10:01: Train Epoch 1: 77/159 Loss: 0.614127
2022-12-20 10:02: Train Epoch 1: 78/159 Loss: 0.549307
2022-12-20 10:04: Train Epoch 1: 79/159 Loss: 0.576092
2022-12-20 10:05: Train Epoch 1: 80/159 Loss: 0.612503
2022-12-20 10:06: Train Epoch 1: 81/159 Loss: 0.630296
2022-12-20 10:08: Train Epoch 1: 82/159 Loss: 0.641961
2022-12-20 10:09: Train Epoch 1: 83/159 Loss: 0.572719
2022-12-20 10:11: Train Epoch 1: 84/159 Loss: 0.583082
2022-12-20 10:12: Train Epoch 1: 85/159 Loss: 0.613015
2022-12-20 10:14: Train Epoch 1: 86/159 Loss: 0.597009
2022-12-20 10:15: Train Epoch 1: 87/159 Loss: 0.608082
2022-12-20 10:17: Train Epoch 1: 88/159 Loss: 0.630838
2022-12-20 10:18: Train Epoch 1: 89/159 Loss: 0.589006
2022-12-20 10:20: Train Epoch 1: 90/159 Loss: 0.590545
2022-12-20 10:21: Train Epoch 1: 91/159 Loss: 0.607365
2022-12-20 10:22: Train Epoch 1: 92/159 Loss: 0.622448
2022-12-20 10:24: Train Epoch 1: 93/159 Loss: 0.594356
2022-12-20 10:25: Train Epoch 1: 94/159 Loss: 0.606164
2022-12-20 10:27: Train Epoch 1: 95/159 Loss: 0.568575
2022-12-20 10:28: Train Epoch 1: 96/159 Loss: 0.590160
2022-12-20 10:30: Train Epoch 1: 97/159 Loss: 0.606748
2022-12-20 10:31: Train Epoch 1: 98/159 Loss: 0.582910
2022-12-20 10:32: Train Epoch 1: 99/159 Loss: 0.575793
2022-12-20 10:34: Train Epoch 1: 100/159 Loss: 0.561593
2022-12-20 10:35: Train Epoch 1: 101/159 Loss: 0.583177
2022-12-20 10:37: Train Epoch 1: 102/159 Loss: 0.587087
2022-12-20 10:38: Train Epoch 1: 103/159 Loss: 0.576564
2022-12-20 10:40: Train Epoch 1: 104/159 Loss: 0.554894
2022-12-20 10:41: Train Epoch 1: 105/159 Loss: 0.582256
2022-12-20 10:43: Train Epoch 1: 106/159 Loss: 0.590777
2022-12-20 10:44: Train Epoch 1: 107/159 Loss: 0.587323
2022-12-20 10:46: Train Epoch 1: 108/159 Loss: 0.551637
2022-12-20 10:47: Train Epoch 1: 109/159 Loss: 0.575816
2022-12-20 10:48: Train Epoch 1: 110/159 Loss: 0.564154
2022-12-20 10:50: Train Epoch 1: 111/159 Loss: 0.549234
2022-12-20 10:51: Train Epoch 1: 112/159 Loss: 0.554015
2022-12-20 10:53: Train Epoch 1: 113/159 Loss: 0.602906
2022-12-20 10:54: Train Epoch 1: 114/159 Loss: 0.566550
2022-12-20 10:56: Train Epoch 1: 115/159 Loss: 0.570067
2022-12-20 10:57: Train Epoch 1: 116/159 Loss: 0.514879
2022-12-20 10:58: Train Epoch 1: 117/159 Loss: 0.512567
2022-12-20 11:00: Train Epoch 1: 118/159 Loss: 0.540646
2022-12-20 11:01: Train Epoch 1: 119/159 Loss: 0.550685
2022-12-20 11:03: Train Epoch 1: 120/159 Loss: 0.546778
2022-12-20 11:04: Train Epoch 1: 121/159 Loss: 0.535580
2022-12-20 11:06: Train Epoch 1: 122/159 Loss: 0.550028
2022-12-20 11:07: Train Epoch 1: 123/159 Loss: 0.505243
2022-12-20 11:08: Train Epoch 1: 124/159 Loss: 0.543942
2022-12-20 11:10: Train Epoch 1: 125/159 Loss: 0.543546
2022-12-20 11:11: Train Epoch 1: 126/159 Loss: 0.552623
2022-12-20 11:13: Train Epoch 1: 127/159 Loss: 0.514177
2022-12-20 11:14: Train Epoch 1: 128/159 Loss: 0.500032
2022-12-20 11:16: Train Epoch 1: 129/159 Loss: 0.530357
2022-12-20 11:17: Train Epoch 1: 130/159 Loss: 0.528162
2022-12-20 11:18: Train Epoch 1: 131/159 Loss: 0.506983
2022-12-20 11:20: Train Epoch 1: 132/159 Loss: 0.545212
2022-12-20 11:21: Train Epoch 1: 133/159 Loss: 0.502599
2022-12-20 11:23: Train Epoch 1: 134/159 Loss: 0.498462
2022-12-20 11:24: Train Epoch 1: 135/159 Loss: 0.555038
2022-12-20 11:25: Train Epoch 1: 136/159 Loss: 0.510340
2022-12-20 11:27: Train Epoch 1: 137/159 Loss: 0.487353
2022-12-20 11:28: Train Epoch 1: 138/159 Loss: 0.505433
2022-12-20 11:30: Train Epoch 1: 139/159 Loss: 0.548676
2022-12-20 11:31: Train Epoch 1: 140/159 Loss: 0.490718
2022-12-20 11:32: Train Epoch 1: 141/159 Loss: 0.482905
2022-12-20 11:34: Train Epoch 1: 142/159 Loss: 0.513053
2022-12-20 11:35: Train Epoch 1: 143/159 Loss: 0.497729
2022-12-20 11:36: Train Epoch 1: 144/159 Loss: 0.511778
2022-12-20 11:38: Train Epoch 1: 145/159 Loss: 0.484817
2022-12-20 11:39: Train Epoch 1: 146/159 Loss: 0.483111
2022-12-20 11:41: Train Epoch 1: 147/159 Loss: 0.480372
2022-12-20 11:42: Train Epoch 1: 148/159 Loss: 0.448380
2022-12-20 11:43: Train Epoch 1: 149/159 Loss: 0.503956
2022-12-20 11:45: Train Epoch 1: 150/159 Loss: 0.446089
2022-12-20 11:46: Train Epoch 1: 151/159 Loss: 0.479050
2022-12-20 11:48: Train Epoch 1: 152/159 Loss: 0.482297
2022-12-20 11:49: Train Epoch 1: 153/159 Loss: 0.459549
2022-12-20 11:50: Train Epoch 1: 154/159 Loss: 0.484628
2022-12-20 11:52: Train Epoch 1: 155/159 Loss: 0.479888
2022-12-20 11:53: Train Epoch 1: 156/159 Loss: 0.470467
2022-12-20 11:55: Train Epoch 1: 157/159 Loss: 0.462937
2022-12-20 11:55: Train Epoch 1: 158/159 Loss: 0.448021
2022-12-20 11:55: **********Train Epoch 1: averaged Loss: 0.584997 
2022-12-20 11:55: 
Epoch time elapsed: 13475.628562688828

2022-12-20 12:00: 
 metrics validation: {'precision': 0.8041666666666667, 'recall': 0.14846153846153845, 'f1-score': 0.2506493506493507, 'support': 1300, 'AUC': 0.8429931952662723, 'AUCPR': 0.704133785893572, 'TP': 193, 'FP': 47, 'TN': 2553, 'FN': 1107} 

2022-12-20 12:00: **********Val Epoch 1: average Loss: 0.513399
2022-12-20 12:00: *********************************Current best model saved!
2022-12-20 12:04: 
 Testing metrics {'precision': 0.6923076923076923, 'recall': 0.10260586319218241, 'f1-score': 0.17872340425531916, 'support': 1228, 'AUC': 0.831005004031873, 'AUCPR': 0.6773849788895518, 'TP': 126, 'FP': 56, 'TN': 2400, 'FN': 1102} 

Traceback (most recent call last):
  File "Run_Model.py", line 201, in <module>
    trainer.train()
  File "/home/joel.chacon/tmp/WildFire_GCN/Trainer.py", line 117, in train
    train_epoch_loss = self.train_epoch(epoch)
  File "/home/joel.chacon/tmp/WildFire_GCN/Trainer.py", line 88, in train_epoch
    loss.backward()
  File "/home/joel.chacon/.local/lib/python3.6/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/joel.chacon/.local/lib/python3.6/site-packages/torch/autograd/__init__.py", line 156, in backward
    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
KeyboardInterrupt
