/home/joel.chacon/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2022-12-28 00:10: log dir: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122800103559122527062
2022-12-28 00:10: Experiment log path in: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122800103559122527062
2022-12-28 00:10: Argument: Namespace(batch_size=256, clc='vec', cuda=True, dataset='2020', dataset_root='/home/joel.chacon/tmp/datasets_grl/', debug=False, default_graph=True, device='cpu', dynamic_features=['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh'], early_stop=True, early_stop_patience=8, embed_dim=32, epochs=30, grad_norm=False, horizon=1, input_dim=25, lag=10, link_len=2, log_dir='/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122800103559122527062', log_step=1, loss_func='nllloss', lr_decay=True, lr_decay_rate=0.1, lr_decay_step='15, 20', lr_init=0.0005, max_grad_norm=5, mode='train', model='fire_GCN', nan_fill=0.5, num_layers=1, num_nodes=625, num_workers=20, output_dim=2, patch_height=25, patch_width=25, persistent_workers=True, pin_memory=True, plot=False, positive_weight=0.5, prefetch_factor=2, real_value=True, rnn_units=32, seed=10, static_features=['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density'], teacher_forcing=False, weight_decay=0.0, window_len=10)
2022-12-28 00:10: Argument batch_size: 256
2022-12-28 00:10: Argument clc: 'vec'
2022-12-28 00:10: Argument cuda: True
2022-12-28 00:10: Argument dataset: '2020'
2022-12-28 00:10: Argument dataset_root: '/home/joel.chacon/tmp/datasets_grl/'
2022-12-28 00:10: Argument debug: False
2022-12-28 00:10: Argument default_graph: True
2022-12-28 00:10: Argument device: 'cpu'
2022-12-28 00:10: Argument dynamic_features: ['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh']
2022-12-28 00:10: Argument early_stop: True
2022-12-28 00:10: Argument early_stop_patience: 8
2022-12-28 00:10: Argument embed_dim: 32
2022-12-28 00:10: Argument epochs: 30
2022-12-28 00:10: Argument grad_norm: False
2022-12-28 00:10: Argument horizon: 1
2022-12-28 00:10: Argument input_dim: 25
2022-12-28 00:10: Argument lag: 10
2022-12-28 00:10: Argument link_len: 2
2022-12-28 00:10: Argument log_dir: '/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122800103559122527062'
2022-12-28 00:10: Argument log_step: 1
2022-12-28 00:10: Argument loss_func: 'nllloss'
2022-12-28 00:10: Argument lr_decay: True
2022-12-28 00:10: Argument lr_decay_rate: 0.1
2022-12-28 00:10: Argument lr_decay_step: '15, 20'
2022-12-28 00:10: Argument lr_init: 0.0005
2022-12-28 00:10: Argument max_grad_norm: 5
2022-12-28 00:10: Argument mode: 'train'
2022-12-28 00:10: Argument model: 'fire_GCN'
2022-12-28 00:10: Argument nan_fill: 0.5
2022-12-28 00:10: Argument num_layers: 1
2022-12-28 00:10: Argument num_nodes: 625
2022-12-28 00:10: Argument num_workers: 20
2022-12-28 00:10: Argument output_dim: 2
2022-12-28 00:10: Argument patch_height: 25
2022-12-28 00:10: Argument patch_width: 25
2022-12-28 00:10: Argument persistent_workers: True
2022-12-28 00:10: Argument pin_memory: True
2022-12-28 00:10: Argument plot: False
2022-12-28 00:10: Argument positive_weight: 0.5
2022-12-28 00:10: Argument prefetch_factor: 2
2022-12-28 00:10: Argument real_value: True
2022-12-28 00:10: Argument rnn_units: 32
2022-12-28 00:10: Argument seed: 10
2022-12-28 00:10: Argument static_features: ['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density']
2022-12-28 00:10: Argument teacher_forcing: False
2022-12-28 00:10: Argument weight_decay: 0.0
2022-12-28 00:10: Argument window_len: 10
++++++++++++++
2020_fire_GCN.conf
++++++++++++++
*****************Model Parameter*****************
node_embeddings torch.Size([625, 32]) True
ln1.weight torch.Size([25]) True
ln1.bias torch.Size([25]) True
encoder.cell_list.0.gate.weights_pool torch.Size([32, 2, 57, 32]) True
encoder.cell_list.0.gate.weights_window torch.Size([32, 1, 32]) True
encoder.cell_list.0.gate.bias_pool torch.Size([32, 64]) True
encoder.cell_list.0.gate.T torch.Size([10]) True
encoder.cell_list.0.update.weights_pool torch.Size([32, 2, 57, 16]) True
encoder.cell_list.0.update.weights_window torch.Size([32, 1, 16]) True
encoder.cell_list.0.update.bias_pool torch.Size([32, 32]) True
encoder.cell_list.0.update.T torch.Size([10]) True
end_conv.weight torch.Size([2, 1, 625, 32]) True
end_conv.bias torch.Size([2]) True
Total params num: 239784
*****************Finish Parameter****************
Positives: 13518 / Negatives: 27036
Dataset length 40554
Positives: 1300 / Negatives: 2600
Dataset length 3900
Positives: 1228 / Negatives: 2456
Dataset length 3684
Applying learning rate decay.
Creat Log File in:  /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/2022122800103559122527062/run.log
2022-12-28 00:12: Train Epoch 1: 0/159 Loss: 0.640340
2022-12-28 00:13: Train Epoch 1: 1/159 Loss: 2.302193
2022-12-28 00:14: Train Epoch 1: 2/159 Loss: 1.077096
2022-12-28 00:16: Train Epoch 1: 3/159 Loss: 1.544129
2022-12-28 00:17: Train Epoch 1: 4/159 Loss: 1.050216
2022-12-28 00:18: Train Epoch 1: 5/159 Loss: 0.617109
2022-12-28 00:19: Train Epoch 1: 6/159 Loss: 1.004501
2022-12-28 00:21: Train Epoch 1: 7/159 Loss: 1.206285
2022-12-28 00:22: Train Epoch 1: 8/159 Loss: 1.037813
2022-12-28 00:24: Train Epoch 1: 9/159 Loss: 0.615306
2022-12-28 00:25: Train Epoch 1: 10/159 Loss: 0.851266
2022-12-28 00:26: Train Epoch 1: 11/159 Loss: 1.022543
2022-12-28 00:28: Train Epoch 1: 12/159 Loss: 0.693438
2022-12-28 00:29: Train Epoch 1: 13/159 Loss: 0.643340
2022-12-28 00:30: Train Epoch 1: 14/159 Loss: 0.768888
2022-12-28 00:32: Train Epoch 1: 15/159 Loss: 0.809756
2022-12-28 00:33: Train Epoch 1: 16/159 Loss: 0.697785
2022-12-28 00:34: Train Epoch 1: 17/159 Loss: 0.623281
2022-12-28 00:36: Train Epoch 1: 18/159 Loss: 0.673132
2022-12-28 00:37: Train Epoch 1: 19/159 Loss: 0.764262
2022-12-28 00:38: Train Epoch 1: 20/159 Loss: 0.632605
2022-12-28 00:40: Train Epoch 1: 21/159 Loss: 0.604085
2022-12-28 00:41: Train Epoch 1: 22/159 Loss: 0.703659
2022-12-28 00:42: Train Epoch 1: 23/159 Loss: 0.751146
2022-12-28 00:44: Train Epoch 1: 24/159 Loss: 0.685785
2022-12-28 00:45: Train Epoch 1: 25/159 Loss: 0.585371
2022-12-28 00:47: Train Epoch 1: 26/159 Loss: 0.661641
2022-12-28 00:48: Train Epoch 1: 27/159 Loss: 0.648035
2022-12-28 00:49: Train Epoch 1: 28/159 Loss: 0.597326
2022-12-28 00:51: Train Epoch 1: 29/159 Loss: 0.666794
2022-12-28 00:52: Train Epoch 1: 30/159 Loss: 0.631713
2022-12-28 00:53: Train Epoch 1: 31/159 Loss: 0.570068
2022-12-28 00:55: Train Epoch 1: 32/159 Loss: 0.584370
2022-12-28 00:56: Train Epoch 1: 33/159 Loss: 0.581708
2022-12-28 00:57: Train Epoch 1: 34/159 Loss: 0.588027
2022-12-28 00:59: Train Epoch 1: 35/159 Loss: 0.553753
2022-12-28 01:00: Train Epoch 1: 36/159 Loss: 0.605979
2022-12-28 01:01: Train Epoch 1: 37/159 Loss: 0.574977
2022-12-28 01:03: Train Epoch 1: 38/159 Loss: 0.539251
2022-12-28 01:04: Train Epoch 1: 39/159 Loss: 0.589219
2022-12-28 01:06: Train Epoch 1: 40/159 Loss: 0.569644
2022-12-28 01:07: Train Epoch 1: 41/159 Loss: 0.571904
2022-12-28 01:08: Train Epoch 1: 42/159 Loss: 0.571332
2022-12-28 01:10: Train Epoch 1: 43/159 Loss: 0.560538
2022-12-28 01:11: Train Epoch 1: 44/159 Loss: 0.524324
2022-12-28 01:12: Train Epoch 1: 45/159 Loss: 0.548781
2022-12-28 01:14: Train Epoch 1: 46/159 Loss: 0.530288
2022-12-28 01:15: Train Epoch 1: 47/159 Loss: 0.507083
2022-12-28 01:16: Train Epoch 1: 48/159 Loss: 0.518875
2022-12-28 01:18: Train Epoch 1: 49/159 Loss: 0.497690
2022-12-28 01:19: Train Epoch 1: 50/159 Loss: 0.483382
2022-12-28 01:21: Train Epoch 1: 51/159 Loss: 0.531698
2022-12-28 01:22: Train Epoch 1: 52/159 Loss: 0.507806
2022-12-28 01:23: Train Epoch 1: 53/159 Loss: 0.476015
2022-12-28 01:25: Train Epoch 1: 54/159 Loss: 0.477148
2022-12-28 01:26: Train Epoch 1: 55/159 Loss: 0.473763
2022-12-28 01:27: Train Epoch 1: 56/159 Loss: 0.462822
2022-12-28 01:29: Train Epoch 1: 57/159 Loss: 0.502879
2022-12-28 01:30: Train Epoch 1: 58/159 Loss: 0.508874
2022-12-28 01:31: Train Epoch 1: 59/159 Loss: 0.405755
2022-12-28 01:33: Train Epoch 1: 60/159 Loss: 0.458647
2022-12-28 01:34: Train Epoch 1: 61/159 Loss: 0.491155
2022-12-28 01:36: Train Epoch 1: 62/159 Loss: 0.477080
2022-12-28 01:37: Train Epoch 1: 63/159 Loss: 0.400914
2022-12-28 01:38: Train Epoch 1: 64/159 Loss: 0.402665
2022-12-28 01:40: Train Epoch 1: 65/159 Loss: 0.420647
2022-12-28 01:41: Train Epoch 1: 66/159 Loss: 0.394540
2022-12-28 01:42: Train Epoch 1: 67/159 Loss: 0.397709
2022-12-28 01:44: Train Epoch 1: 68/159 Loss: 0.422928
2022-12-28 01:45: Train Epoch 1: 69/159 Loss: 0.355785
2022-12-28 01:46: Train Epoch 1: 70/159 Loss: 0.433746
2022-12-28 01:48: Train Epoch 1: 71/159 Loss: 0.414377
2022-12-28 01:49: Train Epoch 1: 72/159 Loss: 0.445863
2022-12-28 01:51: Train Epoch 1: 73/159 Loss: 0.386759
2022-12-28 01:52: Train Epoch 1: 74/159 Loss: 0.397977
2022-12-28 01:53: Train Epoch 1: 75/159 Loss: 0.353610
2022-12-28 01:55: Train Epoch 1: 76/159 Loss: 0.411238
2022-12-28 01:56: Train Epoch 1: 77/159 Loss: 0.373554
2022-12-28 01:57: Train Epoch 1: 78/159 Loss: 0.344892
2022-12-28 01:59: Train Epoch 1: 79/159 Loss: 0.375967
2022-12-28 02:00: Train Epoch 1: 80/159 Loss: 0.407010
2022-12-28 02:01: Train Epoch 1: 81/159 Loss: 0.325443
2022-12-28 02:03: Train Epoch 1: 82/159 Loss: 0.382018
2022-12-28 02:04: Train Epoch 1: 83/159 Loss: 0.359529
2022-12-28 02:06: Train Epoch 1: 84/159 Loss: 0.398613
2022-12-28 02:07: Train Epoch 1: 85/159 Loss: 0.363120
2022-12-28 02:08: Train Epoch 1: 86/159 Loss: 0.334145
2022-12-28 02:10: Train Epoch 1: 87/159 Loss: 0.341731
2022-12-28 02:11: Train Epoch 1: 88/159 Loss: 0.373286
2022-12-28 02:12: Train Epoch 1: 89/159 Loss: 0.362235
2022-12-28 02:14: Train Epoch 1: 90/159 Loss: 0.309550
2022-12-28 02:15: Train Epoch 1: 91/159 Loss: 0.372667
2022-12-28 02:16: Train Epoch 1: 92/159 Loss: 0.371518
2022-12-28 02:18: Train Epoch 1: 93/159 Loss: 0.356243
2022-12-28 02:19: Train Epoch 1: 94/159 Loss: 0.363756
2022-12-28 02:21: Train Epoch 1: 95/159 Loss: 0.304346
2022-12-28 02:22: Train Epoch 1: 96/159 Loss: 0.376682
2022-12-28 02:23: Train Epoch 1: 97/159 Loss: 0.271243
2022-12-28 02:25: Train Epoch 1: 98/159 Loss: 0.310249
2022-12-28 02:26: Train Epoch 1: 99/159 Loss: 0.349389
2022-12-28 02:27: Train Epoch 1: 100/159 Loss: 0.352745
2022-12-28 02:29: Train Epoch 1: 101/159 Loss: 0.334700
2022-12-28 02:30: Train Epoch 1: 102/159 Loss: 0.370329
2022-12-28 02:32: Train Epoch 1: 103/159 Loss: 0.265781
2022-12-28 02:33: Train Epoch 1: 104/159 Loss: 0.339152
2022-12-28 02:34: Train Epoch 1: 105/159 Loss: 0.305885
2022-12-28 02:36: Train Epoch 1: 106/159 Loss: 0.301922
2022-12-28 02:37: Train Epoch 1: 107/159 Loss: 0.290645
2022-12-28 02:38: Train Epoch 1: 108/159 Loss: 0.290678
2022-12-28 02:40: Train Epoch 1: 109/159 Loss: 0.321027
2022-12-28 02:41: Train Epoch 1: 110/159 Loss: 0.375281
2022-12-28 02:43: Train Epoch 1: 111/159 Loss: 0.280963
2022-12-28 02:44: Train Epoch 1: 112/159 Loss: 0.357025
2022-12-28 02:45: Train Epoch 1: 113/159 Loss: 0.319355
2022-12-28 02:47: Train Epoch 1: 114/159 Loss: 0.269698
2022-12-28 02:48: Train Epoch 1: 115/159 Loss: 0.421800
2022-12-28 02:49: Train Epoch 1: 116/159 Loss: 0.285263
2022-12-28 02:51: Train Epoch 1: 117/159 Loss: 0.406353
2022-12-28 02:52: Train Epoch 1: 118/159 Loss: 0.342878
2022-12-28 02:53: Train Epoch 1: 119/159 Loss: 0.355836
2022-12-28 02:55: Train Epoch 1: 120/159 Loss: 0.334592
2022-12-28 02:56: Train Epoch 1: 121/159 Loss: 0.274913
2022-12-28 02:58: Train Epoch 1: 122/159 Loss: 0.381169
2022-12-28 02:59: Train Epoch 1: 123/159 Loss: 0.279299
2022-12-28 03:00: Train Epoch 1: 124/159 Loss: 0.331787
2022-12-28 03:02: Train Epoch 1: 125/159 Loss: 0.290090
2022-12-28 03:03: Train Epoch 1: 126/159 Loss: 0.319034
2022-12-28 03:04: Train Epoch 1: 127/159 Loss: 0.324090
2022-12-28 03:06: Train Epoch 1: 128/159 Loss: 0.333965
2022-12-28 03:07: Train Epoch 1: 129/159 Loss: 0.330537
2022-12-28 03:08: Train Epoch 1: 130/159 Loss: 0.376197
2022-12-28 03:10: Train Epoch 1: 131/159 Loss: 0.315656
2022-12-28 03:11: Train Epoch 1: 132/159 Loss: 0.301607
2022-12-28 03:12: Train Epoch 1: 133/159 Loss: 0.286296
2022-12-28 03:14: Train Epoch 1: 134/159 Loss: 0.329200
2022-12-28 03:15: Train Epoch 1: 135/159 Loss: 0.328148
2022-12-28 03:16: Train Epoch 1: 136/159 Loss: 0.308868
2022-12-28 03:18: Train Epoch 1: 137/159 Loss: 0.381371
2022-12-28 03:19: Train Epoch 1: 138/159 Loss: 0.347689
2022-12-28 03:21: Train Epoch 1: 139/159 Loss: 0.322562
2022-12-28 03:22: Train Epoch 1: 140/159 Loss: 0.398144
2022-12-28 03:23: Train Epoch 1: 141/159 Loss: 0.339907
2022-12-28 03:25: Train Epoch 1: 142/159 Loss: 0.300164
2022-12-28 03:26: Train Epoch 1: 143/159 Loss: 0.323061
2022-12-28 03:27: Train Epoch 1: 144/159 Loss: 0.391479
2022-12-28 03:29: Train Epoch 1: 145/159 Loss: 0.256666
2022-12-28 03:30: Train Epoch 1: 146/159 Loss: 0.343544
2022-12-28 03:31: Train Epoch 1: 147/159 Loss: 0.319775
2022-12-28 03:33: Train Epoch 1: 148/159 Loss: 0.306654
2022-12-28 03:34: Train Epoch 1: 149/159 Loss: 0.279245
2022-12-28 03:35: Train Epoch 1: 150/159 Loss: 0.283871
2022-12-28 03:37: Train Epoch 1: 151/159 Loss: 0.296719
2022-12-28 03:38: Train Epoch 1: 152/159 Loss: 0.298999
2022-12-28 03:39: Train Epoch 1: 153/159 Loss: 0.257655
2022-12-28 03:41: Train Epoch 1: 154/159 Loss: 0.267706
2022-12-28 03:42: Train Epoch 1: 155/159 Loss: 0.329683
2022-12-28 03:43: Train Epoch 1: 156/159 Loss: 0.345044
2022-12-28 03:45: Train Epoch 1: 157/159 Loss: 0.263395
2022-12-28 03:45: Train Epoch 1: 158/159 Loss: 0.265658
2022-12-28 03:45: **********Train Epoch 1: averaged Loss: 0.474046 
2022-12-28 03:45: 
Epoch time elapsed: 12905.48510479927

2022-12-28 03:51: 
 metrics validation: {'precision': 0.658465991316932, 'recall': 0.7, 'f1-score': 0.6785980611483967, 'support': 1300, 'AUC': 0.8228073964497041, 'AUCPR': 0.7521148015870356, 'TP': 910, 'FP': 472, 'TN': 2128, 'FN': 390} 

2022-12-28 03:51: **********Val Epoch 1: average Loss: 0.569043
2022-12-28 03:51: *********************************Current best model saved!
/home/joel.chacon/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2022-12-28 03:57: 
 Testing metrics {'precision': 0.7290969899665551, 'recall': 0.7100977198697068, 'f1-score': 0.7194719471947194, 'support': 1228, 'AUC': 0.8575027984381798, 'AUCPR': 0.7840645075699583, 'TP': 872, 'FP': 324, 'TN': 2132, 'FN': 356} 

2022-12-28 03:59: Train Epoch 2: 0/159 Loss: 0.313504
2022-12-28 04:00: Train Epoch 2: 1/159 Loss: 0.305053
2022-12-28 04:01: Train Epoch 2: 2/159 Loss: 0.328246
2022-12-28 04:03: Train Epoch 2: 3/159 Loss: 0.309726
2022-12-28 04:04: Train Epoch 2: 4/159 Loss: 0.303413
2022-12-28 04:05: Train Epoch 2: 5/159 Loss: 0.268297
2022-12-28 04:07: Train Epoch 2: 6/159 Loss: 0.391928
2022-12-28 04:08: Train Epoch 2: 7/159 Loss: 0.347099
2022-12-28 04:09: Train Epoch 2: 8/159 Loss: 0.316805
2022-12-28 04:11: Train Epoch 2: 9/159 Loss: 0.288181
2022-12-28 04:12: Train Epoch 2: 10/159 Loss: 0.238905
2022-12-28 04:14: Train Epoch 2: 11/159 Loss: 0.297625
2022-12-28 04:15: Train Epoch 2: 12/159 Loss: 0.258620
2022-12-28 04:16: Train Epoch 2: 13/159 Loss: 0.250738
2022-12-28 04:18: Train Epoch 2: 14/159 Loss: 0.273083
2022-12-28 04:19: Train Epoch 2: 15/159 Loss: 0.342810
2022-12-28 04:21: Train Epoch 2: 16/159 Loss: 0.311683
2022-12-28 04:22: Train Epoch 2: 17/159 Loss: 0.300490
2022-12-28 04:23: Train Epoch 2: 18/159 Loss: 0.254572
2022-12-28 04:25: Train Epoch 2: 19/159 Loss: 0.242941
2022-12-28 04:26: Train Epoch 2: 20/159 Loss: 0.259544
2022-12-28 04:28: Train Epoch 2: 21/159 Loss: 0.359135
2022-12-28 04:29: Train Epoch 2: 22/159 Loss: 0.270647
2022-12-28 04:30: Train Epoch 2: 23/159 Loss: 0.312714
2022-12-28 04:32: Train Epoch 2: 24/159 Loss: 0.270101
2022-12-28 04:33: Train Epoch 2: 25/159 Loss: 0.316508
2022-12-28 04:34: Train Epoch 2: 26/159 Loss: 0.285566
2022-12-28 04:36: Train Epoch 2: 27/159 Loss: 0.253205
2022-12-28 04:37: Train Epoch 2: 28/159 Loss: 0.276043
2022-12-28 04:39: Train Epoch 2: 29/159 Loss: 0.272890
2022-12-28 04:40: Train Epoch 2: 30/159 Loss: 0.385208
2022-12-28 04:41: Train Epoch 2: 31/159 Loss: 0.325413
2022-12-28 04:43: Train Epoch 2: 32/159 Loss: 0.281570
2022-12-28 04:44: Train Epoch 2: 33/159 Loss: 0.273940
2022-12-28 04:45: Train Epoch 2: 34/159 Loss: 0.273556
2022-12-28 04:47: Train Epoch 2: 35/159 Loss: 0.297362
2022-12-28 04:48: Train Epoch 2: 36/159 Loss: 0.241274
2022-12-28 04:50: Train Epoch 2: 37/159 Loss: 0.249589
2022-12-28 04:51: Train Epoch 2: 38/159 Loss: 0.324811
2022-12-28 04:52: Train Epoch 2: 39/159 Loss: 0.340953
2022-12-28 04:54: Train Epoch 2: 40/159 Loss: 0.260219
2022-12-28 04:55: Train Epoch 2: 41/159 Loss: 0.333363
2022-12-28 04:57: Train Epoch 2: 42/159 Loss: 0.349521
2022-12-28 04:58: Train Epoch 2: 43/159 Loss: 0.249640
2022-12-28 04:59: Train Epoch 2: 44/159 Loss: 0.298826
2022-12-28 05:01: Train Epoch 2: 45/159 Loss: 0.305189
2022-12-28 05:02: Train Epoch 2: 46/159 Loss: 0.281562
2022-12-28 05:03: Train Epoch 2: 47/159 Loss: 0.328315
2022-12-28 05:05: Train Epoch 2: 48/159 Loss: 0.308678
2022-12-28 05:06: Train Epoch 2: 49/159 Loss: 0.295050
2022-12-28 05:08: Train Epoch 2: 50/159 Loss: 0.311969
2022-12-28 05:09: Train Epoch 2: 51/159 Loss: 0.273734
2022-12-28 05:10: Train Epoch 2: 52/159 Loss: 0.184772
2022-12-28 05:12: Train Epoch 2: 53/159 Loss: 0.285098
2022-12-28 05:13: Train Epoch 2: 54/159 Loss: 0.256621
2022-12-28 05:14: Train Epoch 2: 55/159 Loss: 0.321115
2022-12-28 05:16: Train Epoch 2: 56/159 Loss: 0.276969
2022-12-28 05:17: Train Epoch 2: 57/159 Loss: 0.312462
2022-12-28 05:18: Train Epoch 2: 58/159 Loss: 0.238195
2022-12-28 05:20: Train Epoch 2: 59/159 Loss: 0.284121
2022-12-28 05:21: Train Epoch 2: 60/159 Loss: 0.328406
2022-12-28 05:23: Train Epoch 2: 61/159 Loss: 0.288770
2022-12-28 05:24: Train Epoch 2: 62/159 Loss: 0.344285
2022-12-28 05:25: Train Epoch 2: 63/159 Loss: 0.278942
2022-12-28 05:27: Train Epoch 2: 64/159 Loss: 0.318542
2022-12-28 05:28: Train Epoch 2: 65/159 Loss: 0.291422
2022-12-28 05:29: Train Epoch 2: 66/159 Loss: 0.276469
2022-12-28 05:31: Train Epoch 2: 67/159 Loss: 0.252088
2022-12-28 05:32: Train Epoch 2: 68/159 Loss: 0.256903
2022-12-28 05:34: Train Epoch 2: 69/159 Loss: 0.341655
2022-12-28 05:35: Train Epoch 2: 70/159 Loss: 0.287417
2022-12-28 05:36: Train Epoch 2: 71/159 Loss: 0.218013
2022-12-28 05:38: Train Epoch 2: 72/159 Loss: 0.238343
2022-12-28 05:39: Train Epoch 2: 73/159 Loss: 0.320779
2022-12-28 05:40: Train Epoch 2: 74/159 Loss: 0.339472
2022-12-28 05:42: Train Epoch 2: 75/159 Loss: 0.223899
2022-12-28 05:43: Train Epoch 2: 76/159 Loss: 0.327937
2022-12-28 05:45: Train Epoch 2: 77/159 Loss: 0.332330
2022-12-28 05:46: Train Epoch 2: 78/159 Loss: 0.222175
2022-12-28 05:47: Train Epoch 2: 79/159 Loss: 0.336178
2022-12-28 05:49: Train Epoch 2: 80/159 Loss: 0.268215
2022-12-28 05:50: Train Epoch 2: 81/159 Loss: 0.263291
2022-12-28 05:51: Train Epoch 2: 82/159 Loss: 0.290622
2022-12-28 05:53: Train Epoch 2: 83/159 Loss: 0.268623
2022-12-28 05:54: Train Epoch 2: 84/159 Loss: 0.399725
2022-12-28 05:56: Train Epoch 2: 85/159 Loss: 0.292836
2022-12-28 05:57: Train Epoch 2: 86/159 Loss: 0.240663
2022-12-28 05:58: Train Epoch 2: 87/159 Loss: 0.286283
2022-12-28 06:00: Train Epoch 2: 88/159 Loss: 0.310544
2022-12-28 06:01: Train Epoch 2: 89/159 Loss: 0.314305
2022-12-28 06:02: Train Epoch 2: 90/159 Loss: 0.296226
2022-12-28 06:04: Train Epoch 2: 91/159 Loss: 0.302166
2022-12-28 06:05: Train Epoch 2: 92/159 Loss: 0.358195
2022-12-28 06:07: Train Epoch 2: 93/159 Loss: 0.298612
2022-12-28 06:08: Train Epoch 2: 94/159 Loss: 0.304849
2022-12-28 06:09: Train Epoch 2: 95/159 Loss: 0.293743
2022-12-28 06:11: Train Epoch 2: 96/159 Loss: 0.246735
2022-12-28 06:12: Train Epoch 2: 97/159 Loss: 0.238323
2022-12-28 06:14: Train Epoch 2: 98/159 Loss: 0.270061
2022-12-28 06:15: Train Epoch 2: 99/159 Loss: 0.285497
2022-12-28 06:16: Train Epoch 2: 100/159 Loss: 0.322221
2022-12-28 06:18: Train Epoch 2: 101/159 Loss: 0.385845
2022-12-28 06:19: Train Epoch 2: 102/159 Loss: 0.258720
2022-12-28 06:21: Train Epoch 2: 103/159 Loss: 0.266171
2022-12-28 06:22: Train Epoch 2: 104/159 Loss: 0.345813
2022-12-28 06:23: Train Epoch 2: 105/159 Loss: 0.243618
2022-12-28 06:25: Train Epoch 2: 106/159 Loss: 0.278983
2022-12-28 06:26: Train Epoch 2: 107/159 Loss: 0.336477
2022-12-28 06:28: Train Epoch 2: 108/159 Loss: 0.250943
2022-12-28 06:29: Train Epoch 2: 109/159 Loss: 0.248091
2022-12-28 06:30: Train Epoch 2: 110/159 Loss: 0.266837
2022-12-28 06:32: Train Epoch 2: 111/159 Loss: 0.326675
2022-12-28 06:33: Train Epoch 2: 112/159 Loss: 0.245497
2022-12-28 06:34: Train Epoch 2: 113/159 Loss: 0.272962
2022-12-28 06:36: Train Epoch 2: 114/159 Loss: 0.270204
2022-12-28 06:37: Train Epoch 2: 115/159 Loss: 0.207583
2022-12-28 06:38: Train Epoch 2: 116/159 Loss: 0.280969
2022-12-28 06:40: Train Epoch 2: 117/159 Loss: 0.338337
2022-12-28 06:41: Train Epoch 2: 118/159 Loss: 0.262041
2022-12-28 06:43: Train Epoch 2: 119/159 Loss: 0.294200
2022-12-28 06:44: Train Epoch 2: 120/159 Loss: 0.413644
2022-12-28 06:45: Train Epoch 2: 121/159 Loss: 0.275412
2022-12-28 06:47: Train Epoch 2: 122/159 Loss: 0.277979
2022-12-28 06:48: Train Epoch 2: 123/159 Loss: 0.325564
2022-12-28 06:49: Train Epoch 2: 124/159 Loss: 0.295068
2022-12-28 06:51: Train Epoch 2: 125/159 Loss: 0.226454
2022-12-28 06:52: Train Epoch 2: 126/159 Loss: 0.367144
2022-12-28 06:54: Train Epoch 2: 127/159 Loss: 0.390990
2022-12-28 06:55: Train Epoch 2: 128/159 Loss: 0.250534
2022-12-28 06:56: Train Epoch 2: 129/159 Loss: 0.378876
2022-12-28 06:58: Train Epoch 2: 130/159 Loss: 0.292207
2022-12-28 06:59: Train Epoch 2: 131/159 Loss: 0.255994
2022-12-28 07:01: Train Epoch 2: 132/159 Loss: 0.295259
2022-12-28 07:02: Train Epoch 2: 133/159 Loss: 0.338206
2022-12-28 07:03: Train Epoch 2: 134/159 Loss: 0.358820
2022-12-28 07:05: Train Epoch 2: 135/159 Loss: 0.371219
2022-12-28 07:06: Train Epoch 2: 136/159 Loss: 0.276232
2022-12-28 07:08: Train Epoch 2: 137/159 Loss: 0.254933
2022-12-28 07:09: Train Epoch 2: 138/159 Loss: 0.229103
2022-12-28 07:10: Train Epoch 2: 139/159 Loss: 0.334612
2022-12-28 07:12: Train Epoch 2: 140/159 Loss: 0.380038
2022-12-28 07:13: Train Epoch 2: 141/159 Loss: 0.283086
2022-12-28 07:15: Train Epoch 2: 142/159 Loss: 0.293225
2022-12-28 07:16: Train Epoch 2: 143/159 Loss: 0.315374
2022-12-28 07:17: Train Epoch 2: 144/159 Loss: 0.264567
2022-12-28 07:19: Train Epoch 2: 145/159 Loss: 0.298232
2022-12-28 07:20: Train Epoch 2: 146/159 Loss: 0.297038
2022-12-28 07:21: Train Epoch 2: 147/159 Loss: 0.292994
2022-12-28 07:23: Train Epoch 2: 148/159 Loss: 0.344091
2022-12-28 07:24: Train Epoch 2: 149/159 Loss: 0.291289
2022-12-28 07:26: Train Epoch 2: 150/159 Loss: 0.263522
2022-12-28 07:27: Train Epoch 2: 151/159 Loss: 0.262205
2022-12-28 07:28: Train Epoch 2: 152/159 Loss: 0.260760
2022-12-28 07:30: Train Epoch 2: 153/159 Loss: 0.398241
2022-12-28 07:31: Train Epoch 2: 154/159 Loss: 0.237115
2022-12-28 07:32: Train Epoch 2: 155/159 Loss: 0.336820
2022-12-28 07:34: Train Epoch 2: 156/159 Loss: 0.325371
2022-12-28 07:35: Train Epoch 2: 157/159 Loss: 0.264239
2022-12-28 07:36: Train Epoch 2: 158/159 Loss: 0.339587
2022-12-28 07:36: **********Train Epoch 2: averaged Loss: 0.294849 
2022-12-28 07:36: 
Epoch time elapsed: 13128.141125202179

2022-12-28 07:41: 
 metrics validation: {'precision': 0.8392018779342723, 'recall': 0.55, 'f1-score': 0.6644981412639405, 'support': 1300, 'AUC': 0.8458991124260355, 'AUCPR': 0.7794621581968816, 'TP': 715, 'FP': 137, 'TN': 2463, 'FN': 585} 

2022-12-28 07:41: **********Val Epoch 2: average Loss: 0.602370
2022-12-28 07:46: 
 Testing metrics {'precision': 0.7290969899665551, 'recall': 0.7100977198697068, 'f1-score': 0.7194719471947194, 'support': 1228, 'AUC': 0.8575027984381798, 'AUCPR': 0.7840645075699583, 'TP': 872, 'FP': 324, 'TN': 2132, 'FN': 356} 

2022-12-28 07:48: Train Epoch 3: 0/159 Loss: 0.385850
2022-12-28 07:49: Train Epoch 3: 1/159 Loss: 0.327293
2022-12-28 07:51: Train Epoch 3: 2/159 Loss: 0.460143
2022-12-28 07:52: Train Epoch 3: 3/159 Loss: 0.326555
2022-12-28 07:53: Train Epoch 3: 4/159 Loss: 0.360261
2022-12-28 07:55: Train Epoch 3: 5/159 Loss: 0.460403
2022-12-28 07:56: Train Epoch 3: 6/159 Loss: 0.286053
2022-12-28 07:58: Train Epoch 3: 7/159 Loss: 0.301001
2022-12-28 07:59: Train Epoch 3: 8/159 Loss: 0.369987
2022-12-28 08:00: Train Epoch 3: 9/159 Loss: 0.410525
2022-12-28 08:02: Train Epoch 3: 10/159 Loss: 0.280812
2022-12-28 08:03: Train Epoch 3: 11/159 Loss: 0.363138
2022-12-28 08:04: Train Epoch 3: 12/159 Loss: 0.349841
2022-12-28 08:06: Train Epoch 3: 13/159 Loss: 0.318040
2022-12-28 08:07: Train Epoch 3: 14/159 Loss: 0.319377
2022-12-28 08:09: Train Epoch 3: 15/159 Loss: 0.388651
2022-12-28 08:10: Train Epoch 3: 16/159 Loss: 0.235145
2022-12-28 08:11: Train Epoch 3: 17/159 Loss: 0.319969
2022-12-28 08:13: Train Epoch 3: 18/159 Loss: 0.326251
2022-12-28 08:14: Train Epoch 3: 19/159 Loss: 0.340613
2022-12-28 08:15: Train Epoch 3: 20/159 Loss: 0.265911
2022-12-28 08:17: Train Epoch 3: 21/159 Loss: 0.348399
2022-12-28 08:18: Train Epoch 3: 22/159 Loss: 0.351842
2022-12-28 08:20: Train Epoch 3: 23/159 Loss: 0.349908
2022-12-28 08:21: Train Epoch 3: 24/159 Loss: 0.318006
2022-12-28 08:22: Train Epoch 3: 25/159 Loss: 0.291263
2022-12-28 08:24: Train Epoch 3: 26/159 Loss: 0.339705
2022-12-28 08:25: Train Epoch 3: 27/159 Loss: 0.284328
2022-12-28 08:26: Train Epoch 3: 28/159 Loss: 0.257189
2022-12-28 08:28: Train Epoch 3: 29/159 Loss: 0.292228
2022-12-28 08:29: Train Epoch 3: 30/159 Loss: 0.294668
2022-12-28 08:31: Train Epoch 3: 31/159 Loss: 0.284794
2022-12-28 08:32: Train Epoch 3: 32/159 Loss: 0.279088
2022-12-28 08:33: Train Epoch 3: 33/159 Loss: 0.246751
2022-12-28 08:35: Train Epoch 3: 34/159 Loss: 0.279248
2022-12-28 08:36: Train Epoch 3: 35/159 Loss: 0.284584
2022-12-28 08:37: Train Epoch 3: 36/159 Loss: 0.254194
2022-12-28 08:39: Train Epoch 3: 37/159 Loss: 0.346997
2022-12-28 08:40: Train Epoch 3: 38/159 Loss: 0.249642
2022-12-28 08:42: Train Epoch 3: 39/159 Loss: 0.322498
2022-12-28 08:43: Train Epoch 3: 40/159 Loss: 0.313003
2022-12-28 08:44: Train Epoch 3: 41/159 Loss: 0.254826
2022-12-28 08:46: Train Epoch 3: 42/159 Loss: 0.307913
2022-12-28 08:47: Train Epoch 3: 43/159 Loss: 0.311105
2022-12-28 08:48: Train Epoch 3: 44/159 Loss: 0.285162
2022-12-28 08:50: Train Epoch 3: 45/159 Loss: 0.376950
2022-12-28 08:51: Train Epoch 3: 46/159 Loss: 0.294493
2022-12-28 08:53: Train Epoch 3: 47/159 Loss: 0.271381
2022-12-28 08:54: Train Epoch 3: 48/159 Loss: 0.331103
2022-12-28 08:55: Train Epoch 3: 49/159 Loss: 0.339160
2022-12-28 08:57: Train Epoch 3: 50/159 Loss: 0.383840
2022-12-28 08:58: Train Epoch 3: 51/159 Loss: 0.225339
2022-12-28 08:59: Train Epoch 3: 52/159 Loss: 0.311590
2022-12-28 09:01: Train Epoch 3: 53/159 Loss: 0.332008
2022-12-28 09:02: Train Epoch 3: 54/159 Loss: 0.277195
2022-12-28 09:03: Train Epoch 3: 55/159 Loss: 0.326657
2022-12-28 09:05: Train Epoch 3: 56/159 Loss: 0.274099
2022-12-28 09:06: Train Epoch 3: 57/159 Loss: 0.250177
2022-12-28 09:08: Train Epoch 3: 58/159 Loss: 0.292741
2022-12-28 09:09: Train Epoch 3: 59/159 Loss: 0.268881
2022-12-28 09:10: Train Epoch 3: 60/159 Loss: 0.309718
2022-12-28 09:12: Train Epoch 3: 61/159 Loss: 0.293048
2022-12-28 09:13: Train Epoch 3: 62/159 Loss: 0.298941
2022-12-28 09:14: Train Epoch 3: 63/159 Loss: 0.319996
2022-12-28 09:16: Train Epoch 3: 64/159 Loss: 0.259386
2022-12-28 09:17: Train Epoch 3: 65/159 Loss: 0.258774
2022-12-28 09:19: Train Epoch 3: 66/159 Loss: 0.242657
2022-12-28 09:20: Train Epoch 3: 67/159 Loss: 0.248425
2022-12-28 09:21: Train Epoch 3: 68/159 Loss: 0.211963
2022-12-28 09:23: Train Epoch 3: 69/159 Loss: 0.318153
2022-12-28 09:24: Train Epoch 3: 70/159 Loss: 0.304298
2022-12-28 09:25: Train Epoch 3: 71/159 Loss: 0.335740
2022-12-28 09:27: Train Epoch 3: 72/159 Loss: 0.364297
2022-12-28 09:28: Train Epoch 3: 73/159 Loss: 0.256538
2022-12-28 09:29: Train Epoch 3: 74/159 Loss: 0.312166
2022-12-28 09:31: Train Epoch 3: 75/159 Loss: 0.260425
2022-12-28 09:32: Train Epoch 3: 76/159 Loss: 0.345512
2022-12-28 09:34: Train Epoch 3: 77/159 Loss: 0.349095
2022-12-28 09:35: Train Epoch 3: 78/159 Loss: 0.290035
2022-12-28 09:36: Train Epoch 3: 79/159 Loss: 0.286239
2022-12-28 09:38: Train Epoch 3: 80/159 Loss: 0.253147
2022-12-28 09:39: Train Epoch 3: 81/159 Loss: 0.335130
2022-12-28 09:40: Train Epoch 3: 82/159 Loss: 0.266938
2022-12-28 09:42: Train Epoch 3: 83/159 Loss: 0.338699
2022-12-28 09:43: Train Epoch 3: 84/159 Loss: 0.269938
2022-12-28 09:44: Train Epoch 3: 85/159 Loss: 0.298336
2022-12-28 09:46: Train Epoch 3: 86/159 Loss: 0.264829
2022-12-28 09:47: Train Epoch 3: 87/159 Loss: 0.300826
2022-12-28 09:49: Train Epoch 3: 88/159 Loss: 0.286472
2022-12-28 09:50: Train Epoch 3: 89/159 Loss: 0.243399
2022-12-28 09:51: Train Epoch 3: 90/159 Loss: 0.302971
2022-12-28 09:53: Train Epoch 3: 91/159 Loss: 0.289225
2022-12-28 09:54: Train Epoch 3: 92/159 Loss: 0.374585
2022-12-28 09:55: Train Epoch 3: 93/159 Loss: 0.311973
2022-12-28 09:57: Train Epoch 3: 94/159 Loss: 0.286254
2022-12-28 09:58: Train Epoch 3: 95/159 Loss: 0.275339
2022-12-28 10:00: Train Epoch 3: 96/159 Loss: 0.247671
2022-12-28 10:01: Train Epoch 3: 97/159 Loss: 0.338826
2022-12-28 10:02: Train Epoch 3: 98/159 Loss: 0.388334
2022-12-28 10:04: Train Epoch 3: 99/159 Loss: 0.301856
2022-12-28 10:05: Train Epoch 3: 100/159 Loss: 0.327467
2022-12-28 10:06: Train Epoch 3: 101/159 Loss: 0.306592
2022-12-28 10:08: Train Epoch 3: 102/159 Loss: 0.232418
2022-12-28 10:09: Train Epoch 3: 103/159 Loss: 0.285798
2022-12-28 10:10: Train Epoch 3: 104/159 Loss: 0.305549
2022-12-28 10:12: Train Epoch 3: 105/159 Loss: 0.334375
2022-12-28 10:13: Train Epoch 3: 106/159 Loss: 0.264982
2022-12-28 10:15: Train Epoch 3: 107/159 Loss: 0.341597
2022-12-28 10:16: Train Epoch 3: 108/159 Loss: 0.322743
2022-12-28 10:17: Train Epoch 3: 109/159 Loss: 0.340889
2022-12-28 10:19: Train Epoch 3: 110/159 Loss: 0.300774
2022-12-28 10:20: Train Epoch 3: 111/159 Loss: 0.405477
2022-12-28 10:22: Train Epoch 3: 112/159 Loss: 0.371430
2022-12-28 10:23: Train Epoch 3: 113/159 Loss: 0.271874
2022-12-28 10:24: Train Epoch 3: 114/159 Loss: 0.362032
2022-12-28 10:26: Train Epoch 3: 115/159 Loss: 0.238261
2022-12-28 10:27: Train Epoch 3: 116/159 Loss: 0.308397
2022-12-28 10:28: Train Epoch 3: 117/159 Loss: 0.277113
2022-12-28 10:30: Train Epoch 3: 118/159 Loss: 0.330447
2022-12-28 10:31: Train Epoch 3: 119/159 Loss: 0.325439
2022-12-28 10:32: Train Epoch 3: 120/159 Loss: 0.278597
2022-12-28 10:34: Train Epoch 3: 121/159 Loss: 0.312644
2022-12-28 10:35: Train Epoch 3: 122/159 Loss: 0.309199
2022-12-28 10:37: Train Epoch 3: 123/159 Loss: 0.294420
2022-12-28 10:38: Train Epoch 3: 124/159 Loss: 0.509052
2022-12-28 10:39: Train Epoch 3: 125/159 Loss: 0.234239
2022-12-28 10:41: Train Epoch 3: 126/159 Loss: 0.251377
2022-12-28 10:42: Train Epoch 3: 127/159 Loss: 0.301208
2022-12-28 10:43: Train Epoch 3: 128/159 Loss: 0.315223
2022-12-28 10:45: Train Epoch 3: 129/159 Loss: 0.289131
2022-12-28 10:46: Train Epoch 3: 130/159 Loss: 0.311925
2022-12-28 10:47: Train Epoch 3: 131/159 Loss: 0.270858
2022-12-28 10:49: Train Epoch 3: 132/159 Loss: 0.260384
2022-12-28 10:50: Train Epoch 3: 133/159 Loss: 0.259787
2022-12-28 10:52: Train Epoch 3: 134/159 Loss: 0.336166
2022-12-28 10:53: Train Epoch 3: 135/159 Loss: 0.382370
2022-12-28 10:54: Train Epoch 3: 136/159 Loss: 0.268386
2022-12-28 10:56: Train Epoch 3: 137/159 Loss: 0.389903
2022-12-28 10:57: Train Epoch 3: 138/159 Loss: 0.269968
2022-12-28 10:58: Train Epoch 3: 139/159 Loss: 0.234366
2022-12-28 11:00: Train Epoch 3: 140/159 Loss: 0.244648
2022-12-28 11:01: Train Epoch 3: 141/159 Loss: 0.293536
2022-12-28 11:03: Train Epoch 3: 142/159 Loss: 0.285038
2022-12-28 11:04: Train Epoch 3: 143/159 Loss: 0.278885
2022-12-28 11:05: Train Epoch 3: 144/159 Loss: 0.278087
2022-12-28 11:07: Train Epoch 3: 145/159 Loss: 0.378904
2022-12-28 11:08: Train Epoch 3: 146/159 Loss: 0.335016
2022-12-28 11:09: Train Epoch 3: 147/159 Loss: 0.293033
2022-12-28 11:11: Train Epoch 3: 148/159 Loss: 0.240900
2022-12-28 11:12: Train Epoch 3: 149/159 Loss: 0.375522
2022-12-28 11:14: Train Epoch 3: 150/159 Loss: 0.241089
2022-12-28 11:15: Train Epoch 3: 151/159 Loss: 0.268354
2022-12-28 11:16: Train Epoch 3: 152/159 Loss: 0.330803
2022-12-28 11:18: Train Epoch 3: 153/159 Loss: 0.314918
2022-12-28 11:19: Train Epoch 3: 154/159 Loss: 0.336948
2022-12-28 11:20: Train Epoch 3: 155/159 Loss: 0.367783
2022-12-28 11:22: Train Epoch 3: 156/159 Loss: 0.253389
2022-12-28 11:23: Train Epoch 3: 157/159 Loss: 0.287309
2022-12-28 11:24: Train Epoch 3: 158/159 Loss: 0.226663
2022-12-28 11:24: **********Train Epoch 3: averaged Loss: 0.306306 
2022-12-28 11:24: 
Epoch time elapsed: 13026.03653550148

2022-12-28 11:29: 
 metrics validation: {'precision': 0.8511166253101737, 'recall': 0.5276923076923077, 'f1-score': 0.6514719848053182, 'support': 1300, 'AUC': 0.8405286982248521, 'AUCPR': 0.7728138311940446, 'TP': 686, 'FP': 120, 'TN': 2480, 'FN': 614} 

2022-12-28 11:29: **********Val Epoch 3: average Loss: 0.643307
2022-12-28 11:34: 
 Testing metrics {'precision': 0.7290969899665551, 'recall': 0.7100977198697068, 'f1-score': 0.7194719471947194, 'support': 1228, 'AUC': 0.8575027984381798, 'AUCPR': 0.7840645075699583, 'TP': 872, 'FP': 324, 'TN': 2132, 'FN': 356} 

2022-12-28 11:36: Train Epoch 4: 0/159 Loss: 0.276213
2022-12-28 11:37: Train Epoch 4: 1/159 Loss: 0.279964
2022-12-28 11:39: Train Epoch 4: 2/159 Loss: 0.281361
2022-12-28 11:40: Train Epoch 4: 3/159 Loss: 0.392835
2022-12-28 11:41: Train Epoch 4: 4/159 Loss: 0.345816
2022-12-28 11:43: Train Epoch 4: 5/159 Loss: 0.335092
2022-12-28 11:44: Train Epoch 4: 6/159 Loss: 0.288939
2022-12-28 11:45: Train Epoch 4: 7/159 Loss: 0.340869
2022-12-28 11:47: Train Epoch 4: 8/159 Loss: 0.294965
2022-12-28 11:48: Train Epoch 4: 9/159 Loss: 0.299734
2022-12-28 11:50: Train Epoch 4: 10/159 Loss: 0.283684
2022-12-28 11:51: Train Epoch 4: 11/159 Loss: 0.325311
2022-12-28 11:52: Train Epoch 4: 12/159 Loss: 0.280280
2022-12-28 11:54: Train Epoch 4: 13/159 Loss: 0.233472
2022-12-28 11:55: Train Epoch 4: 14/159 Loss: 0.287499
2022-12-28 11:56: Train Epoch 4: 15/159 Loss: 0.333290
2022-12-28 11:58: Train Epoch 4: 16/159 Loss: 0.290983
2022-12-28 11:59: Train Epoch 4: 17/159 Loss: 0.381642
2022-12-28 12:01: Train Epoch 4: 18/159 Loss: 0.328083
2022-12-28 12:02: Train Epoch 4: 19/159 Loss: 0.246897
2022-12-28 12:03: Train Epoch 4: 20/159 Loss: 0.292121
2022-12-28 12:05: Train Epoch 4: 21/159 Loss: 0.274760
2022-12-28 12:06: Train Epoch 4: 22/159 Loss: 0.309657
2022-12-28 12:08: Train Epoch 4: 23/159 Loss: 0.308790
2022-12-28 12:09: Train Epoch 4: 24/159 Loss: 0.319745
2022-12-28 12:10: Train Epoch 4: 25/159 Loss: 0.281524
2022-12-28 12:12: Train Epoch 4: 26/159 Loss: 0.315917
2022-12-28 12:13: Train Epoch 4: 27/159 Loss: 0.257169
2022-12-28 12:14: Train Epoch 4: 28/159 Loss: 0.258698
2022-12-28 12:16: Train Epoch 4: 29/159 Loss: 0.331005
2022-12-28 12:17: Train Epoch 4: 30/159 Loss: 0.280998
2022-12-28 12:19: Train Epoch 4: 31/159 Loss: 0.312233
2022-12-28 12:20: Train Epoch 4: 32/159 Loss: 0.318727
2022-12-28 12:21: Train Epoch 4: 33/159 Loss: 0.368294
2022-12-28 12:23: Train Epoch 4: 34/159 Loss: 0.222102
2022-12-28 12:24: Train Epoch 4: 35/159 Loss: 0.354574
2022-12-28 12:25: Train Epoch 4: 36/159 Loss: 0.305284
2022-12-28 12:27: Train Epoch 4: 37/159 Loss: 0.358138
2022-12-28 12:28: Train Epoch 4: 38/159 Loss: 0.267921
2022-12-28 12:30: Train Epoch 4: 39/159 Loss: 0.278840
2022-12-28 12:31: Train Epoch 4: 40/159 Loss: 0.250457
2022-12-28 12:32: Train Epoch 4: 41/159 Loss: 0.306225
2022-12-28 12:34: Train Epoch 4: 42/159 Loss: 0.327685
2022-12-28 12:35: Train Epoch 4: 43/159 Loss: 0.309532
2022-12-28 12:36: Train Epoch 4: 44/159 Loss: 0.347063
2022-12-28 12:38: Train Epoch 4: 45/159 Loss: 0.353641
2022-12-28 12:39: Train Epoch 4: 46/159 Loss: 0.348837
2022-12-28 12:40: Train Epoch 4: 47/159 Loss: 0.271257
2022-12-28 12:42: Train Epoch 4: 48/159 Loss: 0.295489
2022-12-28 12:43: Train Epoch 4: 49/159 Loss: 0.290270
2022-12-28 12:45: Train Epoch 4: 50/159 Loss: 0.302265
2022-12-28 12:46: Train Epoch 4: 51/159 Loss: 0.338615
2022-12-28 12:47: Train Epoch 4: 52/159 Loss: 0.233964
2022-12-28 12:49: Train Epoch 4: 53/159 Loss: 0.361998
2022-12-28 12:50: Train Epoch 4: 54/159 Loss: 0.306472
2022-12-28 12:52: Train Epoch 4: 55/159 Loss: 0.289634
2022-12-28 12:53: Train Epoch 4: 56/159 Loss: 0.244144
2022-12-28 12:54: Train Epoch 4: 57/159 Loss: 0.288523
2022-12-28 12:56: Train Epoch 4: 58/159 Loss: 0.279843
2022-12-28 12:57: Train Epoch 4: 59/159 Loss: 0.254219
2022-12-28 12:58: Train Epoch 4: 60/159 Loss: 0.306825
2022-12-28 13:00: Train Epoch 4: 61/159 Loss: 0.305502
2022-12-28 13:01: Train Epoch 4: 62/159 Loss: 0.320376
2022-12-28 13:03: Train Epoch 4: 63/159 Loss: 0.229456
2022-12-28 13:04: Train Epoch 4: 64/159 Loss: 0.354159
2022-12-28 13:05: Train Epoch 4: 65/159 Loss: 0.267787
2022-12-28 13:07: Train Epoch 4: 66/159 Loss: 0.287049
2022-12-28 13:08: Train Epoch 4: 67/159 Loss: 0.275413
2022-12-28 13:09: Train Epoch 4: 68/159 Loss: 0.296177
2022-12-28 13:11: Train Epoch 4: 69/159 Loss: 0.297117
2022-12-28 13:12: Train Epoch 4: 70/159 Loss: 0.286933
2022-12-28 13:13: Train Epoch 4: 71/159 Loss: 0.231983
2022-12-28 13:15: Train Epoch 4: 72/159 Loss: 0.324509
2022-12-28 13:16: Train Epoch 4: 73/159 Loss: 0.263143
2022-12-28 13:17: Train Epoch 4: 74/159 Loss: 0.295776
2022-12-28 13:19: Train Epoch 4: 75/159 Loss: 0.410963
2022-12-28 13:20: Train Epoch 4: 76/159 Loss: 0.363175
2022-12-28 13:22: Train Epoch 4: 77/159 Loss: 0.242134
2022-12-28 13:23: Train Epoch 4: 78/159 Loss: 0.333620
2022-12-28 13:24: Train Epoch 4: 79/159 Loss: 0.253288
2022-12-28 13:26: Train Epoch 4: 80/159 Loss: 0.270855
2022-12-28 13:27: Train Epoch 4: 81/159 Loss: 0.239026
2022-12-28 13:28: Train Epoch 4: 82/159 Loss: 0.243944
2022-12-28 13:30: Train Epoch 4: 83/159 Loss: 0.304369
2022-12-28 13:31: Train Epoch 4: 84/159 Loss: 0.285647
2022-12-28 13:33: Train Epoch 4: 85/159 Loss: 0.279804
2022-12-28 13:34: Train Epoch 4: 86/159 Loss: 0.281422
2022-12-28 13:35: Train Epoch 4: 87/159 Loss: 0.259830
2022-12-28 13:37: Train Epoch 4: 88/159 Loss: 0.265684
2022-12-28 13:38: Train Epoch 4: 89/159 Loss: 0.283389
2022-12-28 13:39: Train Epoch 4: 90/159 Loss: 0.270311
2022-12-28 13:41: Train Epoch 4: 91/159 Loss: 0.295440
2022-12-28 13:42: Train Epoch 4: 92/159 Loss: 0.258574
2022-12-28 13:43: Train Epoch 4: 93/159 Loss: 0.247253
2022-12-28 13:45: Train Epoch 4: 94/159 Loss: 0.288587
2022-12-28 13:46: Train Epoch 4: 95/159 Loss: 0.291830
2022-12-28 13:48: Train Epoch 4: 96/159 Loss: 0.308210
2022-12-28 13:49: Train Epoch 4: 97/159 Loss: 0.311011
2022-12-28 13:50: Train Epoch 4: 98/159 Loss: 0.295297
2022-12-28 13:52: Train Epoch 4: 99/159 Loss: 0.259661
2022-12-28 13:53: Train Epoch 4: 100/159 Loss: 0.275603
2022-12-28 13:54: Train Epoch 4: 101/159 Loss: 0.334669
2022-12-28 13:56: Train Epoch 4: 102/159 Loss: 0.258938
2022-12-28 13:57: Train Epoch 4: 103/159 Loss: 0.331164
2022-12-28 13:59: Train Epoch 4: 104/159 Loss: 0.245352
2022-12-28 14:00: Train Epoch 4: 105/159 Loss: 0.274927
2022-12-28 14:01: Train Epoch 4: 106/159 Loss: 0.319697
2022-12-28 14:03: Train Epoch 4: 107/159 Loss: 0.276153
2022-12-28 14:04: Train Epoch 4: 108/159 Loss: 0.238800
2022-12-28 14:05: Train Epoch 4: 109/159 Loss: 0.208171
2022-12-28 14:07: Train Epoch 4: 110/159 Loss: 0.314727
2022-12-28 14:08: Train Epoch 4: 111/159 Loss: 0.303623
2022-12-28 14:09: Train Epoch 4: 112/159 Loss: 0.297449
2022-12-28 14:11: Train Epoch 4: 113/159 Loss: 0.259728
2022-12-28 14:12: Train Epoch 4: 114/159 Loss: 0.329148
2022-12-28 14:13: Train Epoch 4: 115/159 Loss: 0.313521
2022-12-28 14:15: Train Epoch 4: 116/159 Loss: 0.279502
2022-12-28 14:16: Train Epoch 4: 117/159 Loss: 0.305921
2022-12-28 14:18: Train Epoch 4: 118/159 Loss: 0.233819
2022-12-28 14:19: Train Epoch 4: 119/159 Loss: 0.278100
2022-12-28 14:20: Train Epoch 4: 120/159 Loss: 0.249214
2022-12-28 14:22: Train Epoch 4: 121/159 Loss: 0.242597
2022-12-28 14:23: Train Epoch 4: 122/159 Loss: 0.325260
2022-12-28 14:25: Train Epoch 4: 123/159 Loss: 0.227401
2022-12-28 14:26: Train Epoch 4: 124/159 Loss: 0.299456
2022-12-28 14:27: Train Epoch 4: 125/159 Loss: 0.316559
2022-12-28 14:29: Train Epoch 4: 126/159 Loss: 0.294014
2022-12-28 14:30: Train Epoch 4: 127/159 Loss: 0.280516
2022-12-28 14:31: Train Epoch 4: 128/159 Loss: 0.203743
2022-12-28 14:33: Train Epoch 4: 129/159 Loss: 0.335837
2022-12-28 14:34: Train Epoch 4: 130/159 Loss: 0.276557
2022-12-28 14:35: Train Epoch 4: 131/159 Loss: 0.227073
2022-12-28 14:37: Train Epoch 4: 132/159 Loss: 0.260874
2022-12-28 14:38: Train Epoch 4: 133/159 Loss: 0.290551
2022-12-28 14:40: Train Epoch 4: 134/159 Loss: 0.270726
2022-12-28 14:41: Train Epoch 4: 135/159 Loss: 0.277487
2022-12-28 14:42: Train Epoch 4: 136/159 Loss: 0.372372
2022-12-28 14:44: Train Epoch 4: 137/159 Loss: 0.294835
2022-12-28 14:45: Train Epoch 4: 138/159 Loss: 0.307250
2022-12-28 14:46: Train Epoch 4: 139/159 Loss: 0.252725
2022-12-28 14:48: Train Epoch 4: 140/159 Loss: 0.374862
2022-12-28 14:49: Train Epoch 4: 141/159 Loss: 0.269763
2022-12-28 14:51: Train Epoch 4: 142/159 Loss: 0.341087
2022-12-28 14:52: Train Epoch 4: 143/159 Loss: 0.281552
2022-12-28 14:53: Train Epoch 4: 144/159 Loss: 0.339897
2022-12-28 14:55: Train Epoch 4: 145/159 Loss: 0.207005
2022-12-28 14:56: Train Epoch 4: 146/159 Loss: 0.278243
