/home/joel.chacon/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)
  return torch._C._cuda_getDeviceCount() > 0
2022-12-24 09:00: log dir: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/20221224090053
2022-12-24 09:00: Experiment log path in: /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/20221224090053
2022-12-24 09:00: Argument: Namespace(batch_size=256, clc='vec', cuda=True, dataset='2020', dataset_root='/home/joel.chacon/tmp/datasets_grl/', debug=False, default_graph=True, device='cpu', dynamic_features=['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh'], early_stop=True, early_stop_patience=5, embed_dim=64, epochs=30, gamma=1.0, grad_norm=False, horizon=1, input_dim=25, lag=10, link_len=2, log_dir='/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/20221224090053', log_step=1, loss_func='nllloss', lr_decay=True, lr_decay_rate=0.1, lr_decay_step='15', lr_init=0.0001, mae_thresh=None, mape_thresh=0.0, max_grad_norm=5, mode='train', model='fire_GCN', nan_fill=0.5, num_layers=1, num_nodes=625, num_workers=12, output_dim=1, patch_height=25, patch_width=25, persistent_workers=True, pin_memory=True, plot=False, positive_weight=0.5, prefetch_factor=2, real_value=True, rnn_units=32, seed=27110, static_features=['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density'], teacher_forcing=False, test_ratio=0.2, val_ratio=0.2, weight_decay=0.01, window_len=10)
2022-12-24 09:00: Argument batch_size: 256
2022-12-24 09:00: Argument clc: 'vec'
2022-12-24 09:00: Argument cuda: True
2022-12-24 09:00: Argument dataset: '2020'
2022-12-24 09:00: Argument dataset_root: '/home/joel.chacon/tmp/datasets_grl/'
2022-12-24 09:00: Argument debug: False
2022-12-24 09:00: Argument default_graph: True
2022-12-24 09:00: Argument device: 'cpu'
2022-12-24 09:00: Argument dynamic_features: ['1 km 16 days NDVI', 'LST_Day_1km', 'LST_Night_1km', 'era5_max_d2m', 'era5_max_t2m', 'era5_max_sp', 'era5_max_tp', 'sminx', 'era5_max_wind_speed', 'era5_min_rh']
2022-12-24 09:00: Argument early_stop: True
2022-12-24 09:00: Argument early_stop_patience: 5
2022-12-24 09:00: Argument embed_dim: 64
2022-12-24 09:00: Argument epochs: 30
2022-12-24 09:00: Argument gamma: 1.0
2022-12-24 09:00: Argument grad_norm: False
2022-12-24 09:00: Argument horizon: 1
2022-12-24 09:00: Argument input_dim: 25
2022-12-24 09:00: Argument lag: 10
2022-12-24 09:00: Argument link_len: 2
2022-12-24 09:00: Argument log_dir: '/home/joel.chacon/tmp/WildFire_GCN/experiments/2020/20221224090053'
2022-12-24 09:00: Argument log_step: 1
2022-12-24 09:00: Argument loss_func: 'nllloss'
2022-12-24 09:00: Argument lr_decay: True
2022-12-24 09:00: Argument lr_decay_rate: 0.1
2022-12-24 09:00: Argument lr_decay_step: '15'
2022-12-24 09:00: Argument lr_init: 0.0001
2022-12-24 09:00: Argument mae_thresh: None
2022-12-24 09:00: Argument mape_thresh: 0.0
2022-12-24 09:00: Argument max_grad_norm: 5
2022-12-24 09:00: Argument mode: 'train'
2022-12-24 09:00: Argument model: 'fire_GCN'
2022-12-24 09:00: Argument nan_fill: 0.5
2022-12-24 09:00: Argument num_layers: 1
2022-12-24 09:00: Argument num_nodes: 625
2022-12-24 09:00: Argument num_workers: 12
2022-12-24 09:00: Argument output_dim: 1
2022-12-24 09:00: Argument patch_height: 25
2022-12-24 09:00: Argument patch_width: 25
2022-12-24 09:00: Argument persistent_workers: True
2022-12-24 09:00: Argument pin_memory: True
2022-12-24 09:00: Argument plot: False
2022-12-24 09:00: Argument positive_weight: 0.5
2022-12-24 09:00: Argument prefetch_factor: 2
2022-12-24 09:00: Argument real_value: True
2022-12-24 09:00: Argument rnn_units: 32
2022-12-24 09:00: Argument seed: 27110
2022-12-24 09:00: Argument static_features: ['dem_mean', 'slope_mean', 'roads_distance', 'waterway_distance', 'population_density']
2022-12-24 09:00: Argument teacher_forcing: False
2022-12-24 09:00: Argument test_ratio: 0.2
2022-12-24 09:00: Argument val_ratio: 0.2
2022-12-24 09:00: Argument weight_decay: 0.01
2022-12-24 09:00: Argument window_len: 10
++++++++++++++
2020_fire_GCN.conf
++++++++++++++
*****************Model Parameter*****************
ln1.weight torch.Size([25]) True
ln1.bias torch.Size([25]) True
convlstm.cell_list.0.conv.weight torch.Size([128, 57, 3, 3]) True
convlstm.cell_list.0.conv.bias torch.Size([128]) True
conv1.weight torch.Size([32, 32, 3, 3]) True
conv1.bias torch.Size([32]) True
fc1.weight torch.Size([64, 4608]) True
fc1.bias torch.Size([64]) True
fc2.weight torch.Size([32, 64]) True
fc2.bias torch.Size([32]) True
fc3.weight torch.Size([2, 32]) True
fc3.bias torch.Size([2]) True
Total params num: 372212
*****************Finish Parameter****************
Positives: 13518 / Negatives: 27036
Dataset length 40554
Positives: 1300 / Negatives: 2600
Dataset length 3900
Positives: 1228 / Negatives: 2456
Dataset length 3684
Applying learning rate decay.
Creat Log File in:  /home/joel.chacon/tmp/WildFire_GCN/experiments/2020/20221224090053/run.log
2022-12-24 09:01: Train Epoch 1: 0/159 Loss: 1.645872
2022-12-24 09:01: Train Epoch 1: 1/159 Loss: 1.245957
2022-12-24 09:01: Train Epoch 1: 2/159 Loss: 1.130276
2022-12-24 09:01: Train Epoch 1: 3/159 Loss: 0.944917
2022-12-24 09:01: Train Epoch 1: 4/159 Loss: 0.951047
2022-12-24 09:01: Train Epoch 1: 5/159 Loss: 0.824396
2022-12-24 09:01: Train Epoch 1: 6/159 Loss: 0.870979
2022-12-24 09:01: Train Epoch 1: 7/159 Loss: 0.787858
2022-12-24 09:01: Train Epoch 1: 8/159 Loss: 0.817410
2022-12-24 09:01: Train Epoch 1: 9/159 Loss: 0.826043
2022-12-24 09:01: Train Epoch 1: 10/159 Loss: 0.804546
2022-12-24 09:02: Train Epoch 1: 11/159 Loss: 0.784425
2022-12-24 09:02: Train Epoch 1: 12/159 Loss: 0.748236
2022-12-24 09:02: Train Epoch 1: 13/159 Loss: 0.757605
2022-12-24 09:02: Train Epoch 1: 14/159 Loss: 0.809206
2022-12-24 09:02: Train Epoch 1: 15/159 Loss: 0.708610
2022-12-24 09:02: Train Epoch 1: 16/159 Loss: 0.730599
2022-12-24 09:02: Train Epoch 1: 17/159 Loss: 0.747776
2022-12-24 09:02: Train Epoch 1: 18/159 Loss: 0.757264
2022-12-24 09:02: Train Epoch 1: 19/159 Loss: 0.703436
2022-12-24 09:02: Train Epoch 1: 20/159 Loss: 0.697198
2022-12-24 09:02: Train Epoch 1: 21/159 Loss: 0.791393
2022-12-24 09:02: Train Epoch 1: 22/159 Loss: 0.729748
2022-12-24 09:03: Train Epoch 1: 23/159 Loss: 0.791510
2022-12-24 09:03: Train Epoch 1: 24/159 Loss: 0.740799
2022-12-24 09:03: Train Epoch 1: 25/159 Loss: 0.739333
2022-12-24 09:03: Train Epoch 1: 26/159 Loss: 0.758807
2022-12-24 09:03: Train Epoch 1: 27/159 Loss: 0.763845
2022-12-24 09:03: Train Epoch 1: 28/159 Loss: 0.720265
2022-12-24 09:03: Train Epoch 1: 29/159 Loss: 0.682775
2022-12-24 09:03: Train Epoch 1: 30/159 Loss: 0.697186
2022-12-24 09:03: Train Epoch 1: 31/159 Loss: 0.735303
2022-12-24 09:03: Train Epoch 1: 32/159 Loss: 0.799932
2022-12-24 09:03: Train Epoch 1: 33/159 Loss: 0.755003
2022-12-24 09:03: Train Epoch 1: 34/159 Loss: 0.703725
2022-12-24 09:04: Train Epoch 1: 35/159 Loss: 0.686651
2022-12-24 09:04: Train Epoch 1: 36/159 Loss: 0.740762
2022-12-24 09:04: Train Epoch 1: 37/159 Loss: 0.710073
2022-12-24 09:04: Train Epoch 1: 38/159 Loss: 0.722585
2022-12-24 09:04: Train Epoch 1: 39/159 Loss: 0.712066
2022-12-24 09:04: Train Epoch 1: 40/159 Loss: 0.684352
2022-12-24 09:04: Train Epoch 1: 41/159 Loss: 0.703257
2022-12-24 09:04: Train Epoch 1: 42/159 Loss: 0.702589
2022-12-24 09:04: Train Epoch 1: 43/159 Loss: 0.637254
2022-12-24 09:04: Train Epoch 1: 44/159 Loss: 0.621610
2022-12-24 09:04: Train Epoch 1: 45/159 Loss: 0.693496
2022-12-24 09:04: Train Epoch 1: 46/159 Loss: 0.665655
2022-12-24 09:05: Train Epoch 1: 47/159 Loss: 0.657345
2022-12-24 09:05: Train Epoch 1: 48/159 Loss: 0.697098
2022-12-24 09:05: Train Epoch 1: 49/159 Loss: 0.659818
2022-12-24 09:05: Train Epoch 1: 50/159 Loss: 0.624143
2022-12-24 09:05: Train Epoch 1: 51/159 Loss: 0.669024
2022-12-24 09:05: Train Epoch 1: 52/159 Loss: 0.643236
2022-12-24 09:05: Train Epoch 1: 53/159 Loss: 0.595092
2022-12-24 09:05: Train Epoch 1: 54/159 Loss: 0.560400
2022-12-24 09:05: Train Epoch 1: 55/159 Loss: 0.570093
2022-12-24 09:05: Train Epoch 1: 56/159 Loss: 0.565219
2022-12-24 09:05: Train Epoch 1: 57/159 Loss: 0.568518
2022-12-24 09:06: Train Epoch 1: 58/159 Loss: 0.588524
